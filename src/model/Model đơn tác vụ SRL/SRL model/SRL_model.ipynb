{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - 5\n",
    "\n",
    "**Tasks :- NER tagging, POS tagging**\n",
    "\n",
    "**Tasks Description**\n",
    "\n",
    "``NER`` :-This is a Named Entity Recognition task where individual words of the sentence are tagged with an entity label it belongs to. The words which don't belong to any entity label are simply labeled as \"O\".\n",
    "\n",
    "``POS`` :- This is a Part of Speech tagging task. A part of speech is a category of words that have similar grammatical properties. Each word of the sentence is tagged with the part of speech label it belongs to. The words which don't belong to any part of speech label are simply labeled as \"O\".\n",
    "\n",
    "**Conversational Utility** :-  In conversational AI context, determining the syntactic parts of the sentence can help in extracting noun-phrases or important keyphrases from the sentence.\n",
    "\n",
    "**Data** :- In this example, we are using the <a href=\"https://www.clips.uantwerpen.be/conll2003/ner/\">coNLL 2003</a> data which is BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "The data is already present in ``coNLL_data`` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 1: Transforming data\n",
    "\n",
    "Raw data is in BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "We already provide a sample transformation function ``coNLL_ner_pos_to_tsv`` to convert this data to required tsv format. \n",
    "\n",
    "Running data transformations will save the required train, dev and test tsv data files under ``data`` directory in root of library. For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/data_transformations.html\">data transformations</a> in documentation.\n",
    "\n",
    "The transformation file should have the following details which is already created ``transform_file_conll.yml``.\n",
    "\n",
    "```\n",
    "transform1:\n",
    "  transform_func: coNLL_ner_pos_to_tsv\n",
    "  read_file_names:\n",
    "    - coNLL_train.txt\n",
    "    - coNLL_testa.txt\n",
    "    - coNLL_testb.txt\n",
    "  read_dir: coNLL_data\n",
    "  save_dir: ../../data\n",
    " ```\n",
    " Following command can be used to run the data transformation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "tien dat coNLL_data\n",
      "Making data from file coNLL_train.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "Processing 145000 rows...\n",
      "Processing 150000 rows...\n",
      "Processing 155000 rows...\n",
      "Processing 160000 rows...\n",
      "Processing 165000 rows...\n",
      "Processing 170000 rows...\n",
      "Processing 175000 rows...\n",
      "Processing 180000 rows...\n",
      "Processing 185000 rows...\n",
      "Processing 190000 rows...\n",
      "Processing 195000 rows...\n",
      "Processing 200000 rows...\n",
      "Processing 205000 rows...\n",
      "Processing 210000 rows...\n",
      "Processing 215000 rows...\n",
      "Processing 220000 rows...\n",
      "Processing 225000 rows...\n",
      "Processing 230000 rows...\n",
      "Processing 235000 rows...\n",
      "Processing 240000 rows...\n",
      "Processing 245000 rows...\n",
      "Processing 250000 rows...\n",
      "Processing 255000 rows...\n",
      "Processing 260000 rows...\n",
      "Processing 265000 rows...\n",
      "Processing 270000 rows...\n",
      "Processing 275000 rows...\n",
      "Processing 280000 rows...\n",
      "Processing 285000 rows...\n",
      "Processing 290000 rows...\n",
      "Processing 295000 rows...\n",
      "Processing 300000 rows...\n",
      "Processing 305000 rows...\n",
      "Processing 310000 rows...\n",
      "Processing 315000 rows...\n",
      "Processing 320000 rows...\n",
      "Processing 325000 rows...\n",
      "Processing 330000 rows...\n",
      "Processing 335000 rows...\n",
      "Processing 340000 rows...\n",
      "Processing 345000 rows...\n",
      "Processing 350000 rows...\n",
      "Processing 355000 rows...\n",
      "Processing 360000 rows...\n",
      "Processing 365000 rows...\n",
      "Processing 370000 rows...\n",
      "Processing 375000 rows...\n",
      "Processing 380000 rows...\n",
      "Processing 385000 rows...\n",
      "Processing 390000 rows...\n",
      "Processing 395000 rows...\n",
      "Processing 400000 rows...\n",
      "Processing 405000 rows...\n",
      "Processing 410000 rows...\n",
      "Processing 415000 rows...\n",
      "Processing 420000 rows...\n",
      "Processing 425000 rows...\n",
      "Processing 430000 rows...\n",
      "Processing 435000 rows...\n",
      "Processing 440000 rows...\n",
      "Processing 445000 rows...\n",
      "Processing 450000 rows...\n",
      "Processing 455000 rows...\n",
      "Processing 460000 rows...\n",
      "Processing 465000 rows...\n",
      "Processing 470000 rows...\n",
      "Processing 475000 rows...\n",
      "Processing 480000 rows...\n",
      "Processing 485000 rows...\n",
      "Processing 490000 rows...\n",
      "Processing 495000 rows...\n",
      "Processing 500000 rows...\n",
      "Processing 505000 rows...\n",
      "Processing 510000 rows...\n",
      "Processing 515000 rows...\n",
      "Processing 520000 rows...\n",
      "Processing 525000 rows...\n",
      "Processing 530000 rows...\n",
      "Processing 535000 rows...\n",
      "Processing 540000 rows...\n",
      "Processing 545000 rows...\n",
      "Processing 550000 rows...\n",
      "Processing 555000 rows...\n",
      "Processing 560000 rows...\n",
      "Processing 565000 rows...\n",
      "Processing 570000 rows...\n",
      "Processing 575000 rows...\n",
      "Processing 580000 rows...\n",
      "Processing 585000 rows...\n",
      "Processing 590000 rows...\n",
      "Processing 595000 rows...\n",
      "Processing 600000 rows...\n",
      "Processing 605000 rows...\n",
      "Processing 610000 rows...\n",
      "Processing 615000 rows...\n",
      "Processing 620000 rows...\n",
      "Processing 625000 rows...\n",
      "Processing 630000 rows...\n",
      "Processing 635000 rows...\n",
      "Processing 640000 rows...\n",
      "Processing 645000 rows...\n",
      "Processing 650000 rows...\n",
      "Processing 655000 rows...\n",
      "Processing 660000 rows...\n",
      "Processing 665000 rows...\n",
      "Processing 670000 rows...\n",
      "Processing 675000 rows...\n",
      "Processing 680000 rows...\n",
      "Processing 685000 rows...\n",
      "Processing 690000 rows...\n",
      "Processing 695000 rows...\n",
      "Processing 700000 rows...\n",
      "Processing 705000 rows...\n",
      "Processing 710000 rows...\n",
      "Processing 715000 rows...\n",
      "Processing 720000 rows...\n",
      "Processing 725000 rows...\n",
      "Processing 730000 rows...\n",
      "Processing 735000 rows...\n",
      "Processing 740000 rows...\n",
      "Processing 745000 rows...\n",
      "Processing 750000 rows...\n",
      "Processing 755000 rows...\n",
      "Processing 760000 rows...\n",
      "Processing 765000 rows...\n",
      "Processing 770000 rows...\n",
      "Processing 775000 rows...\n",
      "Processing 780000 rows...\n",
      "Processing 785000 rows...\n",
      "Processing 790000 rows...\n",
      "Processing 795000 rows...\n",
      "Processing 800000 rows...\n",
      "Processing 805000 rows...\n",
      "Processing 810000 rows...\n",
      "Processing 815000 rows...\n",
      "Processing 820000 rows...\n",
      "Processing 825000 rows...\n",
      "Processing 830000 rows...\n",
      "Processing 835000 rows...\n",
      "Processing 840000 rows...\n",
      "Processing 845000 rows...\n",
      "Processing 850000 rows...\n",
      "Processing 855000 rows...\n",
      "Processing 860000 rows...\n",
      "Processing 865000 rows...\n",
      "Processing 870000 rows...\n",
      "Processing 875000 rows...\n",
      "Processing 880000 rows...\n",
      "Processing 885000 rows...\n",
      "Processing 890000 rows...\n",
      "Processing 895000 rows...\n",
      "Processing 900000 rows...\n",
      "Processing 905000 rows...\n",
      "Processing 910000 rows...\n",
      "Processing 915000 rows...\n",
      "Processing 920000 rows...\n",
      "Processing 925000 rows...\n",
      "Processing 930000 rows...\n",
      "Processing 935000 rows...\n",
      "Processing 940000 rows...\n",
      "Processing 945000 rows...\n",
      "Processing 950000 rows...\n",
      "Processing 955000 rows...\n",
      "Processing 960000 rows...\n",
      "Processing 965000 rows...\n",
      "Processing 970000 rows...\n",
      "Processing 975000 rows...\n",
      "Processing 980000 rows...\n",
      "Processing 985000 rows...\n",
      "Processing 990000 rows...\n",
      "Processing 995000 rows...\n",
      "Processing 1000000 rows...\n",
      "Processing 1005000 rows...\n",
      "Processing 1010000 rows...\n",
      "Processing 1015000 rows...\n",
      "Processing 1020000 rows...\n",
      "Processing 1025000 rows...\n",
      "Processing 1030000 rows...\n",
      "Processing 1035000 rows...\n",
      "Processing 1040000 rows...\n",
      "Processing 1045000 rows...\n",
      "Processing 1050000 rows...\n",
      "Processing 1055000 rows...\n",
      "Processing 1060000 rows...\n",
      "Processing 1065000 rows...\n",
      "Processing 1070000 rows...\n",
      "Processing 1075000 rows...\n",
      "Processing 1080000 rows...\n",
      "Processing 1085000 rows...\n",
      "Processing 1090000 rows...\n",
      "Processing 1095000 rows...\n",
      "Processing 1100000 rows...\n",
      "Processing 1105000 rows...\n",
      "Processing 1110000 rows...\n",
      "Processing 1115000 rows...\n",
      "Processing 1120000 rows...\n",
      "Processing 1125000 rows...\n",
      "Processing 1130000 rows...\n",
      "Processing 1135000 rows...\n",
      "Processing 1140000 rows...\n",
      "Processing 1145000 rows...\n",
      "Processing 1150000 rows...\n",
      "Processing 1155000 rows...\n",
      "Processing 1160000 rows...\n",
      "Processing 1165000 rows...\n",
      "Processing 1170000 rows...\n",
      "Processing 1175000 rows...\n",
      "Processing 1180000 rows...\n",
      "Processing 1185000 rows...\n",
      "Processing 1190000 rows...\n",
      "Processing 1195000 rows...\n",
      "Processing 1200000 rows...\n",
      "Processing 1205000 rows...\n",
      "Processing 1210000 rows...\n",
      "Processing 1215000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Created NER label map from train file coNLL_train.txt\n",
      "{'B-A1': 0, 'I-A1': 1, 'O': 2, 'B-V': 3, 'B-A0': 4, 'I-A0': 5, 'B-A4': 6, 'I-A4': 7, 'I-A3': 8, 'B-A2': 9, 'I-A2': 10, 'B-A3': 11}\n",
      "label Map NER written at content/data/ner_coNLL_train_label_map.joblib\n",
      "Created POS label map from train file coNLL_train.txt\n",
      "{'B-O': 0}\n",
      "label Map POS written at content/data/pos_coNLL_train_label_map.joblib\n",
      "Max len of sentence:  83\n",
      "Mean len of sentences:  28.17264015333014\n",
      "Median len of sentences:  27.0\n",
      "Making data from file coNLL_testa.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "Processing 145000 rows...\n",
      "Processing 150000 rows...\n",
      "Processing 155000 rows...\n",
      "Processing 160000 rows...\n",
      "Processing 165000 rows...\n",
      "Processing 170000 rows...\n",
      "Processing 175000 rows...\n",
      "Processing 180000 rows...\n",
      "Processing 185000 rows...\n",
      "Processing 190000 rows...\n",
      "Processing 195000 rows...\n",
      "Processing 200000 rows...\n",
      "Processing 205000 rows...\n",
      "Processing 210000 rows...\n",
      "Processing 215000 rows...\n",
      "Processing 220000 rows...\n",
      "Processing 225000 rows...\n",
      "Processing 230000 rows...\n",
      "Processing 235000 rows...\n",
      "Processing 240000 rows...\n",
      "Processing 245000 rows...\n",
      "Processing 250000 rows...\n",
      "Processing 255000 rows...\n",
      "Processing 260000 rows...\n",
      "Processing 265000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  118\n",
      "Mean len of sentences:  27.94001304064334\n",
      "Median len of sentences:  27.0\n",
      "Making data from file coNLL_testb.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "Processing 145000 rows...\n",
      "Processing 150000 rows...\n",
      "Processing 155000 rows...\n",
      "Processing 160000 rows...\n",
      "Processing 165000 rows...\n",
      "Processing 170000 rows...\n",
      "Processing 175000 rows...\n",
      "Processing 180000 rows...\n",
      "Processing 185000 rows...\n",
      "Processing 190000 rows...\n",
      "Processing 195000 rows...\n",
      "Processing 200000 rows...\n",
      "Processing 205000 rows...\n",
      "Processing 210000 rows...\n",
      "Processing 215000 rows...\n",
      "Processing 220000 rows...\n",
      "Processing 225000 rows...\n",
      "Processing 230000 rows...\n",
      "Processing 235000 rows...\n",
      "Processing 240000 rows...\n",
      "Processing 245000 rows...\n",
      "Processing 250000 rows...\n",
      "Processing 255000 rows...\n",
      "Processing 260000 rows...\n",
      "Processing 265000 rows...\n",
      "Processing 270000 rows...\n",
      "Processing 275000 rows...\n",
      "Processing 280000 rows...\n",
      "Processing 285000 rows...\n",
      "Processing 290000 rows...\n",
      "Processing 295000 rows...\n",
      "Processing 300000 rows...\n",
      "Processing 305000 rows...\n",
      "Processing 310000 rows...\n",
      "Processing 315000 rows...\n",
      "Processing 320000 rows...\n",
      "Processing 325000 rows...\n",
      "Processing 330000 rows...\n",
      "Processing 335000 rows...\n",
      "Processing 340000 rows...\n",
      "Processing 345000 rows...\n",
      "Processing 350000 rows...\n",
      "Processing 355000 rows...\n",
      "Processing 360000 rows...\n",
      "Processing 365000 rows...\n",
      "Processing 370000 rows...\n",
      "Processing 375000 rows...\n",
      "Processing 380000 rows...\n",
      "Processing 385000 rows...\n",
      "Processing 390000 rows...\n",
      "Processing 395000 rows...\n",
      "Processing 400000 rows...\n",
      "Processing 405000 rows...\n",
      "Processing 410000 rows...\n",
      "Processing 415000 rows...\n",
      "Processing 420000 rows...\n",
      "Processing 425000 rows...\n",
      "Processing 430000 rows...\n",
      "Processing 435000 rows...\n",
      "Processing 440000 rows...\n",
      "Processing 445000 rows...\n",
      "Processing 450000 rows...\n",
      "Processing 455000 rows...\n",
      "Processing 460000 rows...\n",
      "Processing 465000 rows...\n",
      "Processing 470000 rows...\n",
      "Processing 475000 rows...\n",
      "Processing 480000 rows...\n",
      "Processing 485000 rows...\n",
      "Processing 490000 rows...\n",
      "Processing 495000 rows...\n",
      "Processing 500000 rows...\n",
      "Processing 505000 rows...\n",
      "Processing 510000 rows...\n",
      "Processing 515000 rows...\n",
      "Processing 520000 rows...\n",
      "Processing 525000 rows...\n",
      "Processing 530000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  83\n",
      "Mean len of sentences:  27.970335760078235\n",
      "Median len of sentences:  27.0\n",
      "Making data from file coNLL_testa_abolish.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  51\n",
      "Mean len of sentences:  24.512455516014235\n",
      "Median len of sentences:  22\n",
      "Making data from file coNLL_testb_abolish.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  51\n",
      "Mean len of sentences:  25.48409893992933\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testa_alter.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  34\n",
      "Mean len of sentences:  22.03543307086614\n",
      "Median len of sentences:  22.0\n",
      "Making data from file coNLL_testb_alter.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  32\n",
      "Mean len of sentences:  21.31764705882353\n",
      "Median len of sentences:  22\n",
      "Making data from file coNLL_testa_begin.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  52\n",
      "Mean len of sentences:  29.80234260614934\n",
      "Median len of sentences:  28\n",
      "Making data from file coNLL_testb_begin.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  60\n",
      "Mean len of sentences:  29.35233918128655\n",
      "Median len of sentences:  28.0\n",
      "Making data from file coNLL_testa_block.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  55\n",
      "Mean len of sentences:  30.472375690607734\n",
      "Median len of sentences:  31.5\n",
      "Making data from file coNLL_testb_block.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  57\n",
      "Mean len of sentences:  30.892561983471076\n",
      "Median len of sentences:  30\n",
      "Making data from file coNLL_testa_catalyse.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  82\n",
      "Mean len of sentences:  38.00512820512821\n",
      "Median len of sentences:  35\n",
      "Making data from file coNLL_testb_catalyse.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  83\n",
      "Mean len of sentences:  35.16836734693877\n",
      "Median len of sentences:  33.0\n",
      "Making data from file coNLL_testa_confer.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  62\n",
      "Mean len of sentences:  32.46056782334385\n",
      "Median len of sentences:  32\n",
      "Making data from file coNLL_testb_confer.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  62\n",
      "Mean len of sentences:  32.27899686520376\n",
      "Median len of sentences:  33\n",
      "Making data from file coNLL_testa_decrease.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  61\n",
      "Mean len of sentences:  30.174358974358974\n",
      "Median len of sentences:  28\n",
      "Making data from file coNLL_testb_decrease.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  61\n",
      "Mean len of sentences:  32.09137055837564\n",
      "Median len of sentences:  29\n",
      "Making data from file coNLL_testa_delete.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  48\n",
      "Mean len of sentences:  26.124087591240876\n",
      "Median len of sentences:  25\n",
      "Making data from file coNLL_testb_delete.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  49\n",
      "Mean len of sentences:  25.493946731234868\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testa_develop.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  118\n",
      "Mean len of sentences:  26.629032258064516\n",
      "Median len of sentences:  20.0\n",
      "Making data from file coNLL_testb_develop.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  42\n",
      "Mean len of sentences:  26.64516129032258\n",
      "Median len of sentences:  20.5\n",
      "Making data from file coNLL_testa_disrupt.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  60\n",
      "Mean len of sentences:  29.680672268907564\n",
      "Median len of sentences:  26\n",
      "Making data from file coNLL_testb_disrupt.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  58\n",
      "Mean len of sentences:  28.75\n",
      "Median len of sentences:  25.5\n",
      "Making data from file coNLL_testa_eliminate.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  51\n",
      "Mean len of sentences:  22.073298429319372\n",
      "Median len of sentences:  17\n",
      "Making data from file coNLL_testb_eliminate.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  52\n",
      "Mean len of sentences:  22.84375\n",
      "Median len of sentences:  18.0\n",
      "Making data from file coNLL_testa_encode.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  50\n",
      "Mean len of sentences:  24.346938775510203\n",
      "Median len of sentences:  23.0\n",
      "Making data from file coNLL_testb_encode.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  50\n",
      "Mean len of sentences:  24.568527918781726\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testa_express.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  42\n",
      "Mean len of sentences:  26.08607594936709\n",
      "Median len of sentences:  26\n",
      "Making data from file coNLL_testb_express.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  45\n",
      "Mean len of sentences:  25.83123425692695\n",
      "Median len of sentences:  26\n",
      "Making data from file coNLL_testa_generate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  56\n",
      "Mean len of sentences:  26.991891891891893\n",
      "Median len of sentences:  24.0\n",
      "Making data from file coNLL_testb_generate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  56\n",
      "Mean len of sentences:  27.26145552560647\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testa_inhibit.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  79\n",
      "Mean len of sentences:  29.383480825958703\n",
      "Median len of sentences:  27\n",
      "Making data from file coNLL_testb_inhibit.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  80\n",
      "Mean len of sentences:  28.926686217008797\n",
      "Median len of sentences:  27\n",
      "Making data from file coNLL_testa_initiate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  64\n",
      "Mean len of sentences:  31.65989847715736\n",
      "Median len of sentences:  28\n",
      "Making data from file coNLL_testb_initiate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  62\n",
      "Mean len of sentences:  31.17766497461929\n",
      "Median len of sentences:  28\n",
      "Making data from file coNLL_testa_lead.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  77\n",
      "Mean len of sentences:  31.428861788617887\n",
      "Median len of sentences:  29.0\n",
      "Making data from file coNLL_testb_lead.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  77\n",
      "Mean len of sentences:  31.39553752535497\n",
      "Median len of sentences:  29\n",
      "Making data from file coNLL_testa_lose.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  57\n",
      "Mean len of sentences:  29.1013986013986\n",
      "Median len of sentences:  30.0\n",
      "Making data from file coNLL_testb_lose.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  56\n",
      "Mean len of sentences:  28.209059233449477\n",
      "Median len of sentences:  29\n",
      "Making data from file coNLL_testa_modify.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  51\n",
      "Mean len of sentences:  32.78313253012048\n",
      "Median len of sentences:  30.0\n",
      "Making data from file coNLL_testb_modify.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  48\n",
      "Mean len of sentences:  32.65868263473054\n",
      "Median len of sentences:  31\n",
      "Making data from file coNLL_testa_mutate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  55\n",
      "Mean len of sentences:  31.75\n",
      "Median len of sentences:  32.0\n",
      "Making data from file coNLL_testb_mutate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  56\n",
      "Mean len of sentences:  33.46122448979592\n",
      "Median len of sentences:  32\n",
      "Making data from file coNLL_testa_proliferate.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  44\n",
      "Mean len of sentences:  26.820987654320987\n",
      "Median len of sentences:  24.5\n",
      "Making data from file coNLL_testb_proliferate.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  43\n",
      "Mean len of sentences:  26.312883435582823\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testa_recognize.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  61\n",
      "Mean len of sentences:  25.83848797250859\n",
      "Median len of sentences:  23\n",
      "Making data from file coNLL_testb_recognize.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  53\n",
      "Mean len of sentences:  24.600682593856654\n",
      "Median len of sentences:  21\n",
      "Making data from file coNLL_testa_result.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  53\n",
      "Mean len of sentences:  28.45766129032258\n",
      "Median len of sentences:  28.0\n",
      "Making data from file coNLL_testb_result.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  52\n",
      "Mean len of sentences:  27.561368209255534\n",
      "Median len of sentences:  27\n",
      "Making data from file coNLL_testa_skip.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  69\n",
      "Mean len of sentences:  32.962025316455694\n",
      "Median len of sentences:  28\n",
      "Making data from file coNLL_testb_skip.txt ...\n",
      "Processing 0 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  70\n",
      "Mean len of sentences:  31.525\n",
      "Median len of sentences:  28.5\n",
      "Making data from file coNLL_testa_splice.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  57\n",
      "Mean len of sentences:  29.325641025641026\n",
      "Median len of sentences:  24.5\n",
      "Making data from file coNLL_testb_splice.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  57\n",
      "Mean len of sentences:  28.307692307692307\n",
      "Median len of sentences:  23.0\n",
      "Making data from file coNLL_testa_transcribe.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  50\n",
      "Mean len of sentences:  24.774718397997496\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testb_transcribe.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  50\n",
      "Mean len of sentences:  24.928838951310862\n",
      "Median len of sentences:  24\n",
      "Making data from file coNLL_testa_transform.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  40\n",
      "Mean len of sentences:  22.544\n",
      "Median len of sentences:  22\n",
      "Making data from file coNLL_testb_transform.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  39\n",
      "Mean len of sentences:  22.3262599469496\n",
      "Median len of sentences:  22\n",
      "Making data from file coNLL_testa_translate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  61\n",
      "Mean len of sentences:  26.696165191740413\n",
      "Median len of sentences:  24.0\n",
      "Making data from file coNLL_testb_translate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  61\n",
      "Mean len of sentences:  26.290560471976402\n",
      "Median len of sentences:  24.0\n",
      "Making data from file coNLL_testa_truncate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  56\n",
      "Mean len of sentences:  35.1055900621118\n",
      "Median len of sentences:  37\n",
      "Making data from file coNLL_testb_truncate.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  55\n",
      "Mean len of sentences:  35.962732919254655\n",
      "Median len of sentences:  37\n",
      "tien dat coNLL_data\n",
      "Making data from file coNLL_train1.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "Processing 145000 rows...\n",
      "Processing 150000 rows...\n",
      "Processing 155000 rows...\n",
      "Processing 160000 rows...\n",
      "Processing 165000 rows...\n",
      "Processing 170000 rows...\n",
      "Processing 175000 rows...\n",
      "Processing 180000 rows...\n",
      "Processing 185000 rows...\n",
      "Processing 190000 rows...\n",
      "Processing 195000 rows...\n",
      "Processing 200000 rows...\n",
      "Processing 205000 rows...\n",
      "Processing 210000 rows...\n",
      "Processing 215000 rows...\n",
      "Processing 220000 rows...\n",
      "Processing 225000 rows...\n",
      "Processing 230000 rows...\n",
      "Processing 235000 rows...\n",
      "Processing 240000 rows...\n",
      "Processing 245000 rows...\n",
      "Processing 250000 rows...\n",
      "Processing 255000 rows...\n",
      "Processing 260000 rows...\n",
      "Processing 265000 rows...\n",
      "Processing 270000 rows...\n",
      "Processing 275000 rows...\n",
      "Processing 280000 rows...\n",
      "Processing 285000 rows...\n",
      "Processing 290000 rows...\n",
      "Processing 295000 rows...\n",
      "Processing 300000 rows...\n",
      "Processing 305000 rows...\n",
      "Processing 310000 rows...\n",
      "Processing 315000 rows...\n",
      "Processing 320000 rows...\n",
      "Processing 325000 rows...\n",
      "Processing 330000 rows...\n",
      "Processing 335000 rows...\n",
      "Processing 340000 rows...\n",
      "Processing 345000 rows...\n",
      "Processing 350000 rows...\n",
      "Processing 355000 rows...\n",
      "Processing 360000 rows...\n",
      "Processing 365000 rows...\n",
      "Processing 370000 rows...\n",
      "Processing 375000 rows...\n",
      "Processing 380000 rows...\n",
      "Processing 385000 rows...\n",
      "Processing 390000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Created NER label map from train file coNLL_train1.txt\n",
      "{'B-amod_2': 0, 'B-nsubjpass_4': 1, 'B-auxpass_4': 2, 'B-root_0': 3, 'B-prep_4': 4, 'B-nn_10': 5, 'I-nn_10': 6, 'B-pobj_5': 7, 'B-prep_10': 8, 'B-amod_15': 9, 'B-nn_15': 10, 'I-nn_15': 11, 'B-pobj_11': 12, 'B-punct_4': 13, 'B-nsubjpass_13': 14, 'B-prep_1': 15, 'B-det_6': 16, 'B-amod_6': 17, 'B-dep_4': 18, 'B-pobj_2': 19, 'B-prep_6': 20, 'B-amod_10': 21, 'B-pobj_7': 22, 'B-aux_13': 23, 'B-auxpass_13': 24, 'B-prep_13': 25, 'B-amod_16': 26, 'B-pobj_14': 27, 'B-punct_13': 28, 'B-nsubjpass_17': 29, 'B-nn_6': 30, 'I-nn_6': 31, 'B-num_6': 32, 'B-cc_6': 33, 'B-conj_6': 34, 'B-prep_8': 35, 'B-det_11': 36, 'B-pobj_9': 37, 'B-dep_11': 38, 'B-cc_12': 39, 'B-conj_12': 40, 'B-aux_17': 41, 'B-auxpass_17': 42, 'B-prep_17': 43, 'B-det_20': 44, 'B-pobj_18': 45, 'B-punct_17': 46, 'B-mark_6': 47, 'B-nn_5': 48, 'I-nn_5': 49, 'B-num_5': 50, 'B-nsubj_6': 51, 'B-advcl_18': 52, 'B-dobj_6': 53, 'B-nn_11': 54, 'I-nn_11': 55, 'B-pobj_8': 56, 'B-appos_11': 57, 'I-punct_18': 58, 'B-nsubj_18': 59, 'B-aux_18': 60, 'B-prep_18': 61, 'B-det_22': 62, 'B-nn_22': 63, 'B-pobj_19': 64, 'B-vmod_22': 65, 'B-prep_23': 66, 'B-det_26': 67, 'B-pobj_24': 68, 'B-prep_26': 69, 'B-amod_29': 70, 'B-pobj_27': 71, 'B-punct_18': 72, 'B-det_2': 73, 'B-nsubjpass_36': 74, 'B-prep_2': 75, 'B-nn_7': 76, 'I-nn_7': 77, 'B-pobj_3': 78, 'B-punct_9': 79, 'B-appos_7': 80, 'I-punct_2': 81, 'B-nsubj_15': 82, 'B-prep_12': 83, 'B-pobj_13': 84, 'B-rcmod_2': 85, 'B-prep_15': 86, 'B-amod_22': 87, 'I-nn_22': 88, 'B-pobj_16': 89, 'B-num_22': 90, 'B-cc_15': 91, 'B-conj_15': 92, 'B-det_32': 93, 'B-nn_32': 94, 'I-nn_32': 95, 'B-num_32': 96, 'B-punct_2': 97, 'B-aux_36': 98, 'B-auxpass_36': 99, 'B-punct_36': 100, 'B-vmod_36': 101, 'B-nsubj_43': 102, 'B-aux_43': 103, 'B-cop_43': 104, 'B-amod_43': 105, 'B-ccomp_38': 106, 'B-prep_43': 107, 'B-det_47': 108, 'B-amod_47': 109, 'B-pobj_44': 110, 'B-prep_47': 111, 'B-det_52': 112, 'B-nn_52': 113, 'I-nn_52': 114, 'B-pobj_48': 115, 'B-det_3': 116, 'B-nn_3': 117, 'B-nsubj_4': 118, 'B-num_7': 119, 'B-prep_7': 120, 'B-prep_9': 121, 'B-nn_12': 122, 'B-pobj_10': 123, 'B-punct_7': 124, 'B-num_15': 125, 'B-conj_7': 126, 'B-amod_18': 127, 'B-vmod_18': 128, 'B-dobj_19': 129, 'B-dep_20': 130, 'B-cc_7': 131, 'B-quantmod_25': 132, 'B-num_26': 133, 'B-det_29': 134, 'B-vmod_29': 135, 'B-dep_32': 136, 'B-dobj_30': 137, 'B-punct_34': 138, 'B-dep_29': 139, 'B-amod_34': 140, 'B-dep_35': 141, 'I-punct_4': 142, 'B-amod_3': 143, 'B-nsubj_10': 144, 'B-prep_3': 145, 'B-pobj_4': 146, 'B-nn_9': 147, 'B-mark_18': 148, 'B-det_13': 149, 'B-cop_18': 150, 'B-neg_18': 151, 'B-nn_18': 152, 'I-nn_18': 153, 'B-ccomp_10': 154, 'B-cc_18': 155, 'B-advmod_24': 156, 'B-cop_24': 157, 'B-amod_24': 158, 'B-conj_18': 159, 'B-prep_24': 160, 'B-nn_27': 161, 'B-pobj_25': 162, 'B-poss_29': 163, 'B-nsubjpass_31': 164, 'B-auxpass_31': 165, 'B-rcmod_27': 166, 'B-prep_31': 167, 'B-det_34': 168, 'B-pobj_32': 169, 'B-prep_34': 170, 'B-amod_37': 171, 'B-pobj_35': 172, 'B-punct_10': 173, 'B-nsubj_20': 174, 'B-det_8': 175, 'B-cc_8': 176, 'B-nn_13': 177, 'I-nn_13': 178, 'B-num_13': 179, 'B-conj_8': 180, 'B-advmod_15': 181, 'B-vmod_8': 182, 'B-det_19': 183, 'B-amod_19': 184, 'B-dobj_20': 185, 'B-punct_20': 186, 'B-vmod_20': 187, 'B-mark_32': 188, 'B-nsubj_32': 189, 'B-prep_28': 190, 'B-det_31': 191, 'B-pobj_29': 192, 'B-ccomp_24': 193, 'B-prep_32': 194, 'B-pobj_33': 195, 'B-det_9': 196, 'B-amod_9': 197, 'I-amod_9': 198, 'B-punct_1': 199, 'B-prep_5': 200, 'B-pobj_1': 201, 'B-nsubj_5': 202, 'B-dobj_5': 203, 'B-punct_11': 204, 'B-cc_14': 205, 'I-cc_11': 206, 'B-conj_11': 207, 'B-punct_5': 208, 'B-det_23': 209, 'B-nn_23': 210, 'B-amod_23': 211, 'I-amod_23': 212, 'B-dep_23': 213, 'B-amod_4': 214, 'B-dobj_1': 215, 'B-nsubj_9': 216, 'B-parataxis_1': 217, 'B-det_12': 218, 'B-amod_12': 219, 'B-dobj_9': 220, 'B-num_17': 221, 'B-conj_14': 222, 'B-punct_19': 223, 'B-dep_17': 224, 'B-cc_19': 225, 'B-conj_19': 226, 'B-appos_19': 227, 'B-dep_24': 228, 'B-nn_29': 229, 'B-mark_49': 230, 'B-advmod_33': 231, 'B-num_36': 232, 'B-cc_33': 233, 'B-conj_33': 234, 'B-nsubj_49': 235, 'B-punct_38': 236, 'B-dep_36': 237, 'B-cc_38': 238, 'B-conj_38': 239, 'B-appos_38': 240, 'B-amod_42': 241, 'B-dep_43': 242, 'B-prep_36': 243, 'B-nn_48': 244, 'B-pobj_46': 245, 'B-advcl_9': 246, 'B-prep_49': 247, 'B-pobj_50': 248, 'B-aux_2': 249, 'B-det_5': 250, 'B-amod_5': 251, 'B-dobj_2': 252, 'B-nsubjpass_12': 253, 'B-dep_7': 254, 'B-auxpass_12': 255, 'B-parataxis_2': 256, 'B-nsubjpass_6': 257, 'B-auxpass_6': 258, 'B-dep_12': 259, 'B-nn_16': 260, 'B-punct_6': 261, 'B-mark_23': 262, 'B-nsubj_23': 263, 'B-prep_19': 264, 'B-pobj_20': 265, 'B-advcl_6': 266, 'B-det_27': 267, 'B-amod_27': 268, 'B-prep_27': 269, 'B-det_30': 270, 'B-pobj_28': 271, 'B-prep_30': 272, 'B-pobj_31': 273, 'B-det_7': 274, 'B-amod_7': 275, 'B-advmod_10': 276, 'B-mwe_10': 277, 'B-amod_13': 278, 'B-prep_22': 279, 'B-amod_25': 280, 'B-pobj_23': 281, 'B-punct_15': 282, 'B-nsubj_3': 283, 'B-mark_12': 284, 'B-nsubj_12': 285, 'I-cc_5': 286, 'B-conj_5': 287, 'B-ccomp_3': 288, 'B-dobj_12': 289, 'B-punct_12': 290, 'B-mark_21': 291, 'B-nsubj_21': 292, 'B-cop_21': 293, 'B-prep_21': 294, 'B-pobj_22': 295, 'B-cc_23': 296, 'B-conj_23': 297, 'B-punct_3': 298, 'B-num_4': 299, 'B-nn_4': 300, 'B-rcmod_4': 301, 'B-num_12': 302, 'B-dep_9': 303, 'B-det_4': 304, 'B-cc_4': 305, 'B-conj_4': 306, 'B-det_16': 307, 'B-prep_16': 308, 'B-advmod_18': 309, 'B-pobj_17': 310, 'B-num_3': 311, 'B-nsubjpass_5': 312, 'B-auxpass_5': 313, 'B-nsubj_7': 314, 'B-dep_5': 315, 'B-dobj_7': 316, 'B-punct_8': 317, 'B-dep_10': 318, 'B-cc_11': 319, 'B-rcmod_8': 320, 'B-prep_20': 321, 'B-pobj_21': 322, 'B-cc_17': 323, 'B-conj_17': 324, 'B-num_29': 325, 'B-dobj_25': 326, 'B-preconj_33': 327, 'B-amod_35': 328, 'B-nn_38': 329, 'B-dep_33': 330, 'B-cop_6': 331, 'B-preconj_13': 332, 'B-nsubj_17': 333, 'B-cc_13': 334, 'B-conj_13': 335, 'B-cop_17': 336, 'B-dep_6': 337, 'B-poss_12': 338, 'I-amod_12': 339, 'B-det_18': 340, 'B-dobj_15': 341, 'B-dep_15': 342, 'B-det_10': 343, 'B-num_10': 344, 'B-det_15': 345, 'B-nsubj_13': 346, 'B-amod_8': 347, 'B-nn_8': 348, 'I-nn_8': 349, 'B-num_8': 350, 'B-appos_8': 351, 'B-rcmod_19': 352, 'I-amod_27': 353, 'B-punct_29': 354, 'B-appos_27': 355, 'I-punct_13': 356, 'B-cop_12': 357, 'B-nsubj_2': 358, 'B-mark_19': 359, 'B-quantmod_6': 360, 'B-mwe_4': 361, 'B-nsubj_19': 362, 'B-amod_17': 363, 'B-aux_19': 364, 'B-ccomp_2': 365, 'B-nn_26': 366, 'B-poss_8': 367, 'B-mark_13': 368, 'B-cop_13': 369, 'B-advcl_4': 370, 'B-cc_5': 371, 'B-advmod_9': 372, 'B-poss_11': 373, 'B-prep_11': 374, 'B-pobj_12': 375, 'B-rcmod_13': 376, 'B-preconj_2': 377, 'B-cc_2': 378, 'B-conj_2': 379, 'B-advmod_7': 380, 'B-amod_11': 381, 'B-vmod_11': 382, 'B-det_17': 383, 'B-nn_17': 384, 'I-nn_17': 385, 'B-nsubjpass_7': 386, 'B-auxpass_7': 387, 'B-num_11': 388, 'B-det_21': 389, 'B-num_21': 390, 'B-nn_21': 391, 'B-vmod_2': 392, 'B-aux_5': 393, 'B-xcomp_3': 394, 'B-pobj_6': 395, 'B-advmod_13': 396, 'B-cop_20': 397, 'B-advmod_20': 398, 'B-amod_20': 399, 'B-nn_20': 400, 'B-dep_2': 401, 'B-cop_19': 402, 'B-nn_25': 403, 'B-amod_14': 404, 'I-amod_14': 405, 'B-nn_14': 406, 'B-dobj_10': 407, 'B-nsubj_16': 408, 'B-rcmod_14': 409, 'B-nn_19': 410, 'B-dobj_16': 411, 'B-advmod_17': 412, 'B-neg_16': 413, 'B-parataxis_4': 414, 'B-acomp_17': 415, 'B-dep_13': 416, 'B-aux_15': 417, 'B-dobj_17': 418, 'B-mark_20': 419, 'I-nsubj_7': 420, 'B-rcmod_5': 421, 'I-nn_12': 422, 'B-appos_12': 423, 'B-cop_3': 424, 'B-preconj_11': 425, 'B-advmod_19': 426, 'B-rcmod_11': 427, 'B-amod_21': 428, 'B-poss_5': 429, 'B-aux_7': 430, 'B-vmod_5': 431, 'B-aux_21': 432, 'B-xcomp_19': 433, 'B-det_28': 434, 'B-amod_28': 435, 'B-nn_28': 436, 'B-amod_31': 437, 'B-prep_52': 438, 'B-prep_35': 439, 'B-pobj_36': 440, 'B-punct_52': 441, 'B-det_43': 442, 'B-dep_40': 443, 'I-dep_41': 444, 'B-nsubjpass_52': 445, 'B-det_46': 446, 'B-punct_46': 447, 'B-vmod_46': 448, 'B-dep_48': 449, 'B-punct_43': 450, 'B-auxpass_52': 451, 'B-parataxis_19': 452, 'B-nn_55': 453, 'B-pobj_53': 454, 'B-cc_53': 455, 'B-neg_58': 456, 'B-conj_53': 457, 'B-amod_60': 458, 'B-pobj_58': 459, 'B-nsubj_8': 460, 'B-mark_11': 461, 'B-expl_11': 462, 'B-dep_8': 463, 'B-quantmod_14': 464, 'B-mwe_12': 465, 'B-nsubj_11': 466, 'B-rcmod_15': 467, 'B-nn_24': 468, 'I-punct_27': 469, 'B-dep_30': 470, 'B-punct_27': 471, 'B-amod_30': 472, 'B-mwe_32': 473, 'B-nsubjpass_39': 474, 'B-auxpass_39': 475, 'B-neg_39': 476, 'B-rcmod_30': 477, 'B-prep_39': 478, 'B-amod_44': 479, 'I-amod_44': 480, 'B-nn_44': 481, 'B-pobj_40': 482, 'B-punct_30': 483, 'I-punct_47': 484, 'B-dep_50': 485, 'B-punct_47': 486, 'B-amod_50': 487, 'B-mwe_52': 488, 'B-prep_50': 489, 'B-nn_54': 490, 'B-pobj_52': 491, 'B-nsubj_58': 492, 'B-cop_58': 493, 'B-advmod_58': 494, 'B-rcmod_50': 495, 'B-prep_58': 496, 'B-nn_61': 497, 'B-pobj_59': 498, 'B-cc_58': 499, 'B-auxpass_64': 500, 'B-conj_58': 501, 'B-prep_64': 502, 'B-det_68': 503, 'B-amod_68': 504, 'B-pobj_65': 505, 'B-prep_68': 506, 'B-amod_71': 507, 'B-pobj_69': 508, 'B-punct_50': 509, 'I-punct_74': 510, 'B-dep_78': 511, 'B-punct_74': 512, 'B-advmod_78': 513, 'B-amod_78': 514, 'B-punct_78': 515, 'B-vmod_78': 516, 'B-prep_80': 517, 'B-det_83': 518, 'B-pobj_81': 519, 'B-amod_83': 520, 'B-prep_83': 521, 'B-det_89': 522, 'B-nn_89': 523, 'I-nn_89': 524, 'B-pobj_85': 525, 'B-cop_4': 526, 'B-aux_20': 527, 'B-xcomp_18': 528, 'B-advmod_34': 529, 'B-mwe_34': 530, 'B-det_38': 531, 'B-amod_38': 532, 'B-pobj_34': 533, 'B-prep_38': 534, 'B-det_44': 535, 'B-pobj_39': 536, 'B-cc_1': 537, 'B-conj_1': 538, 'B-dep_1': 539, 'I-amod_15': 540, 'B-appos_1': 541, 'B-cc_9': 542, 'B-conj_9': 543, 'B-cc_25': 544, 'B-conj_25': 545, 'B-mwe_30': 546, 'B-pobj_30': 547, 'B-punct_33': 548, 'B-appos_31': 549, 'I-punct_9': 550, 'B-cop_7': 551, 'B-advmod_14': 552, 'B-prep_14': 553, 'B-num_18': 554, 'B-punct_14': 555, 'B-num_27': 556, 'B-prep_29': 557, 'B-number_32': 558, 'B-amod_33': 559, 'B-nsubjpass_9': 560, 'B-auxpass_9': 561, 'B-appos_6': 562, 'I-punct_6': 563, 'B-det_14': 564, 'B-pobj_15': 565, 'B-advmod_21': 566, 'B-punct_21': 567, 'B-appos_4': 568, 'B-advmod_12': 569, 'B-advmod_16': 570, 'B-dobj_21': 571, 'B-nn_2': 572, 'B-dobj_3': 573, 'I-nn_9': 574, 'B-cc_3': 575, 'B-conj_3': 576, 'B-iobj_20': 577, 'B-preconj_7': 578, 'B-neg_12': 579, 'B-mark_7': 580, 'B-dep_19': 581, 'B-pcomp_8': 582, 'B-num_19': 583, 'B-vmod_12': 584, 'B-num_23': 585, 'B-mark_16': 586, 'B-nsubjpass_16': 587, 'B-vmod_6': 588, 'B-auxpass_16': 589, 'B-preconj_19': 590, 'I-amod_6': 591, 'B-nsubjpass_15': 592, 'I-amod_10': 593, 'B-appos_10': 594, 'B-auxpass_15': 595, 'B-cc_22': 596, 'B-conj_22': 597, 'B-cc_16': 598, 'B-conj_16': 599, 'B-appos_30': 600, 'B-det_41': 601, 'B-amod_41': 602, 'B-nn_41': 603, 'I-nn_41': 604, 'B-num_41': 605, 'B-dobj_4': 606, 'B-cc_10': 607, 'B-conj_10': 608, 'B-pcomp_15': 609, 'B-rcmod_18': 610, 'B-det_24': 611, 'B-punct_32': 612, 'B-cc_32': 613, 'B-amod_32': 614, 'B-advmod_3': 615, 'B-ccomp_5': 616, 'I-amod_4': 617, 'I-nn_25': 618, 'B-num_25': 619, 'B-det_36': 620, 'B-amod_36': 621, 'B-nn_36': 622, 'B-dobj_32': 623, 'B-det_40': 624, 'B-nn_40': 625, 'B-pobj_37': 626, 'B-nsubj_42': 627, 'B-rcmod_40': 628, 'B-dobj_42': 629, 'B-prep_44': 630, 'B-pobj_45': 631, 'B-cc_47': 632, 'B-conj_47': 633, 'B-neg_55': 634, 'I-nn_55': 635, 'B-nsubjpass_57': 636, 'B-auxpass_57': 637, 'B-conj_32': 638, 'B-dobj_8': 639, 'B-vmod_19': 640, 'I-nn_24': 641, 'B-vmod_27': 642, 'B-dobj_28': 643, 'B-prep_33': 644, 'B-pcomp_34': 645, 'B-dobj_35': 646, 'B-amod_39': 647, 'B-cc_39': 648, 'B-conj_39': 649, 'B-mark_2': 650, 'B-advcl_16': 651, 'B-punct_16': 652, 'B-dep_18': 653, 'B-punct_23': 654, 'B-appos_18': 655, 'I-nn_29': 656, 'B-nsubj_39': 657, 'B-rcmod_35': 658, 'B-advmod_46': 659, 'B-amod_46': 660, 'B-nn_46': 661, 'I-nn_46': 662, 'B-prep_46': 663, 'B-det_51': 664, 'B-nn_51': 665, 'I-nn_51': 666, 'B-pobj_47': 667, 'B-punct_53': 668, 'B-appos_46': 669, 'I-punct_16': 670, 'B-poss_2': 671, 'B-mark_17': 672, 'B-det_25': 673, 'B-nn_30': 674, 'B-pobj_26': 675, 'B-nn_34': 676, 'B-cop_9': 677, 'B-appos_9': 678, 'B-aux_16': 679, 'B-rcmod_9': 680, 'B-auxpass_19': 681, 'B-xcomp_16': 682, 'B-poss_24': 683, 'I-amod_24': 684, 'B-nsubjpass_8': 685, 'B-rcmod_1': 686, 'B-aux_8': 687, 'B-auxpass_8': 688, 'I-prep_6': 689, 'B-quantmod_18': 690, 'B-mwe_16': 691, 'B-nsubjpass_32': 692, 'B-advmod_23': 693, 'B-punct_25': 694, 'B-prep_25': 695, 'I-prep_25': 696, 'I-punct_19': 697, 'B-auxpass_32': 698, 'B-advcl_13': 699, 'B-advmod_32': 700, 'B-det_37': 701, 'B-nn_37': 702, 'B-aux_3': 703, 'B-rcmod_7': 704, 'B-advmod_11': 705, 'B-advmod_29': 706, 'B-dobj_29': 707, 'B-advmod_5': 708, 'B-dep_22': 709, 'B-advmod_4': 710, 'B-aux_6': 711, 'B-xcomp_4': 712, 'B-cop_11': 713, 'B-cop_23': 714, 'B-neg_23': 715, 'I-nn_31': 716, 'I-amod_3': 717, 'I-nn_19': 718, 'B-num_24': 719, 'B-punct_24': 720, 'B-cc_30': 721, 'B-conj_30': 722, 'B-rcmod_24': 723, 'B-det_39': 724, 'B-dep_3': 725, 'B-mark_14': 726, 'B-nsubjpass_14': 727, 'B-auxpass_14': 728, 'B-num_28': 729, 'B-cop_10': 730, 'B-vmod_10': 731, 'B-vmod_4': 732, 'B-nsubjpass_20': 733, 'B-auxpass_20': 734, 'B-ccomp_9': 735, 'B-possessive_29': 736, 'B-npadvmod_32': 737, 'B-dobj_36': 738, 'B-amod_48': 739, 'B-dobj_46': 740, 'B-det_53': 741, 'B-amod_53': 742, 'B-nn_53': 743, 'B-pobj_49': 744, 'B-punct_55': 745, 'B-amod_55': 746, 'B-dep_56': 747, 'B-appos_55': 748, 'B-punct_63': 749, 'B-number_63': 750, 'B-dep_59': 751, 'B-amod_59': 752, 'B-dep_65': 753, 'B-prep_69': 754, 'B-pobj_70': 755, 'B-nsubjpass_3': 756, 'B-auxpass_3': 757, 'B-rcmod_16': 758, 'B-pcomp_22': 759, 'B-dobj_23': 760, 'I-nn_3': 761, 'I-nn_4': 762, 'B-dobj_18': 763, 'B-poss_25': 764, 'I-nn_16': 765, 'B-dep_16': 766, 'I-punct_7': 767, 'B-cop_5': 768, 'B-vmod_9': 769, 'I-advmod_10': 770, 'B-nsubjpass_19': 771, 'B-advcl_5': 772, 'B-mark_26': 773, 'B-expl_26': 774, 'B-preconj_28': 775, 'B-nsubj_26': 776, 'B-cc_28': 777, 'B-conj_28': 778, 'B-dep_28': 779, 'B-nsubjpass_11': 780, 'B-auxpass_11': 781, 'B-nn_31': 782, 'B-vmod_14': 783, 'B-xcomp_13': 784, 'B-num_16': 785, 'B-expl_2': 786, 'I-amod_8': 787, 'B-advcl_2': 788, 'B-aux_22': 789, 'B-xcomp_20': 790, 'B-amod_26': 791, 'I-amod_26': 792, 'B-dobj_22': 793, 'I-amod_7': 794, 'B-mwe_14': 795, 'B-mark_33': 796, 'B-nsubjpass_33': 797, 'B-auxpass_33': 798, 'B-cop_39': 799, 'B-cop_16': 800, 'B-cc_20': 801, 'B-conj_20': 802, 'B-aux_9': 803, 'B-xcomp_7': 804, 'B-poss_15': 805, 'B-nsubjpass_22': 806, 'B-auxpass_22': 807, 'B-preconj_22': 808, 'B-advmod_22': 809, 'I-advmod_22': 810, 'B-advmod_25': 811, 'B-nsubj_14': 812, 'B-aux_14': 813, 'I-amod_17': 814, 'B-dobj_14': 815, 'B-rcmod_17': 816, 'B-xcomp_12': 817, 'B-csubjpass_38': 818, 'B-aux_38': 819, 'B-neg_38': 820, 'B-auxpass_38': 821, 'B-ccomp_6': 822, 'B-pcomp_18': 823, 'B-pcomp_25': 824, 'B-dobj_26': 825, 'B-nn_35': 826, 'B-nn_43': 827, 'I-nn_43': 828, 'B-ccomp_4': 829, 'B-advcl_14': 830, 'B-rcmod_6': 831, 'B-vmod_13': 832, 'B-mark_10': 833, 'B-nsubjpass_38': 834, 'I-nn_23': 835, 'B-dobj_11': 836, 'B-cc_31': 837, 'I-nn_35': 838, 'B-conj_31': 839, 'B-num_14': 840, 'B-appos_26': 841, 'B-nsubj_33': 842, 'B-cop_33': 843, 'B-rcmod_26': 844, 'B-advmod_39': 845, 'B-nn_39': 846, 'B-dep_39': 847, 'I-amod_19': 848, 'B-dep_21': 849, 'I-punct_3': 850, 'B-nsubj_29': 851, 'B-nsubj_38': 852, 'B-cop_38': 853, 'B-quantmod_9': 854, 'B-number_13': 855, 'B-xcomp_5': 856, 'B-advmod_6': 857, 'I-amod_20': 858, 'B-number_24': 859, 'B-number_19': 860, 'B-dep_26': 861, 'B-num_31': 862, 'B-nsubjpass_10': 863, 'B-auxpass_10': 864, 'B-prt_10': 865, 'B-appos_3': 866, 'B-xcomp_15': 867, 'I-amod_22': 868, 'B-mark_30': 869, 'B-nsubj_30': 870, 'B-advcl_15': 871, 'B-mark_15': 872, 'B-pcomp_21': 873, 'B-appos_20': 874, 'B-appos_17': 875, 'I-punct_17': 876, 'B-punct_26': 877, 'B-appos_24': 878, 'I-amod_31': 879, 'B-amod_40': 880, 'I-nn_44': 881, 'B-prep_51': 882, 'B-num_54': 883, 'B-amod_58': 884, 'B-advmod_2': 885, 'B-acomp_8': 886, 'B-npadvmod_7': 887, 'I-advmod_13': 888, 'B-pcomp_9': 889, 'B-parataxis_13': 890, 'B-ccomp_7': 891, 'B-parataxis_7': 892, 'B-number_23': 893, 'B-dep_31': 894, 'B-number_30': 895, 'B-pcomp_32': 896, 'B-acomp_33': 897, 'B-num_37': 898, 'B-dep_14': 899, 'B-vmod_26': 900, 'B-dobj_27': 901, 'B-punct_37': 902, 'B-prep_37': 903, 'B-num_40': 904, 'B-pobj_38': 905, 'B-pcomp_13': 906, 'B-nsubj_24': 907, 'B-nn_33': 908, 'B-aux_10': 909, 'B-xcomp_8': 910, 'B-vmod_15': 911, 'B-poss_18': 912, 'B-aux_31': 913, 'B-acomp_31': 914, 'B-punct_31': 915, 'B-quantmod_3': 916, 'B-aux_11': 917, 'B-pcomp_12': 918, 'I-punct_8': 919, 'B-number_5': 920, 'B-neg_9': 921, 'B-neg_8': 922, 'B-xcomp_10': 923, 'I-aux_6': 924, 'B-preconj_18': 925, 'I-amod_18': 926, 'B-parataxis_11': 927, 'B-advmod_28': 928, 'B-punct_28': 929, 'B-nsubj_28': 930, 'B-cop_28': 931, 'B-parataxis_12': 932, 'B-preconj_32': 933, 'I-nsubj_6': 934, 'B-number_14': 935, 'B-appos_5': 936, 'B-cc_34': 937, 'B-conj_34': 938, 'B-dep_34': 939, 'B-rcmod_3': 940, 'B-punct_22': 941, 'B-dobj_13': 942, 'B-nsubjpass_18': 943, 'B-aux_4': 944, 'B-quantmod_12': 945, 'B-auxpass_18': 946, 'I-amod_30': 947, 'B-appos_25': 948, 'B-nsubjpass_34': 949, 'B-vmod_3': 950, 'B-appos_13': 951, 'B-nsubj_25': 952, 'B-cop_25': 953, 'B-aux_34': 954, 'B-auxpass_34': 955, 'I-nn_14': 956, 'B-poss_19': 957, 'B-nsubj_22': 958, 'B-cop_22': 959, 'B-advmod_8': 960, 'B-mark_8': 961, 'B-cc_29': 962, 'B-conj_29': 963, 'B-num_20': 964, 'B-pcomp_23': 965, 'I-nn_28': 966, 'B-dobj_24': 967, 'B-num_30': 968, 'I-nsubj_5': 969, 'B-acomp_15': 970, 'B-mark_24': 971, 'B-ccomp_16': 972, 'B-cc_27': 973, 'B-conj_27': 974, 'B-dep_27': 975, 'B-det_33': 976, 'B-aux_29': 977, 'B-rcmod_28': 978, 'B-pcomp_1': 979, 'B-mark_29': 980, 'B-preconj_15': 981, 'B-cop_29': 982, 'B-mark_37': 983, 'B-nsubj_37': 984, 'B-dobj_37': 985, 'B-pcomp_40': 986, 'B-dobj_41': 987, 'B-aux_23': 988, 'B-vmod_32': 989, 'B-cc_36': 990, 'B-conj_36': 991, 'B-pobj_41': 992, 'B-cc_45': 993, 'B-conj_45': 994, 'I-cc_31': 995, 'B-advmod_38': 996, 'B-nsubjpass_41': 997, 'B-auxpass_41': 998, 'B-prep_41': 999, 'B-pobj_42': 1000, 'B-nn_47': 1001, 'B-punct_54': 1002, 'B-dep_52': 1003, 'B-amod_54': 1004, 'B-prep_55': 1005, 'B-pobj_56': 1006, 'I-cc_16': 1007, 'B-cop_8': 1008, 'B-mark_22': 1009, 'B-advcl_3': 1010, 'B-nsubj_31': 1011, 'B-advmod_31': 1012, 'B-vmod_34': 1013, 'B-mark_4': 1014, 'B-poss_14': 1015, 'B-mark_34': 1016, 'B-nsubj_34': 1017, 'B-cop_34': 1018, 'B-ccomp_17': 1019, 'I-amod_40': 1020, 'I-amod_29': 1021, 'B-appos_29': 1022, 'B-number_16': 1023, 'B-parataxis_8': 1024, 'B-number_7': 1025, 'B-mark_25': 1026, 'B-advcl_19': 1027, 'B-number_20': 1028, 'I-advmod_12': 1029, 'B-aux_30': 1030, 'B-dep_37': 1031, 'B-cc_37': 1032, 'B-conj_37': 1033, 'B-pcomp_37': 1034, 'B-dobj_38': 1035, 'B-punct_42': 1036, 'B-vmod_42': 1037, 'B-num_47': 1038, 'B-prep_40': 1039, 'I-punct_5': 1040, 'B-cc_26': 1041, 'I-nn_33': 1042, 'B-conj_26': 1043, 'B-punct_35': 1044, 'B-advmod_41': 1045, 'B-mwe_41': 1046, 'B-cc_43': 1047, 'I-nn_50': 1048, 'B-punct_48': 1049, 'B-conj_43': 1050, 'B-amod_52': 1051, 'B-prep_53': 1052, 'B-pobj_54': 1053, 'I-amod_11': 1054, 'I-amod_34': 1055, 'I-amod_16': 1056, 'B-csubj_15': 1057, 'B-parataxis_6': 1058, 'B-appos_22': 1059, 'B-quantmod_28': 1060, 'B-mwe_26': 1061, 'B-mark_5': 1062, 'B-npadvmod_12': 1063, 'B-acomp_16': 1064, 'B-number_31': 1065, 'B-punct_41': 1066, 'B-det_45': 1067, 'B-prep_45': 1068, 'B-num_50': 1069, 'B-advmod_49': 1070, 'B-appos_50': 1071, 'B-cc_52': 1072, 'B-conj_52': 1073, 'B-det_35': 1074, 'B-appos_28': 1075, 'B-vmod_35': 1076, 'B-det_42': 1077, 'B-nn_42': 1078, 'I-nn_42': 1079, 'B-punct_45': 1080, 'B-nn_45': 1081, 'B-dep_42': 1082, 'B-appos_45': 1083, 'B-dep_45': 1084, 'B-amod_45': 1085, 'B-neg_21': 1086, 'I-amod_21': 1087, 'B-expl_4': 1088, 'B-pcomp_5': 1089, 'B-neg_6': 1090, 'B-neg_5': 1091, 'I-dep_13': 1092, 'B-poss_21': 1093, 'B-vmod_21': 1094, 'I-dep_6': 1095, 'B-neg_17': 1096, 'B-num_9': 1097, 'B-xcomp_14': 1098, 'B-advcl_10': 1099, 'B-appos_15': 1100, 'B-number_3': 1101, 'I-dep_3': 1102, 'B-preconj_20': 1103, 'I-dep_20': 1104, 'I-nn_27': 1105, 'B-appos_23': 1106, 'B-neg_10': 1107, 'I-punct_10': 1108, 'B-cc_21': 1109, 'B-conj_21': 1110, 'B-parataxis_5': 1111, 'B-aux_27': 1112, 'B-xcomp_25': 1113, 'B-appos_14': 1114, 'B-xcomp_17': 1115, 'B-poss_20': 1116, 'B-poss_26': 1117, 'I-dep_31': 1118, 'B-poss_35': 1119, 'B-cc_35': 1120, 'B-conj_35': 1121, 'B-neg_20': 1122, 'B-nsubj_36': 1123, 'B-neg_36': 1124, 'B-appos_40': 1125, 'I-punct_37': 1126, 'B-appos_37': 1127, 'B-cop_49': 1128, 'B-rcmod_46': 1129, 'B-det_57': 1130, 'B-amod_57': 1131, 'B-nn_57': 1132, 'B-advcl_8': 1133, 'B-pcomp_26': 1134, 'B-mark_31': 1135, 'B-cop_31': 1136, 'B-advcl_23': 1137, 'B-advcl_26': 1138, 'B-preconj_16': 1139, 'B-cop_26': 1140, 'B-advmod_26': 1141, 'B-mark_35': 1142, 'B-nsubj_35': 1143, 'B-vmod_16': 1144, 'B-dobj_31': 1145, 'B-ccomp_8': 1146, 'I-nn_40': 1147, 'I-nn_30': 1148, 'B-dobj_34': 1149, 'B-det_48': 1150, 'I-amod_48': 1151, 'B-vmod_48': 1152, 'B-neg_13': 1153, 'B-vmod_7': 1154, 'B-poss_32': 1155, 'I-nn_20': 1156, 'B-nsubj_27': 1157, 'B-vmod_41': 1158, 'I-nn_53': 1159, 'B-appos_43': 1160, 'B-num_53': 1161, 'B-punct_56': 1162, 'B-appos_53': 1163, 'B-pobj_61': 1164, 'B-cc_62': 1165, 'B-conj_62': 1166, 'B-dep_62': 1167, 'B-quantmod_13': 1168, 'I-quantmod_13': 1169, 'I-cc_3': 1170, 'B-number_4': 1171, 'I-amod_13': 1172, 'B-cop_15': 1173, 'B-advmod_36': 1174, 'I-amod_39': 1175, 'B-parataxis_15': 1176, 'B-ccomp_25': 1177, 'B-num_34': 1178, 'B-poss_16': 1179, 'B-xcomp_11': 1180, 'I-amod_28': 1181, 'B-mark_28': 1182, 'I-nn_36': 1183, 'I-nn_21': 1184, 'B-possessive_32': 1185, 'I-nn_39': 1186, 'B-neg_19': 1187, 'B-aux_24': 1188, 'B-neg_7': 1189, 'B-mark_27': 1190, 'I-nn_26': 1191, 'B-nsubjpass_27': 1192, 'B-auxpass_27': 1193, 'I-nn_34': 1194, 'B-advcl_17': 1195, 'B-poss_17': 1196, 'B-pcomp_6': 1197, 'B-num_2': 1198, 'B-cc_24': 1199, 'B-number_28': 1200, 'B-conj_24': 1201, 'B-num_38': 1202, 'B-number_10': 1203, 'B-cop_14': 1204, 'I-amod_5': 1205, 'B-quantmod_10': 1206, 'B-pcomp_19': 1207, 'B-nsubjpass_25': 1208, 'B-aux_25': 1209, 'B-auxpass_25': 1210, 'B-nsubjpass_26': 1211, 'B-auxpass_26': 1212, 'B-rcmod_23': 1213, 'B-npadvmod_8': 1214, 'I-npadvmod_8': 1215, 'B-ccomp_12': 1216, 'I-punct_1': 1217, 'B-pcomp_11': 1218, 'B-appos_16': 1219, 'B-aux_12': 1220, 'B-vmod_17': 1221, 'B-nsubjpass_40': 1222, 'B-vmod_1': 1223, 'I-dep_33': 1224, 'I-dep_36': 1225, 'I-amod_36': 1226, 'B-aux_40': 1227, 'B-auxpass_40': 1228, 'B-punct_40': 1229, 'B-number_25': 1230, 'B-preconj_4': 1231, 'I-aux_13': 1232, 'B-quantmod_23': 1233, 'B-preconj_27': 1234, 'B-num_39': 1235, 'B-xcomp_9': 1236, 'B-advmod_27': 1237, 'B-acomp_25': 1238, 'B-poss_9': 1239, 'B-nsubjpass_23': 1240, 'B-auxpass_23': 1241, 'I-advmod_25': 1242, 'I-cc_23': 1243, 'I-cc_19': 1244, 'B-neg_4': 1245, 'I-pobj_9': 1246, 'B-mwe_18': 1247, 'B-advcl_31': 1248, 'B-dep_38': 1249, 'B-mwe_17': 1250, 'B-xcomp_6': 1251, 'I-nsubj_18': 1252, 'I-amod_25': 1253, 'B-ccomp_13': 1254, 'I-nn_38': 1255, 'B-ccomp_21': 1256, 'I-amod_35': 1257, 'B-appos_35': 1258, 'B-npadvmod_21': 1259, 'B-number_33': 1260, 'B-nsubjpass_42': 1261, 'B-auxpass_42': 1262, 'B-ccomp_15': 1263, 'B-appos_2': 1264, 'B-neg_34': 1265, 'B-cc_40': 1266, 'B-num_44': 1267, 'B-conj_40': 1268, 'B-neg_3': 1269, 'I-dep_28': 1270, 'B-neg_14': 1271, 'B-predet_27': 1272, 'B-pcomp_28': 1273, 'B-mark_42': 1274, 'B-aux_42': 1275, 'B-advcl_29': 1276, 'B-pobj_51': 1277, 'B-poss_22': 1278, 'B-nsubjpass_24': 1279, 'B-auxpass_24': 1280, 'B-appos_32': 1281, 'B-prep_42': 1282, 'B-pobj_43': 1283, 'B-neg_26': 1284, 'B-parataxis_9': 1285, 'B-advmod_30': 1286, 'I-advmod_29': 1287, 'B-advcl_20': 1288, 'B-npadvmod_18': 1289, 'I-npadvmod_18': 1290, 'B-nsubjpass_21': 1291, 'B-auxpass_21': 1292, 'B-rcmod_20': 1293, 'B-parataxis_3': 1294, 'B-npadvmod_23': 1295, 'I-npadvmod_23': 1296, 'B-cop_27': 1297, 'B-rcmod_10': 1298, 'B-advcl_12': 1299, 'B-aux_26': 1300, 'B-ccomp_20': 1301, 'B-aux_28': 1302, 'B-xcomp_26': 1303, 'B-npadvmod_17': 1304, 'B-xcomp_27': 1305, 'B-mwe_40': 1306, 'B-conj_41': 1307, 'B-cc_41': 1308, 'B-aux_33': 1309, 'I-amod_37': 1310, 'B-dobj_33': 1311, 'B-appos_34': 1312, 'B-vmod_28': 1313, 'B-neg_15': 1314, 'B-quantmod_19': 1315, 'I-quantmod_19': 1316, 'B-vmod_24': 1317, 'I-advmod_11': 1318, 'I-advmod_2': 1319, 'I-punct_24': 1320, 'I-advmod_7': 1321, 'B-mwe_13': 1322, 'I-advmod_21': 1323, 'B-preconj_24': 1324, 'B-nn_49': 1325, 'B-punct_49': 1326, 'B-appos_58': 1327, 'B-num_58': 1328, 'B-punct_58': 1329, 'B-neg_35': 1330, 'B-quantmod_5': 1331, 'B-mwe_3': 1332, 'B-preconj_12': 1333, 'B-auxpass_28': 1334, 'I-cc_9': 1335, 'B-num_35': 1336, 'B-parataxis_17': 1337, 'I-cc_13': 1338, 'B-ccomp_28': 1339, 'B-ccomp_14': 1340, 'B-quantmod_24': 1341, 'B-expl_6': 1342, 'B-vmod_23': 1343, 'B-rcmod_12': 1344, 'B-punct_39': 1345, 'B-appos_39': 1346, 'B-punct_44': 1347, 'B-num_48': 1348, 'I-punct_12': 1349, 'B-number_15': 1350, 'B-expl_14': 1351, 'B-neg_11': 1352, 'B-acomp_22': 1353, 'B-number_27': 1354, 'I-punct_11': 1355, 'B-dep_25': 1356, 'B-npadvmod_28': 1357, 'B-nsubj_40': 1358, 'B-parataxis_21': 1359, 'B-advmod_40': 1360, 'I-nn_48': 1361, 'B-advcl_25': 1362, 'B-advcl_7': 1363, 'B-xcomp_2': 1364, 'B-xcomp_23': 1365, 'B-number_36': 1366, 'B-mark_9': 1367, 'B-advcl_33': 1368, 'B-preconj_39': 1369, 'I-advmod_50': 1370, 'I-amod_53': 1371, 'B-appos_41': 1372, 'B-npadvmod_11': 1373, 'B-possessive_20': 1374, 'B-prt_3': 1375, 'B-mwe_11': 1376, 'B-aux_35': 1377, 'B-xcomp_33': 1378, 'B-preconj_37': 1379, 'B-possessive_37': 1380, 'B-dep_53': 1381, 'B-nsubj_53': 1382, 'B-poss_52': 1383, 'B-possessive_52': 1384, 'B-det_56': 1385, 'B-amod_56': 1386, 'B-det_62': 1387, 'B-amod_62': 1388, 'B-dep_74': 1389, 'B-punct_65': 1390, 'B-advmod_69': 1391, 'B-amod_69': 1392, 'B-nsubj_74': 1393, 'B-amod_72': 1394, 'B-aux_74': 1395, 'B-det_77': 1396, 'B-amod_77': 1397, 'B-dobj_74': 1398, 'B-prep_77': 1399, 'B-pobj_78': 1400, 'B-possessive_16': 1401, 'B-possessive_15': 1402, 'B-pcomp_10': 1403, 'B-xcomp_21': 1404, 'B-poss_27': 1405, 'B-possessive_27': 1406, 'B-mwe_27': 1407, 'B-preconj_29': 1408, 'B-mwe_8': 1409, 'B-poss_6': 1410, 'B-possessive_6': 1411, 'B-ccomp_18': 1412, 'B-ccomp_22': 1413, 'B-auxpass_29': 1414, 'B-num_1': 1415, 'B-advcl_11': 1416, 'B-poss_13': 1417, 'I-punct_23': 1418, 'B-pcomp_20': 1419, 'B-advmod_35': 1420, 'I-advmod_27': 1421, 'B-mark_38': 1422, 'B-mwe_20': 1423, 'I-punct_15': 1424, 'B-neg_40': 1425, 'B-advmod_48': 1426, 'B-mark_48': 1427, 'B-nsubj_48': 1428, 'B-advcl_38': 1429, 'B-det_50': 1430, 'B-nsubj_51': 1431, 'B-xcomp_48': 1432, 'B-cc_51': 1433, 'B-conj_51': 1434, 'B-dep_51': 1435, 'B-npadvmod_4': 1436, 'B-mwe_28': 1437, 'B-appos_33': 1438, 'B-neg_2': 1439, 'I-punct_21': 1440, 'I-cc_4': 1441, 'I-nsubj_3': 1442, 'B-poss_3': 1443, 'B-dep_47': 1444, 'B-punct_51': 1445, 'I-punct_22': 1446, 'B-mwe_7': 1447, 'B-vmod_25': 1448, 'B-poss_30': 1449, 'B-vmod_47': 1450, 'B-prep_48': 1451, 'B-poss_53': 1452, 'B-cc_49': 1453, 'B-conj_49': 1454, 'B-nn_58': 1455, 'I-nn_58': 1456, 'B-pobj_55': 1457, 'B-nn_62': 1458, 'I-nn_62': 1459, 'B-neg_22': 1460, 'I-dep_25': 1461, 'B-vmod_30': 1462, 'B-mark_43': 1463, 'B-poss_4': 1464, 'B-pcomp_14': 1465, 'B-preconj_9': 1466, 'I-advmod_4': 1467, 'B-nsubjpass_28': 1468, 'B-advcl_21': 1469, 'B-mwe_35': 1470, 'B-cc_46': 1471, 'B-conj_46': 1472, 'B-mwe_29': 1473, 'B-acomp_23': 1474, 'I-amod_32': 1475, 'B-pcomp_27': 1476, 'B-neg_25': 1477, 'B-ccomp_19': 1478, 'B-pcomp_17': 1479, 'B-poss_28': 1480, 'B-possessive_10': 1481, 'B-poss_43': 1482, 'I-amod_42': 1483, 'I-amod_43': 1484, 'B-cop_35': 1485, 'B-num_46': 1486, 'B-dep_44': 1487, 'B-preconj_3': 1488, 'B-num_33': 1489, 'I-amod_33': 1490, 'B-preconj_21': 1491, 'I-cc_21': 1492, 'B-poss_37': 1493, 'I-dep_14': 1494, 'B-pcomp_31': 1495, 'I-pobj_31': 1496, 'I-cc_2': 1497, 'B-xcomp_24': 1498, 'B-neg_32': 1499, 'B-ccomp_26': 1500, 'B-advcl_22': 1501, 'B-cop_30': 1502, 'B-preconj_17': 1503, 'B-preconj_8': 1504, 'B-xcomp_28': 1505, 'B-mark_39': 1506, 'B-advmod_42': 1507, 'B-neg_42': 1508, 'I-cc_12': 1509, 'B-pcomp_7': 1510, 'B-preconj_26': 1511, 'I-advmod_38': 1512, 'B-dobj_39': 1513, 'B-vmod_40': 1514, 'I-nn_45': 1515, 'B-rcmod_38': 1516, 'B-dobj_40': 1517, 'B-poss_33': 1518, 'B-nsubjpass_35': 1519, 'B-auxpass_35': 1520, 'I-prep_7': 1521, 'I-cc_17': 1522, 'B-ccomp_11': 1523, 'B-advmod_43': 1524, 'B-advcl_27': 1525, 'B-mwe_21': 1526, 'B-npadvmod_10': 1527, 'B-neg_29': 1528, 'B-rcmod_29': 1529, 'I-advmod_41': 1530, 'B-neg_47': 1531, 'B-dobj_44': 1532, 'B-poss_50': 1533, 'B-cop_32': 1534, 'I-punct_14': 1535, 'B-appos_21': 1536, 'B-appos_36': 1537, 'B-neg_27': 1538, 'B-advcl_37': 1539, 'B-nsubjpass_37': 1540, 'B-auxpass_37': 1541, 'B-neg_37': 1542, 'B-auxpass_46': 1543, 'B-amod_51': 1544, 'I-number_3': 1545, 'B-number_9': 1546, 'B-number_6': 1547, 'I-punct_29': 1548, 'I-punct_40': 1549, 'B-dep_41': 1550, 'I-advmod_9': 1551, 'I-ccomp_9': 1552, 'I-dep_18': 1553, 'B-preconj_30': 1554, 'I-dep_8': 1555, 'B-preconj_31': 1556, 'B-xcomp_36': 1557, 'B-mark_47': 1558, 'B-nsubj_47': 1559, 'B-cop_47': 1560, 'B-predet_9': 1561, 'B-npadvmod_9': 1562, 'I-advmod_15': 1563, 'B-advcl_32': 1564, 'B-npadvmod_19': 1565, 'B-aux_32': 1566, 'B-acomp_9': 1567, 'B-quantmod_32': 1568, 'B-pcomp_33': 1569, 'B-neg_31': 1570, 'B-possessive_18': 1571, 'I-dep_34': 1572, 'B-number_39': 1573, 'B-number_26': 1574, 'B-mark_40': 1575, 'B-ccomp_36': 1576, 'I-advmod_32': 1577, 'B-advmod_44': 1578, 'I-advmod_34': 1579, 'B-npadvmod_5': 1580, 'I-nsubj_8': 1581, 'B-advmod_37': 1582, 'B-npadvmod_25': 1583, 'B-xcomp_22': 1584, 'B-mwe_24': 1585, 'B-appos_51': 1586, 'B-punct_61': 1587, 'B-amod_61': 1588, 'B-appos_57': 1589, 'B-aux_39': 1590, 'B-ccomp_23': 1591, 'B-number_12': 1592, 'B-quantmod_22': 1593, 'B-possessive_9': 1594, 'B-possessive_17': 1595, 'B-cc_42': 1596, 'B-conj_42': 1597, 'B-preconj_51': 1598, 'B-num_51': 1599, 'B-dobj_47': 1600, 'B-mark_36': 1601, 'I-dep_11': 1602, 'B-acomp_13': 1603, 'I-dep_30': 1604, 'I-amod_54': 1605, 'B-appos_48': 1606, 'B-cc_54': 1607, 'B-conj_54': 1608, 'B-dobj_56': 1609, 'B-det_63': 1610, 'B-amod_63': 1611, 'B-nn_63': 1612, 'I-nn_37': 1613, 'B-aux_45': 1614, 'B-vmod_31': 1615, 'B-nn_50': 1616, 'B-det_55': 1617, 'B-advmod_54': 1618, 'B-ccomp_27': 1619, 'B-advcl_34': 1620, 'B-aux_46': 1621, 'B-vmod_44': 1622, 'B-mark_67': 1623, 'B-nn_60': 1624, 'B-nsubj_67': 1625, 'B-prep_60': 1626, 'B-prep_63': 1627, 'B-det_66': 1628, 'B-pobj_64': 1629, 'B-amod_70': 1630, 'B-nn_70': 1631, 'B-dobj_67': 1632, 'B-prep_67': 1633, 'B-det_73': 1634, 'B-pobj_71': 1635, 'B-prep_73': 1636, 'B-pobj_74': 1637, 'I-dep_21': 1638, 'B-mwe_23': 1639, 'B-npadvmod_14': 1640, 'B-mark_41': 1641, 'B-aux_41': 1642, 'B-ccomp_34': 1643, 'B-dobj_49': 1644, 'B-nsubjpass_29': 1645, 'B-possessive_4': 1646, 'B-preconj_5': 1647, 'B-npadvmod_16': 1648, 'I-conj_8': 1649, 'B-possessive_24': 1650, 'B-prt_5': 1651, 'B-number_2': 1652, 'B-nsubjpass_30': 1653, 'B-auxpass_30': 1654, 'B-nsubj_44': 1655, 'B-cop_44': 1656, 'B-cc_48': 1657, 'B-conj_48': 1658, 'B-number_48': 1659, 'B-dep_57': 1660, 'B-dep_58': 1661, 'B-dep_63': 1662, 'B-rcmod_36': 1663, 'B-advcl_24': 1664, 'I-advmod_14': 1665, 'B-parataxis_10': 1666, 'B-rcmod_21': 1667, 'B-expl_13': 1668, 'B-pcomp_3': 1669, 'B-npadvmod_43': 1670, 'B-npadvmod_13': 1671, 'B-cop_37': 1672, 'B-npadvmod_42': 1673, 'B-nsubj_45': 1674, 'B-rcmod_43': 1675, 'B-det_49': 1676, 'B-neg_41': 1677, 'B-poss_7': 1678, 'B-possessive_5': 1679, 'B-poss_10': 1680, 'I-amod_46': 1681, 'B-aux_44': 1682, 'B-xcomp_41': 1683, 'B-quantmod_16': 1684, 'B-predet_3': 1685, 'B-amod_1': 1686, 'B-neg_45': 1687, 'B-punct_60': 1688, 'I-amod_50': 1689, 'B-advmod_45': 1690, 'B-poss_31': 1691, 'I-num_20': 1692, 'B-mark_3': 1693, 'I-punct_32': 1694, 'B-cc_55': 1695, 'B-conj_55': 1696, 'I-punct_31': 1697, 'B-number_8': 1698, 'B-npadvmod_3': 1699, 'I-dep_29': 1700, 'I-punct_44': 1701, 'B-quantmod_51': 1702, 'B-number_51': 1703, 'B-prep_54': 1704, 'B-prep_61': 1705, 'B-nn_64': 1706, 'B-pobj_62': 1707, 'B-advmod_68': 1708, 'B-det_70': 1709, 'B-pobj_68': 1710, 'B-prep_70': 1711, 'B-amod_75': 1712, 'B-cc_72': 1713, 'B-conj_72': 1714, 'I-punct_78': 1715, 'B-dep_82': 1716, 'B-det_81': 1717, 'B-nsubj_82': 1718, 'B-det_86': 1719, 'B-nn_86': 1720, 'B-amod_86': 1721, 'B-dobj_82': 1722, 'B-punct_90': 1723, 'B-dep_103': 1724, 'B-nn_94': 1725, 'B-num_94': 1726, 'B-nsubj_103': 1727, 'B-vmod_94': 1728, 'B-amod_97': 1729, 'B-dobj_95': 1730, 'B-prep_97': 1731, 'B-nn_102': 1732, 'B-cc_99': 1733, 'B-conj_99': 1734, 'B-pobj_98': 1735, 'B-det_106': 1736, 'B-amod_106': 1737, 'B-dobj_103': 1738, 'B-prep_106': 1739, 'B-det_109': 1740, 'B-pobj_107': 1741, 'B-prep_109': 1742, 'B-det_114': 1743, 'B-npadvmod_113': 1744, 'B-amod_114': 1745, 'B-pobj_110': 1746, 'B-npadvmod_36': 1747, 'B-number_29': 1748, 'B-nsubj_50': 1749, 'I-nn_56': 1750, 'B-appos_52': 1751, 'B-dobj_50': 1752, 'B-prep_56': 1753, 'B-pobj_57': 1754, 'B-appos_59': 1755, 'B-vmod_59': 1756, 'B-nn_66': 1757, 'B-parataxis_29': 1758, 'B-pcomp_24': 1759, 'B-rcmod_31': 1760, 'B-nsubjpass_48': 1761, 'B-auxpass_48': 1762, 'B-parataxis_26': 1763, 'I-nn_47': 1764, 'I-dep_1': 1765, 'I-conj_6': 1766, 'B-mwe_22': 1767, 'B-expl_5': 1768, 'B-quantmod_7': 1769, 'B-expl_21': 1770, 'I-dep_24': 1771, 'I-dep_26': 1772, 'I-dep_27': 1773, 'B-preconj_6': 1774, 'I-conj_3': 1775, 'I-dep_10': 1776, 'I-cc_8': 1777, 'I-advmod_30': 1778, 'B-cc_50': 1779, 'I-nn_54': 1780, 'B-conj_50': 1781, 'B-dep_55': 1782, 'I-dep_55': 1783, 'I-punct_33': 1784, 'B-dep_67': 1785, 'B-punct_64': 1786, 'B-det_67': 1787, 'I-dep_17': 1788, 'B-xcomp_31': 1789, 'B-neg_46': 1790, 'B-npadvmod_22': 1791, 'I-npadvmod_22': 1792, 'B-mwe_25': 1793, 'I-conj_23': 1794, 'I-nn_49': 1795, 'B-num_49': 1796, 'B-acomp_18': 1797, 'I-dep_4': 1798, 'B-cc_44': 1799, 'B-conj_44': 1800, 'B-xcomp_30': 1801, 'I-amod_58': 1802, 'B-dobj_53': 1803, 'B-rcmod_25': 1804, 'I-amod_38': 1805, 'B-num_57': 1806, 'B-punct_59': 1807, 'B-num_64': 1808, 'B-cc_61': 1809, 'B-conj_61': 1810, 'B-advcl_49': 1811, 'B-rcmod_22': 1812, 'B-nsubjpass_44': 1813, 'B-auxpass_44': 1814, 'B-advcl_40': 1815, 'B-mwe_5': 1816, 'B-aux_37': 1817, 'I-aux_37': 1818, 'B-appos_42': 1819, 'B-possessive_49': 1820, 'B-vmod_43': 1821, 'B-poss_23': 1822, 'B-possessive_21': 1823, 'I-cc_20': 1824, 'I-punct_20': 1825, 'I-dep_15': 1826, 'B-pcomp_16': 1827, 'B-acomp_10': 1828, 'B-advmod_1': 1829, 'B-rcmod_33': 1830, 'B-quantmod_20': 1831, 'B-npadvmod_6': 1832, 'B-xcomp_29': 1833, 'B-advcl_39': 1834, 'B-num_43': 1835, 'B-npadvmod_44': 1836, 'B-xcomp_39': 1837, 'B-csubj_6': 1838, 'I-npadvmod_14': 1839, 'B-vmod_49': 1840, 'B-predet_22': 1841, 'I-punct_49': 1842, 'B-nsubjpass_49': 1843, 'B-auxpass_49': 1844, 'B-poss_54': 1845, 'B-aux_56': 1846, 'B-vmod_54': 1847, 'I-punct_51': 1848, 'B-nsubjpass_51': 1849, 'B-auxpass_51': 1850, 'I-dep_7': 1851, 'B-acomp_7': 1852, 'B-possessive_1': 1853, 'I-conj_13': 1854, 'B-mwe_15': 1855, 'B-vmod_38': 1856, 'B-mwe_54': 1857, 'I-prep_8': 1858, 'B-cop_41': 1859, 'B-xcomp_37': 1860, 'B-pcomp_29': 1861, 'B-npadvmod_20': 1862, 'B-nsubjpass_43': 1863, 'B-auxpass_43': 1864, 'B-npadvmod_30': 1865, 'B-npadvmod_24': 1866, 'B-poss_34': 1867, 'B-possessive_28': 1868, 'B-possessive_26': 1869, 'I-prep_14': 1870, 'B-nsubj_41': 1871, 'B-tmod_7': 1872, 'B-cop_53': 1873, 'B-advmod_53': 1874, 'B-cc_57': 1875, 'B-conj_57': 1876, 'B-cc_59': 1877, 'B-conj_59': 1878, 'B-punct_66': 1879, 'B-num_75': 1880, 'B-cc_68': 1881, 'B-conj_68': 1882, 'B-cc_70': 1883, 'B-conj_70': 1884, 'B-dep_66': 1885, 'B-prep_75': 1886, 'B-num_79': 1887, 'B-nn_79': 1888, 'B-pobj_76': 1889, 'I-punct_53': 1890, 'I-dep_37': 1891, 'B-cop_48': 1892, 'B-parataxis_22': 1893, 'B-punct_57': 1894, 'I-punct_48': 1895, 'I-prep_4': 1896, 'I-npadvmod_20': 1897, 'B-quantmod_8': 1898, 'B-tmod_5': 1899, 'B-acomp_3': 1900, 'B-neg_30': 1901, 'I-dep_19': 1902, 'I-punct_25': 1903, 'I-advmod_17': 1904, 'B-expl_22': 1905, 'B-npadvmod_15': 1906, 'B-ccomp_30': 1907, 'B-xcomp_35': 1908, 'B-num_45': 1909, 'B-rcmod_32': 1910, 'I-advmod_8': 1911, 'I-advmod_6': 1912, 'B-neg_24': 1913, 'B-quantmod_27': 1914, 'I-quantmod_27': 1915, 'B-rcmod_37': 1916, 'I-npadvmod_36': 1917, 'I-dep_35': 1918, 'B-dep_49': 1919, 'I-dep_22': 1920, 'I-npadvmod_19': 1921, 'B-npadvmod_38': 1922, 'B-nsubj_46': 1923, 'B-cop_46': 1924, 'B-pcomp_42': 1925, 'B-expl_10': 1926, 'I-advmod_26': 1927, 'B-csubj_14': 1928, 'I-cc_1': 1929, 'B-quantmod_21': 1930, 'B-csubj_9': 1931, 'I-dep_16': 1932, 'B-preconj_14': 1933, 'I-npadvmod_24': 1934, 'I-cc_14': 1935, 'B-predet_5': 1936, 'B-npadvmod_27': 1937, 'B-amod_49': 1938, 'B-dep_46': 1939, 'B-acomp_11': 1940, 'I-cc_38': 1941, 'B-nsubj_54': 1942, 'B-dobj_54': 1943, 'B-csubj_19': 1944, 'B-number_42': 1945, 'B-tmod_39': 1946, 'B-prt_29': 1947, 'B-ccomp_37': 1948, 'B-aux_48': 1949, 'I-advmod_20': 1950, 'B-npadvmod_26': 1951, 'B-predet_7': 1952, 'B-cop_42': 1953, 'I-nsubj_40': 1954, 'B-nn_56': 1955, 'B-advmod_63': 1956, 'B-det_61': 1957, 'I-amod_61': 1958, 'B-nsubjpass_63': 1959, 'B-auxpass_63': 1960, 'B-advcl_47': 1961, 'B-npadvmod_67': 1962, 'I-dep_23': 1963, 'B-npadvmod_2': 1964, 'B-prt_21': 1965, 'I-conj_12': 1966, 'B-poss_36': 1967, 'B-expl_17': 1968, 'I-dep_9': 1969, 'B-num_42': 1970, 'B-parataxis_16': 1971, 'B-mwe_51': 1972, 'B-quantmod_52': 1973, 'B-cop_40': 1974, 'B-quantmod_39': 1975, 'B-mwe_46': 1976, 'B-quantmod_47': 1977, 'I-conj_29': 1978, 'B-num_59': 1979, 'B-nn_59': 1980, 'B-dep_64': 1981, 'B-mwe_63': 1982, 'B-quantmod_64': 1983, 'B-mwe_38': 1984, 'B-npadvmod_34': 1985, 'I-aux_17': 1986, 'B-vmod_33': 1987, 'I-dep_53': 1988, 'B-dep_60': 1989, 'I-dep_60': 1990, 'B-num_62': 1991, 'B-nn_83': 1992, 'B-amod_66': 1993, 'B-appos_62': 1994, 'I-punct_62': 1995, 'B-num_70': 1996, 'B-amod_74': 1997, 'I-amod_74': 1998, 'B-appos_70': 1999, 'B-num_78': 2000, 'B-punct_81': 2001, 'B-amod_81': 2002, 'B-appos_78': 2003, 'B-pobj_60': 2004, 'B-det_87': 2005, 'B-nn_87': 2006, 'B-pobj_84': 2007, 'B-advcl_35': 2008, 'I-conj_7': 2009, 'B-mwe_9': 2010, 'I-cc_24': 2011, 'B-parataxis_14': 2012, 'B-dobj_43': 2013, 'B-npadvmod_53': 2014, 'I-npadvmod_53': 2015, 'B-advcl_28': 2016, 'B-mwe_36': 2017, 'B-ccomp_31': 2018, 'B-mark_45': 2019, 'B-dobj_45': 2020, 'I-amod_47': 2021, 'I-amod_51': 2022, 'B-acomp_12': 2023, 'I-advmod_3': 2024, 'B-quantmod_4': 2025, 'B-tmod_6': 2026, 'I-xcomp_11': 2027, 'B-quantmod_17': 2028, 'B-quantmod_34': 2029, 'B-quantmod_26': 2030, 'B-neg_28': 2031, 'B-number_22': 2032, 'I-cc_6': 2033, 'B-npadvmod_29': 2034, 'B-advmod_47': 2035, 'B-pcomp_53': 2036, 'B-mwe_37': 2037, 'B-quantmod_38': 2038, 'B-det_59': 2039, 'B-pobj_73': 2040, 'B-mwe_6': 2041, 'B-parataxis_23': 2042, 'B-cop_36': 2043, 'I-prep_19': 2044, 'B-possessive_11': 2045, 'B-quantmod_36': 2046, 'B-neg_43': 2047, 'B-mwe_2': 2048, 'I-npadvmod_6': 2049, 'B-preconj_34': 2050, 'B-npadvmod_33': 2051, 'I-npadvmod_33': 2052, 'I-npadvmod_38': 2053, 'B-predet_10': 2054, 'B-xcomp_32': 2055, 'B-npadvmod_50': 2056, 'I-npadvmod_50': 2057, 'B-preconj_10': 2058, 'I-advmod_16': 2059, 'B-quantmod_41': 2060, 'B-mwe_39': 2061, 'B-mwe_1': 2062, 'I-conj_15': 2063, 'B-acomp_29': 2064, 'B-mwe_19': 2065, 'I-advmod_46': 2066, 'B-ccomp_43': 2067, 'B-advcl_36': 2068, 'I-punct_36': 2069, 'B-prt_18': 2070, 'B-prt_12': 2071, 'B-acomp_6': 2072, 'B-xcomp_34': 2073, 'B-dobj_48': 2074, 'B-det_54': 2075, 'B-acomp_55': 2076, 'B-mwe_58': 2077, 'B-det_60': 2078, 'I-aux_15': 2079, 'B-possessive_12': 2080, 'I-aux_44': 2081, 'I-cc_15': 2082, 'B-predet_6': 2083, 'I-quantmod_7': 2084, 'B-predet_31': 2085, 'I-dep_38': 2086, 'B-nsubj_64': 2087, 'B-amod_67': 2088, 'B-dobj_64': 2089, 'B-nsubj_71': 2090, 'B-rcmod_69': 2091, 'B-nn_74': 2092, 'I-nn_74': 2093, 'B-dobj_71': 2094, 'I-npadvmod_10': 2095, 'I-advmod_19': 2096, 'I-punct_38': 2097, 'B-dobj_51': 2098, 'I-dep_39': 2099, 'B-acomp_36': 2100, 'I-num_22': 2101, 'B-num_56': 2102, 'B-pcomp_41': 2103, 'B-nsubjpass_45': 2104, 'B-auxpass_45': 2105, 'B-rcmod_41': 2106, 'B-advmod_50': 2107, 'B-nsubjpass_50': 2108, 'B-auxpass_50': 2109, 'B-advcl_45': 2110, 'B-nsubj_55': 2111, 'B-dobj_55': 2112, 'B-nsubj_60': 2113, 'B-parataxis_55': 2114, 'B-dobj_60': 2115, 'B-cc_65': 2116, 'B-conj_65': 2117, 'I-cc_7': 2118, 'B-parataxis_40': 2119, 'I-punct_28': 2120, 'B-parataxis_39': 2121, 'B-parataxis_18': 2122, 'B-parataxis_44': 2123, 'B-parataxis_20': 2124, 'B-acomp_4': 2125, 'B-csubj_10': 2126, 'B-prt_28': 2127, 'B-advmod_52': 2128, 'B-nsubj_52': 2129, 'B-aux_52': 2130, 'B-rcmod_48': 2131, 'B-dobj_52': 2132, 'I-advmod_18': 2133, 'B-punct_62': 2134, 'B-appos_60': 2135, 'B-expl_3': 2136, 'B-rcmod_34': 2137, 'B-preconj_40': 2138, 'I-amod_45': 2139, 'I-npadvmod_3': 2140, 'I-nsubj_11': 2141, 'B-appos_47': 2142, 'B-quantmod_15': 2143, 'B-mwe_33': 2144, 'I-mark_15': 2145, 'B-preconj_35': 2146, 'I-amod_41': 2147, 'I-prep_11': 2148, 'B-npadvmod_41': 2149, 'I-cc_18': 2150, 'B-preconj_23': 2151, 'B-npadvmod_37': 2152, 'I-npadvmod_37': 2153, 'I-npadvmod_43': 2154, 'I-npadvmod_27': 2155, 'B-nsubj_59': 2156, 'I-nn_57': 2157, 'B-cop_59': 2158, 'B-aux_61': 2159, 'B-xcomp_59': 2160, 'I-nn_64': 2161, 'B-dobj_61': 2162, 'B-dep_92': 2163, 'B-punct_72': 2164, 'B-nsubj_92': 2165, 'I-amod_81': 2166, 'B-nn_81': 2167, 'B-prep_82': 2168, 'I-nn_87': 2169, 'B-pobj_83': 2170, 'B-prep_87': 2171, 'B-nn_91': 2172, 'I-nn_91': 2173, 'B-pobj_88': 2174, 'B-poss_94': 2175, 'B-dobj_92': 2176, 'B-aux_96': 2177, 'B-nn_99': 2178, 'I-nn_99': 2179, 'B-dobj_96': 2180, 'B-dep_111': 2181, 'B-punct_102': 2182, 'B-nsubj_111': 2183, 'B-nn_110': 2184, 'I-nn_110': 2185, 'B-poss_115': 2186, 'B-amod_115': 2187, 'I-amod_115': 2188, 'B-dobj_111': 2189, 'B-cc_111': 2190, 'B-conj_111': 2191, 'B-dobj_117': 2192, 'B-nsubjpass_121': 2193, 'B-auxpass_121': 2194, 'B-rcmod_118': 2195, 'B-prep_121': 2196, 'B-npadvmod_125': 2197, 'I-npadvmod_125': 2198, 'B-amod_126': 2199, 'B-pobj_122': 2200, 'B-prep_126': 2201, 'B-nn_133': 2202, 'I-nn_133': 2203, 'B-pobj_127': 2204, 'B-number_17': 2205, 'I-dep_2': 2206, 'B-vmod_37': 2207, 'B-ccomp_33': 2208, 'B-predet_13': 2209, 'B-prt_37': 2210, 'B-ccomp_40': 2211, 'B-quantmod_48': 2212, 'B-num_67': 2213, 'B-punct_67': 2214, 'I-mark_8': 2215, 'B-advcl_30': 2216, 'I-advmod_23': 2217, 'B-neg_51': 2218, 'B-quantmod_30': 2219, 'B-npadvmod_31': 2220, 'I-npadvmod_31': 2221, 'B-poss_39': 2222, 'B-pcomp_30': 2223, 'B-aux_47': 2224, 'I-prep_5': 2225, 'B-mark_46': 2226, 'I-advmod_28': 2227, 'B-number_34': 2228, 'B-rcmod_44': 2229, 'B-number_52': 2230, 'B-num_60': 2231, 'B-num_55': 2232, 'I-amod_60': 2233, 'B-number_65': 2234, 'B-quantmod_65': 2235, 'B-num_68': 2236, 'B-nn_68': 2237, 'B-ccomp_29': 2238, 'B-poss_51': 2239, 'B-vmod_55': 2240, 'B-acomp_21': 2241, 'B-neg_33': 2242, 'I-mwe_12': 2243, 'I-cc_28': 2244, 'I-dep_12': 2245, 'I-aux_23': 2246, 'B-preconj_43': 2247, 'I-dep_5': 2248, 'I-punct_26': 2249, 'I-cc_30': 2250, 'B-possessive_7': 2251, 'I-punct_39': 2252, 'I-punct_30': 2253, 'B-possessive_13': 2254, 'B-nsubjpass_47': 2255, 'B-possessive_33': 2256, 'B-auxpass_47': 2257, 'B-parataxis_31': 2258, 'B-parataxis_46': 2259, 'I-dep_44': 2260, 'B-quantmod_49': 2261, 'B-quantmod_60': 2262, 'B-possessive_8': 2263, 'B-expl_7': 2264, 'I-npadvmod_12': 2265, 'I-npadvmod_16': 2266, 'I-aux_7': 2267, 'B-neg_50': 2268, 'B-acomp_37': 2269, 'B-predet_32': 2270, 'I-advmod_35': 2271, 'B-acomp_28': 2272, 'B-expl_8': 2273, 'I-npadvmod_15': 2274, 'I-cc_10': 2275, 'B-possessive_2': 2276, 'B-mark_50': 2277, 'B-vmod_45': 2278, 'B-ccomp_39': 2279, 'B-preconj_55': 2280, 'B-prep_65': 2281, 'B-pobj_66': 2282, 'B-vmod_67': 2283, 'B-dobj_68': 2284, 'B-npadvmod_48': 2285, 'B-npadvmod_35': 2286, 'B-aux_49': 2287, 'I-amod_52': 2288, 'I-dep_49': 2289, 'B-vmod_39': 2290, 'I-conj_27': 2291, 'B-cop_45': 2292, 'B-cop_51': 2293, 'B-prep_59': 2294, 'B-parataxis_45': 2295, 'B-pcomp_60': 2296, 'B-advmod_65': 2297, 'B-quantmod_11': 2298, 'I-cc_26': 2299, 'I-npadvmod_17': 2300, 'B-pcomp_38': 2301, 'B-prt_11': 2302, 'B-poss_40': 2303, 'B-pcomp_4': 2304, 'I-advmod_5': 2305, 'B-npadvmod_39': 2306, 'B-nsubjpass_53': 2307, 'B-aux_53': 2308, 'B-auxpass_53': 2309, 'B-aux_50': 2310, 'B-xcomp_43': 2311, 'I-number_7': 2312, 'B-rcmod_39': 2313, 'B-mark_44': 2314, 'B-aux_57': 2315, 'B-mark_88': 2316, 'B-nsubj_88': 2317, 'I-dep_67': 2318, 'I-dep_63': 2319, 'B-cc_67': 2320, 'B-dep_70': 2321, 'I-dep_71': 2322, 'B-conj_67': 2323, 'B-pobj_72': 2324, 'I-nn_81': 2325, 'B-dep_85': 2326, 'B-advmod_86': 2327, 'B-amod_87': 2328, 'B-pobj_82': 2329, 'B-ccomp_57': 2330, 'B-amod_90': 2331, 'B-dobj_88': 2332, 'B-prep_88': 2333, 'B-det_94': 2334, 'B-amod_94': 2335, 'B-pobj_91': 2336, 'B-punct_88': 2337, 'B-advmod_97': 2338, 'B-advcl_88': 2339, 'B-prep_99': 2340, 'B-det_105': 2341, 'B-amod_105': 2342, 'B-cc_102': 2343, 'B-conj_102': 2344, 'B-pobj_100': 2345, 'B-punct_109': 2346, 'B-dep_108': 2347, 'I-dep_109': 2348, 'I-dep_105': 2349, 'B-cc_109': 2350, 'B-dep_112': 2351, 'I-dep_113': 2352, 'B-conj_109': 2353, 'B-prep_105': 2354, 'B-det_120': 2355, 'B-nn_120': 2356, 'I-nn_120': 2357, 'B-pobj_115': 2358, 'I-nsubj_10': 2359, 'I-advmod_33': 2360, 'I-advmod_24': 2361, 'B-poss_38': 2362, 'B-predet_12': 2363, 'I-conj_4': 2364, 'B-expl_23': 2365, 'B-pcomp_36': 2366, 'I-punct_46': 2367, 'I-dobj_7': 2368, 'B-parataxis_47': 2369, 'B-dobj_59': 2370, 'I-mark_28': 2371, 'I-nsubj_20': 2372, 'I-npadvmod_9': 2373, 'B-prep_62': 2374, 'B-nsubjpass_62': 2375, 'B-auxpass_62': 2376, 'B-neg_62': 2377, 'I-nn_68': 2378, 'B-pobj_63': 2379, 'I-punct_71': 2380, 'B-punct_71': 2381, 'B-prep_74': 2382, 'B-det_78': 2383, 'B-pobj_75': 2384, 'B-prep_78': 2385, 'B-pobj_79': 2386, 'B-advmod_81': 2387, 'I-punct_86': 2388, 'B-dep_94': 2389, 'B-punct_86': 2390, 'B-nsubj_94': 2391, 'B-prep_89': 2392, 'B-pobj_90': 2393, 'B-cc_91': 2394, 'B-conj_91': 2395, 'B-det_99': 2396, 'B-amod_99': 2397, 'B-dobj_94': 2398, 'B-det_102': 2399, 'B-cc_104': 2400, 'I-cc_99': 2401, 'B-det_108': 2402, 'B-amod_108': 2403, 'B-nn_108': 2404, 'B-prep_108': 2405, 'B-pobj_109': 2406, 'B-punct_108': 2407, 'B-nsubjpass_114': 2408, 'B-auxpass_114': 2409, 'B-rcmod_108': 2410, 'B-prep_114': 2411, 'I-punct_123': 2412, 'B-dep_130': 2413, 'B-punct_123': 2414, 'B-nn_126': 2415, 'B-nsubj_130': 2416, 'B-nn_129': 2417, 'B-det_135': 2418, 'B-amod_135': 2419, 'B-nn_135': 2420, 'I-nn_135': 2421, 'B-dobj_130': 2422, 'B-punct_135': 2423, 'B-nsubj_138': 2424, 'B-rcmod_135': 2425, 'B-amod_140': 2426, 'B-dobj_138': 2427, 'B-prep_140': 2428, 'B-pobj_141': 2429, 'B-punct_146': 2430, 'B-dep_158': 2431, 'B-mark_149': 2432, 'B-advcl_158': 2433, 'B-prep_149': 2434, 'B-dep_154': 2435, 'B-nn_154': 2436, 'I-nn_154': 2437, 'B-pobj_150': 2438, 'B-punct_158': 2439, 'B-amod_157': 2440, 'B-nsubj_158': 2441, 'B-nn_163': 2442, 'I-nn_163': 2443, 'B-cc_160': 2444, 'B-conj_160': 2445, 'B-dobj_158': 2446, 'B-ccomp_35': 2447, 'B-acomp_20': 2448, 'B-rcmod_42': 2449, 'B-cop_50': 2450, 'I-punct_42': 2451, 'B-number_18': 2452, 'I-punct_59': 2453, 'B-punct_69': 2454, 'B-num_69': 2455, 'B-dep_69': 2456, 'B-cc_73': 2457, 'B-conj_73': 2458, 'B-punct_80': 2459, 'B-dep_86': 2460, 'B-dep_83': 2461, 'B-prep_86': 2462, 'B-nn_90': 2463, 'B-pobj_87': 2464, 'B-punct_94': 2465, 'B-amod_96': 2466, 'B-dep_97': 2467, 'B-amod_98': 2468, 'B-num_101': 2469, 'B-dep_99': 2470, 'B-amod_101': 2471, 'B-dep_102': 2472, 'B-mark_113': 2473, 'B-nsubj_113': 2474, 'B-punct_106': 2475, 'B-conj_106': 2476, 'B-cc_106': 2477, 'B-aux_113': 2478, 'B-neg_113': 2479, 'B-rcmod_94': 2480, 'B-advmod_113': 2481, 'B-number_40': 2482, 'B-number_45': 2483, 'I-nsubjpass_19': 2484, 'B-neg_44': 2485, 'I-aux_19': 2486, 'I-amod_49': 2487, 'B-predet_19': 2488, 'I-nsubj_28': 2489, 'I-conj_17': 2490, 'I-npadvmod_13': 2491, 'I-mark_10': 2492, 'B-csubj_22': 2493, 'B-acomp_14': 2494, 'B-vmod_53': 2495, 'B-prep_57': 2496, 'I-punct_64': 2497, 'B-det_69': 2498, 'I-amod_69': 2499, 'I-mark_13': 2500, 'B-nsubjpass_46': 2501, 'B-preconj_25': 2502, 'I-dep_52': 2503, 'B-quantmod_57': 2504, 'B-parataxis_48': 2505, 'B-possessive_23': 2506, 'B-possessive_14': 2507, 'B-acomp_27': 2508, 'B-advmod_67': 2509, 'I-dep_40': 2510, 'I-dep_46': 2511, 'B-npadvmod_45': 2512, 'B-nsubjpass_54': 2513, 'B-auxpass_54': 2514, 'B-det_58': 2515, 'B-predet_18': 2516, 'B-appos_44': 2517, 'B-pcomp_49': 2518, 'B-poss_59': 2519, 'I-cc_36': 2520, 'I-nn_59': 2521, 'I-npadvmod_25': 2522, 'I-punct_41': 2523, 'B-appos_54': 2524, 'I-quantmod_6': 2525, 'I-quantmod_15': 2526, 'I-quantmod_26': 2527, 'I-cc_25': 2528, 'B-preconj_36': 2529, 'B-preconj_47': 2530, 'B-mwe_44': 2531, 'B-quantmod_45': 2532, 'B-pcomp_35': 2533, 'B-acomp_2': 2534, 'B-dep_54': 2535, 'B-amod_65': 2536, 'I-aux_8': 2537, 'I-nsubj_4': 2538, 'B-pcomp_39': 2539, 'I-prep_20': 2540, 'B-nsubj_56': 2541, 'B-preconj_60': 2542, 'B-cc_60': 2543, 'B-conj_60': 2544, 'B-advmod_56': 2545, 'B-num_52': 2546, 'B-expl_28': 2547, 'B-csubj_5': 2548, 'B-cop_52': 2549, 'B-advmod_51': 2550, 'B-amod_64': 2551, 'I-amod_64': 2552, 'B-appos_66': 2553, 'B-amod_79': 2554, 'I-amod_79': 2555, 'B-dep_77': 2556, 'B-appos_79': 2557, 'B-prep_79': 2558, 'I-amod_86': 2559, 'B-cc_86': 2560, 'B-conj_86': 2561, 'B-det_91': 2562, 'B-appos_88': 2563, 'B-amod_91': 2564, 'B-prep_92': 2565, 'B-det_98': 2566, 'B-nn_98': 2567, 'B-num_98': 2568, 'B-pobj_93': 2569, 'B-punct_79': 2570, 'B-poss_41': 2571, 'B-pcomp_54': 2572, 'B-prt_22': 2573, 'B-iobj_8': 2574, 'B-iobj_28': 2575, 'B-tmod_4': 2576, 'B-mwe_31': 2577, 'B-possessive_19': 2578, 'I-quantmod_3': 2579, 'B-possessive_34': 2580, 'B-nn_69': 2581, 'B-advmod_59': 2582, 'B-csubj_12': 2583, 'B-poss_55': 2584, 'B-dobj_57': 2585, 'B-prt_32': 2586, 'B-acomp_50': 2587, 'B-mark_53': 2588, 'I-nn_61': 2589, 'I-mark_19': 2590, 'I-mark_44': 2591, 'B-aux_59': 2592, 'B-xcomp_56': 2593, 'B-iobj_38': 2594, 'I-advmod_42': 2595, 'I-nsubj_14': 2596, 'B-expl_30': 2597, 'B-predet_15': 2598, 'B-prt_27': 2599, 'B-mark_51': 2600, 'B-advcl_51': 2601, 'I-nn_63': 2602, 'B-acomp_19': 2603, 'B-xcomp_38': 2604, 'B-aux_51': 2605, 'B-xcomp_49': 2606, 'B-mwe_45': 2607, 'B-prt_16': 2608, 'B-rcmod_51': 2609, 'I-conj_44': 2610, 'B-ccomp_32': 2611, 'B-csubj_18': 2612, 'B-expl_16': 2613, 'B-advcl_43': 2614, 'I-punct_43': 2615, 'B-number_43': 2616, 'B-xcomp_46': 2617, 'B-iobj_4': 2618, 'B-acomp_5': 2619, 'B-csubjpass_15': 2620, 'B-number_11': 2621, 'B-xcomp_45': 2622, 'B-number_49': 2623, 'I-prep_26': 2624, 'I-nn_60': 2625, 'B-det_65': 2626, 'B-nn_65': 2627, 'I-mark_11': 2628, 'B-quantmod_31': 2629, 'I-quantmod_31': 2630, 'B-preconj_45': 2631, 'B-advmod_55': 2632, 'B-cc_56': 2633, 'B-conj_56': 2634, 'I-cc_22': 2635, 'B-nn_73': 2636, 'B-punct_73': 2637, 'B-nn_77': 2638, 'B-punct_77': 2639, 'B-nn_82': 2640, 'B-punct_82': 2641, 'B-num_84': 2642, 'B-num_89': 2643, 'B-punct_89': 2644, 'I-quantmod_10': 2645, 'B-quantmod_33': 2646, 'I-quantmod_11': 2647, 'I-prep_29': 2648, 'B-cop_55': 2649, 'B-npadvmod_55': 2650, 'B-rcmod_49': 2651, 'B-aux_62': 2652, 'B-dobj_62': 2653, 'B-iobj_14': 2654, 'B-ccomp_45': 2655, 'B-mwe_62': 2656, 'B-csubj_8': 2657, 'B-appos_49': 2658, 'I-pobj_40': 2659, 'B-iobj_6': 2660, 'B-advmod_57': 2661, 'B-expl_18': 2662, 'I-mark_26': 2663, 'B-mwe_49': 2664, 'B-mark_59': 2665, 'B-ccomp_48': 2666, 'B-poss_42': 2667, 'B-csubj_16': 2668, 'I-cc_27': 2669, 'I-conj_11': 2670, 'I-conj_18': 2671, 'B-pcomp_52': 2672, 'B-det_64': 2673, 'B-cc_64': 2674, 'B-conj_64': 2675, 'B-mark_64': 2676, 'B-poss_67': 2677, 'B-det_71': 2678, 'B-quantmod_2': 2679, 'I-dobj_9': 2680, 'I-advmod_40': 2681, 'I-mwe_6': 2682, 'I-npadvmod_11': 2683, 'B-xcomp_44': 2684, 'B-expl_27': 2685, 'I-punct_57': 2686, 'B-nsubj_63': 2687, 'B-rcmod_57': 2688, 'B-nn_67': 2689, 'B-dobj_63': 2690, 'B-preconj_46': 2691, 'I-pobj_2': 2692, 'I-npadvmod_7': 2693, 'B-poss_45': 2694, 'B-parataxis_25': 2695, 'I-aux_18': 2696, 'B-acomp_32': 2697, 'B-poss_44': 2698, 'B-possessive_42': 2699, 'B-mark_60': 2700, 'B-vmod_51': 2701, 'B-poss_58': 2702, 'B-possessive_58': 2703, 'B-cop_60': 2704, 'B-ccomp_46': 2705, 'B-mwe_43': 2706, 'B-ccomp_1': 2707, 'B-nsubjpass_60': 2708, 'B-auxpass_60': 2709, 'B-xcomp_60': 2710, 'I-amod_56': 2711, 'B-appos_56': 2712, 'B-punct_70': 2713, 'B-appos_68': 2714, 'B-possessive_3': 2715, 'B-det_74': 2716, 'B-preconj_38': 2717, 'B-predet_16': 2718, 'I-aux_12': 2719, 'I-punct_35': 2720, 'B-pcomp_58': 2721, 'I-amod_71': 2722, 'B-nn_71': 2723, 'I-nn_71': 2724, 'I-advmod_31': 2725, 'B-xcomp_40': 2726, 'I-npadvmod_21': 2727, 'I-pobj_11': 2728, 'B-rcmod_47': 2729, 'B-csubj_28': 2730, 'B-xcomp_42': 2731, 'I-npadvmod_32': 2732, 'I-npadvmod_48': 2733, 'I-nn_69': 2734, 'B-preconj_48': 2735, 'I-quantmod_32': 2736, 'I-dep_74': 2737, 'B-mark_80': 2738, 'B-nsubj_80': 2739, 'B-dobj_80': 2740, 'B-advmod_102': 2741, 'B-mark_102': 2742, 'B-nsubj_102': 2743, 'B-dep_93': 2744, 'B-cc_97': 2745, 'I-cc_94': 2746, 'B-conj_94': 2747, 'B-cc_98': 2748, 'B-conj_98': 2749, 'B-dobj_102': 2750, 'B-cc_103': 2751, 'B-nn_106': 2752, 'B-conj_103': 2753, 'B-prep_103': 2754, 'B-nn_109': 2755, 'B-nn_114': 2756, 'B-advmod_118': 2757, 'B-dobj_118': 2758, 'B-acomp_24': 2759, 'B-neg_49': 2760, 'B-number_53': 2761, 'B-number_62': 2762, 'B-cc_63': 2763, 'B-number_66': 2764, 'B-conj_63': 2765, 'B-appos_69': 2766, 'B-nn_75': 2767, 'B-vmod_75': 2768, 'B-prep_76': 2769, 'B-number_79': 2770, 'B-amod_80': 2771, 'B-pobj_77': 2772, 'B-cc_80': 2773, 'B-number_83': 2774, 'B-amod_84': 2775, 'B-conj_80': 2776, 'B-mark_58': 2777, 'B-vmod_60': 2778, 'B-advmod_64': 2779, 'B-advmod_62': 2780, 'B-vmod_58': 2781, 'B-number_21': 2782, 'B-cop_2': 2783, 'B-pcomp_45': 2784, 'B-aux_54': 2785, 'I-punct_34': 2786, 'B-nsubjpass_67': 2787, 'B-auxpass_67': 2788, 'I-aux_26': 2789, 'I-dep_42': 2790, 'I-dep_43': 2791, 'B-mark_54': 2792, 'B-neg_54': 2793, 'B-ccomp_44': 2794, 'I-prep_24': 2795, 'B-mark_55': 2796, 'I-advmod_43': 2797, 'B-iobj_10': 2798, 'B-mark_52': 2799, 'I-pobj_33': 2800, 'I-amod_57': 2801, 'B-vmod_57': 2802, 'B-possessive_25': 2803, 'I-num_27': 2804, 'B-acomp_48': 2805, 'B-pcomp_47': 2806, 'B-preconj_50': 2807, 'B-advmod_60': 2808, 'B-number_67': 2809, 'B-number_72': 2810, 'I-amod_75': 2811, 'B-poss_48': 2812, 'B-advcl_42': 2813, 'B-advmod_66': 2814, 'B-nsubjpass_66': 2815, 'B-auxpass_66': 2816, 'I-mark_18': 2817, 'B-acomp_43': 2818, 'B-quantmod_50': 2819, 'B-number_61': 2820, 'B-prt_4': 2821, 'I-cc_33': 2822, 'B-pcomp_50': 2823, 'B-parataxis_24': 2824, 'B-csubj_11': 2825, 'B-expl_24': 2826, 'B-possessive_22': 2827, 'B-csubj_7': 2828, 'I-pcomp_9': 2829, 'I-pcomp_26': 2830, 'B-ccomp_47': 2831, 'I-nsubj_34': 2832, 'B-cop_54': 2833, 'I-dep_57': 2834, 'B-nsubj_73': 2835, 'I-dep_70': 2836, 'B-nn_80': 2837, 'B-nsubj_81': 2838, 'B-csubj_31': 2839, 'B-poss_49': 2840, 'B-cop_1': 2841, 'B-nsubj_1': 2842, 'B-expl_1': 2843, 'B-mark_73': 2844, 'B-nsubjpass_73': 2845, 'B-auxpass_73': 2846, 'B-advcl_62': 2847, 'B-vmod_77': 2848, 'B-det_82': 2849, 'I-nn_82': 2850, 'B-dobj_78': 2851, 'B-dep_76': 2852, 'B-nsubj_76': 2853, 'B-aux_71': 2854, 'B-vmod_65': 2855, 'I-nn_75': 2856, 'B-punct_76': 2857, 'B-advmod_80': 2858, 'B-pobj_80': 2859, 'B-mwe_84': 2860, 'B-poss_86': 2861, 'B-aux_88': 2862, 'B-vmod_86': 2863, 'B-acomp_26': 2864, 'I-conj_43': 2865, 'B-advcl_56': 2866, 'I-cc_52': 2867, 'B-aux_55': 2868, 'B-xcomp_51': 2869, 'B-possessive_31': 2870, 'I-xcomp_25': 2871, 'B-acomp_49': 2872, 'B-npadvmod_40': 2873, 'B-punct_68': 2874, 'B-dep_68': 2875, 'B-punct_75': 2876, 'B-nn_78': 2877, 'B-conj_75': 2878, 'B-cc_75': 2879, 'B-appos_75': 2880, 'B-dep_89': 2881, 'B-predet_47': 2882, 'B-neg_48': 2883, 'B-nsubjpass_68': 2884, 'B-aux_68': 2885, 'B-auxpass_68': 2886, 'B-prep_71': 2887, 'I-aux_14': 2888, 'B-expl_20': 2889, 'I-nsubj_13': 2890, 'I-dep_50': 2891, 'I-mark_22': 2892, 'B-ccomp_41': 2893, 'B-number_44': 2894, 'B-quantmod_29': 2895, 'B-number_37': 2896, 'B-quantmod_37': 2897, 'B-number_46': 2898, 'I-amod_55': 2899, 'I-nn_65': 2900, 'I-amod_72': 2901, 'B-expl_15': 2902, 'B-xcomp_57': 2903, 'B-parataxis_41': 2904, 'B-prt_23': 2905, 'B-pcomp_43': 2906, 'B-acomp_39': 2907, 'B-expl_9': 2908, 'B-predet_4': 2909, 'B-possessive_36': 2910, 'I-conj_16': 2911, 'I-mwe_8': 2912, 'I-npadvmod_35': 2913, 'B-vmod_52': 2914, 'B-tmod_17': 2915, 'B-mark_61': 2916, 'B-advcl_61': 2917, 'I-prep_42': 2918, 'B-nsubj_61': 2919, 'B-cop_61': 2920, 'I-mwe_29': 2921, 'I-mwe_32': 2922, 'I-cc_34': 2923, 'I-quantmod_16': 2924, 'B-mark_56': 2925, 'B-nsubjpass_56': 2926, 'B-auxpass_56': 2927, 'I-prep_10': 2928, 'B-auxpass_59': 2929, 'B-advcl_46': 2930, 'B-cop_56': 2931, 'B-csubj_20': 2932, 'B-advcl_53': 2933, 'B-ccomp_42': 2934, 'I-mwe_5': 2935, 'I-npadvmod_28': 2936, 'B-expl_19': 2937, 'B-parataxis_38': 2938, 'B-pcomp_55': 2939, 'I-advmod_44': 2940, 'B-dep_75': 2941, 'I-amod_65': 2942, 'B-nsubjpass_75': 2943, 'B-nn_72': 2944, 'B-auxpass_75': 2945, 'B-advmod_75': 2946, 'B-number_55': 2947, 'B-advcl_76': 2948, 'B-nsubjpass_76': 2949, 'B-auxpass_76': 2950, 'B-nsubjpass_94': 2951, 'B-nn_92': 2952, 'B-auxpass_94': 2953, 'B-prep_94': 2954, 'B-pobj_95': 2955, 'B-prep_96': 2956, 'B-pobj_97': 2957, 'B-vmod_99': 2958, 'B-nn_103': 2959, 'I-nn_103': 2960, 'B-dobj_100': 2961, 'B-preconj_108': 2962, 'B-nsubjpass_124': 2963, 'B-nn_111': 2964, 'B-pobj_112': 2965, 'B-cc_108': 2966, 'B-det_117': 2967, 'B-conj_108': 2968, 'B-prep_117': 2969, 'B-pobj_118': 2970, 'B-nn_122': 2971, 'B-pobj_120': 2972, 'B-auxpass_124': 2973, 'B-prep_124': 2974, 'B-pobj_125': 2975, 'B-acomp_30': 2976, 'I-conj_2': 2977, 'B-neg_61': 2978, 'B-advcl_48': 2979, 'B-preconj_54': 2980, 'B-acomp_44': 2981, 'I-mark_9': 2982, 'I-dobj_33': 2983, 'B-rcmod_45': 2984, 'I-quantmod_12': 2985, 'B-parataxis_28': 2986, 'B-neg_52': 2987, 'B-parataxis_42': 2988, 'I-punct_60': 2989, 'B-csubj_17': 2990, 'B-predet_20': 2991, 'I-quantmod_17': 2992, 'B-mark_57': 2993, 'B-nsubj_57': 2994, 'I-cc_45': 2995, 'B-parataxis_57': 2996, 'B-num_72': 2997, 'B-num_77': 2998, 'I-num_14': 2999, 'I-punct_50': 3000, 'B-dep_72': 3001, 'B-parataxis_68': 3002, 'B-expl_35': 3003, 'B-quantmod_69': 3004, 'B-quantmod_75': 3005, 'B-num_82': 3006, 'B-dep_79': 3007, 'B-prep_72': 3008, 'B-det_76': 3009, 'B-amod_76': 3010, 'I-dep_32': 3011, 'B-acomp_51': 3012, 'B-poss_47': 3013, 'I-advmod_37': 3014, 'B-ccomp_49': 3015, 'B-aux_66': 3016, 'B-vmod_61': 3017, 'B-prep_66': 3018, 'B-pobj_67': 3019, 'B-poss_63': 3020, 'B-aux_65': 3021, 'B-vmod_63': 3022, 'B-dobj_65': 3023, 'B-dep_84': 3024, 'B-mark_84': 3025, 'B-nsubj_84': 3026, 'B-aux_84': 3027, 'B-prep_84': 3028, 'B-det_93': 3029, 'B-amod_93': 3030, 'B-prep_93': 3031, 'B-pobj_94': 3032, 'B-csubj_30': 3033, 'B-rcmod_52': 3034, 'I-aux_28': 3035, 'B-nsubjpass_59': 3036, 'B-rcmod_54': 3037, 'B-pcomp_46': 3038, 'B-parataxis_32': 3039, 'B-mark_70': 3040, 'B-nsubj_70': 3041, 'B-cop_70': 3042, 'B-ccomp_62': 3043, 'B-expl_63': 3044, 'B-rcmod_65': 3045, 'B-aux_76': 3046, 'B-xcomp_74': 3047, 'B-dobj_76': 3048, 'B-cc_77': 3049, 'B-conj_77': 3050, 'B-cc_82': 3051, 'B-conj_82': 3052, 'I-punct_45': 3053, 'B-nsubjpass_71': 3054, 'B-neg_69': 3055, 'B-auxpass_71': 3056, 'B-parataxis_59': 3057, 'I-punct_76': 3058, 'B-det_79': 3059, 'B-nsubj_83': 3060, 'B-cop_83': 3061, 'B-npadvmod_83': 3062, 'I-npadvmod_83': 3063, 'B-punct_85': 3064, 'I-punct_83': 3065, 'B-punct_83': 3066, 'B-nsubjpass_99': 3067, 'B-prep_91': 3068, 'B-pobj_92': 3069, 'B-vmod_93': 3070, 'B-det_97': 3071, 'B-auxpass_99': 3072, 'B-parataxis_83': 3073, 'B-poss_56': 3074, 'B-preconj_44': 3075, 'I-punct_52': 3076, 'B-nsubjpass_78': 3077, 'B-auxpass_78': 3078, 'B-rcmod_74': 3079, 'B-amod_82': 3080, 'I-amod_82': 3081, 'B-dep_95': 3082, 'B-amod_89': 3083, 'B-nsubjpass_95': 3084, 'B-auxpass_95': 3085, 'B-prep_95': 3086, 'B-pobj_96': 3087, 'I-aux_25': 3088, 'B-csubjpass_27': 3089, 'I-conj_25': 3090, 'B-possessive_59': 3091, 'B-acomp_42': 3092, 'B-pcomp_2': 3093, 'I-aux_22': 3094, 'B-predet_46': 3095, 'B-ccomp_54': 3096, 'B-nn_76': 3097, 'I-punct_58': 3098, 'B-nsubj_68': 3099, 'B-num_66': 3100, 'I-prep_1': 3101, 'B-predet_41': 3102, 'B-nsubj_62': 3103, 'B-predet_21': 3104, 'B-csubjpass_13': 3105, 'B-mwe_42': 3106, 'B-advmod_61': 3107, 'I-aux_16': 3108, 'B-parataxis_37': 3109, 'B-preconj_42': 3110, 'I-nsubj_9': 3111, 'I-prep_36': 3112, 'I-advmod_49': 3113, 'I-prep_12': 3114, 'B-appos_64': 3115, 'B-vmod_64': 3116, 'I-nn_72': 3117, 'B-appos_72': 3118, 'I-prep_23': 3119, 'I-num_3': 3120, 'B-nsubj_78': 3121, 'I-nn_66': 3122, 'B-aux_70': 3123, 'B-xcomp_68': 3124, 'B-det_72': 3125, 'B-dobj_70': 3126, 'B-aux_78': 3127, 'B-parataxis_52': 3128, 'B-preconj_81': 3129, 'B-dep_81': 3130, 'B-cc_81': 3131, 'B-conj_81': 3132, 'B-mark_97': 3133, 'B-punct_97': 3134, 'B-nsubj_97': 3135, 'B-cc_93': 3136, 'B-conj_93': 3137, 'B-cop_97': 3138, 'B-preconj_102': 3139, 'B-amod_109': 3140, 'B-nsubj_114': 3141, 'B-aux_114': 3142, 'B-parataxis_97': 3143, 'B-amod_119': 3144, 'B-cc_116': 3145, 'B-conj_116': 3146, 'I-conj_20': 3147, 'I-conj_42': 3148, 'B-vmod_66': 3149, 'B-possessive_43': 3150, 'B-rcmod_53': 3151, 'B-nsubjpass_83': 3152, 'B-aux_83': 3153, 'B-auxpass_83': 3154, 'B-rcmod_86': 3155, 'I-nn_93': 3156, 'I-nsubjpass_6': 3157, 'B-preconj_53': 3158, 'I-num_50': 3159, 'B-predet_54': 3160, 'B-mwe_50': 3161, 'I-dobj_17': 3162, 'I-nsubj_19': 3163, 'I-prep_40': 3164, 'B-amod_73': 3165, 'I-amod_73': 3166, 'B-cc_79': 3167, 'B-conj_79': 3168, 'B-dobj_83': 3169, 'B-punct_91': 3170, 'B-nsubj_95': 3171, 'B-prep_98': 3172, 'I-nn_102': 3173, 'B-pobj_99': 3174, 'B-punct_104': 3175, 'B-dep_98': 3176, 'B-prep_104': 3177, 'B-nn_107': 3178, 'B-pobj_105': 3179, 'B-punct_107': 3180, 'B-conj_107': 3181, 'B-cc_107': 3182, 'B-num_114': 3183, 'B-punct_117': 3184, 'B-appos_114': 3185, 'I-punct_104': 3186, 'B-nn_121': 3187, 'B-advmod_95': 3188, 'B-punct_95': 3189, 'B-mark_127': 3190, 'B-nn_125': 3191, 'I-nn_126': 3192, 'B-nsubj_127': 3193, 'B-advcl_95': 3194, 'B-preconj_130': 3195, 'B-neg_130': 3196, 'B-dobj_127': 3197, 'B-punct_133': 3198, 'B-appos_130': 3199, 'B-cc_130': 3200, 'B-det_138': 3201, 'B-amod_138': 3202, 'B-conj_130': 3203, 'B-punct_141': 3204, 'B-nn_141': 3205, 'B-dep_138': 3206, 'B-cc_141': 3207, 'B-nn_145': 3208, 'B-num_145': 3209, 'B-conj_141': 3210, 'B-vmod_50': 3211, 'B-num_63': 3212, 'B-conj_66': 3213, 'B-cc_66': 3214, 'I-cc_44': 3215, 'B-preconj_49': 3216, 'I-conj_36': 3217, 'B-npadvmod_52': 3218, 'I-npadvmod_52': 3219, 'B-mwe_56': 3220, 'I-prep_32': 3221, 'B-prt_46': 3222, 'B-num_61': 3223, 'I-punct_68': 3224, 'B-prep_81': 3225, 'B-num_83': 3226, 'B-appos_81': 3227, 'B-vmod_81': 3228, 'B-det_92': 3229, 'B-amod_92': 3230, 'B-vmod_92': 3231, 'B-num_97': 3232, 'B-nn_97': 3233, 'B-dobj_93': 3234, 'I-punct_105': 3235, 'B-dep_110': 3236, 'B-punct_105': 3237, 'B-nsubj_110': 3238, 'B-dobj_110': 3239, 'B-num_116': 3240, 'I-punct_120': 3241, 'B-dep_125': 3242, 'B-punct_120': 3243, 'B-det_124': 3244, 'B-amod_124': 3245, 'B-nsubj_125': 3246, 'B-det_129': 3247, 'B-num_129': 3248, 'B-dobj_125': 3249, 'B-vmod_129': 3250, 'B-det_134': 3251, 'B-nn_134': 3252, 'I-nn_134': 3253, 'B-prep_134': 3254, 'B-pobj_135': 3255, 'B-num_136': 3256, 'B-punct_139': 3257, 'B-appos_129': 3258, 'I-punct_125': 3259, 'B-vmod_125': 3260, 'B-prep_142': 3261, 'B-det_145': 3262, 'B-pobj_143': 3263, 'B-prep_145': 3264, 'B-det_152': 3265, 'B-amod_152': 3266, 'I-amod_152': 3267, 'B-nn_152': 3268, 'B-pobj_146': 3269, 'B-aux_154': 3270, 'B-xcomp_142': 3271, 'B-det_158': 3272, 'B-num_158': 3273, 'B-nn_158': 3274, 'B-dobj_154': 3275, 'B-prep_158': 3276, 'B-pobj_159': 3277, 'B-pobj_161': 3278, 'B-num_162': 3279, 'I-quantmod_21': 3280, 'I-prep_13': 3281, 'B-nsubj_93': 3282, 'B-appos_77': 3283, 'B-cop_93': 3284, 'B-nn_93': 3285, 'B-punct_93': 3286, 'I-amod_93': 3287, 'B-nsubj_96': 3288, 'B-advmod_96': 3289, 'B-rcmod_93': 3290, 'B-nn_101': 3291, 'B-det_107': 3292, 'B-advmod_106': 3293, 'B-amod_107': 3294, 'B-nsubj_112': 3295, 'B-appos_107': 3296, 'B-advmod_112': 3297, 'B-prep_112': 3298, 'B-det_116': 3299, 'B-amod_116': 3300, 'B-pobj_113': 3301, 'B-dep_144': 3302, 'B-punct_119': 3303, 'B-nsubj_144': 3304, 'B-prep_123': 3305, 'B-det_128': 3306, 'B-nn_128': 3307, 'I-nn_128': 3308, 'B-pobj_124': 3309, 'B-cc_121': 3310, 'B-conj_121': 3311, 'B-prep_130': 3312, 'B-pobj_131': 3313, 'B-amod_141': 3314, 'B-appos_135': 3315, 'B-advmod_144': 3316, 'B-det_147': 3317, 'B-nn_147': 3318, 'B-dobj_144': 3319, 'I-punct_61': 3320, 'B-nsubj_65': 3321, 'B-amod_85': 3322, 'B-vmod_85': 3323, 'B-nn_88': 3324, 'B-dobj_86': 3325, 'B-pobj_89': 3326, 'B-vmod_95': 3327, 'B-possessive_40': 3328, 'B-neg_74': 3329, 'B-tmod_29': 3330, 'B-mwe_55': 3331, 'B-quantmod_56': 3332, 'B-prt_20': 3333, 'B-prt_13': 3334, 'B-nsubjpass_65': 3335, 'B-auxpass_65': 3336, 'I-ccomp_19': 3337, 'I-amod_59': 3338, 'B-nsubjpass_79': 3339, 'I-nn_77': 3340, 'B-auxpass_79': 3341, 'B-xcomp_54': 3342, 'B-prt_19': 3343, 'B-advcl_44': 3344, 'I-aux_20': 3345, 'B-advcl_41': 3346, 'I-cc_47': 3347, 'B-prt_6': 3348, 'I-nsubj_24': 3349, 'B-conj_76': 3350, 'B-cc_76': 3351, 'B-advmod_83': 3352, 'B-cc_83': 3353, 'B-conj_83': 3354, 'B-aux_91': 3355, 'B-xcomp_89': 3356, 'B-punct_98': 3357, 'B-dep_109': 3358, 'B-nsubjpass_109': 3359, 'B-cc_100': 3360, 'B-conj_100': 3361, 'I-dep_100': 3362, 'B-prep_100': 3363, 'B-pobj_106': 3364, 'B-auxpass_109': 3365, 'B-prep_111': 3366, 'B-prep_115': 3367, 'B-nn_118': 3368, 'I-nn_119': 3369, 'B-pobj_116': 3370, 'I-prep_31': 3371, 'B-cop_67': 3372, 'B-ccomp_67': 3373, 'B-cop_63': 3374, 'B-num_73': 3375, 'B-appos_65': 3376, 'B-preconj_57': 3377, 'B-neg_63': 3378, 'B-aux_73': 3379, 'B-neg_73': 3380, 'B-dobj_73': 3381, 'I-xcomp_8': 3382, 'I-quantmod_29': 3383, 'I-npadvmod_42': 3384, 'B-advcl_63': 3385, 'B-csubjpass_20': 3386, 'B-number_41': 3387, 'B-preconj_56': 3388, 'B-dep_61': 3389, 'I-conj_9': 3390, 'B-npadvmod_47': 3391, 'I-npadvmod_47': 3392, 'B-number_47': 3393, 'I-npadvmod_29': 3394, 'B-expl_33': 3395, 'B-quantmod_44': 3396}\n",
      "label Map NER written at content/data/ner_coNLL_train1_label_map.joblib\n",
      "Created POS label map from train file coNLL_train1.txt\n",
      "{'_': 0}\n",
      "label Map POS written at content/data/pos_coNLL_train1_label_map.joblib\n",
      "Max len of sentence:  164\n",
      "Mean len of sentences:  26.269321656936334\n",
      "Median len of sentences:  24.0\n",
      "Making data from file coNLL_testa1.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  65\n",
      "Mean len of sentences:  25.80722891566265\n",
      "Median len of sentences:  24.0\n",
      "Making data from file coNLL_testb1.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  67\n",
      "Mean len of sentences:  25.517319277108435\n",
      "Median len of sentences:  24.0\n",
      "Making data from file coNLL_testb1_dummy.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  59\n",
      "Mean len of sentences:  25.305555555555557\n",
      "Median len of sentences:  24.0\n",
      "tien dat coNLL_data\n",
      "Making data from file coNLL_train2.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "Processing 145000 rows...\n",
      "Processing 150000 rows...\n",
      "Processing 155000 rows...\n",
      "Processing 160000 rows...\n",
      "Processing 165000 rows...\n",
      "Processing 170000 rows...\n",
      "Processing 175000 rows...\n",
      "Processing 180000 rows...\n",
      "Processing 185000 rows...\n",
      "Processing 190000 rows...\n",
      "Processing 195000 rows...\n",
      "Processing 200000 rows...\n",
      "Processing 205000 rows...\n",
      "Processing 210000 rows...\n",
      "Processing 215000 rows...\n",
      "Processing 220000 rows...\n",
      "Processing 225000 rows...\n",
      "Processing 230000 rows...\n",
      "Processing 235000 rows...\n",
      "Processing 240000 rows...\n",
      "Processing 245000 rows...\n",
      "Processing 250000 rows...\n",
      "Processing 255000 rows...\n",
      "Processing 260000 rows...\n",
      "Processing 265000 rows...\n",
      "Processing 270000 rows...\n",
      "Processing 275000 rows...\n",
      "Processing 280000 rows...\n",
      "Processing 285000 rows...\n",
      "Processing 290000 rows...\n",
      "Processing 295000 rows...\n",
      "Processing 300000 rows...\n",
      "Processing 305000 rows...\n",
      "Processing 310000 rows...\n",
      "Processing 315000 rows...\n",
      "Processing 320000 rows...\n",
      "Processing 325000 rows...\n",
      "Processing 330000 rows...\n",
      "Processing 335000 rows...\n",
      "Processing 340000 rows...\n",
      "Processing 345000 rows...\n",
      "Processing 350000 rows...\n",
      "Processing 355000 rows...\n",
      "Processing 360000 rows...\n",
      "Processing 365000 rows...\n",
      "Processing 370000 rows...\n",
      "Processing 375000 rows...\n",
      "Processing 380000 rows...\n",
      "Processing 385000 rows...\n",
      "Processing 390000 rows...\n",
      "Processing 395000 rows...\n",
      "Processing 400000 rows...\n",
      "Processing 405000 rows...\n",
      "Processing 410000 rows...\n",
      "Processing 415000 rows...\n",
      "Processing 420000 rows...\n",
      "Processing 425000 rows...\n",
      "Processing 430000 rows...\n",
      "Processing 435000 rows...\n",
      "Processing 440000 rows...\n",
      "Processing 445000 rows...\n",
      "Processing 450000 rows...\n",
      "Processing 455000 rows...\n",
      "Processing 460000 rows...\n",
      "Processing 465000 rows...\n",
      "Processing 470000 rows...\n",
      "Processing 475000 rows...\n",
      "Processing 480000 rows...\n",
      "Processing 485000 rows...\n",
      "Processing 490000 rows...\n",
      "Processing 495000 rows...\n",
      "Processing 500000 rows...\n",
      "Processing 505000 rows...\n",
      "Processing 510000 rows...\n",
      "Processing 515000 rows...\n",
      "Processing 520000 rows...\n",
      "Processing 525000 rows...\n",
      "Processing 530000 rows...\n",
      "Processing 535000 rows...\n",
      "Processing 540000 rows...\n",
      "Processing 545000 rows...\n",
      "Processing 550000 rows...\n",
      "Processing 555000 rows...\n",
      "Processing 560000 rows...\n",
      "Processing 565000 rows...\n",
      "Processing 570000 rows...\n",
      "Processing 575000 rows...\n",
      "Processing 580000 rows...\n",
      "Processing 585000 rows...\n",
      "Processing 590000 rows...\n",
      "Processing 595000 rows...\n",
      "Processing 600000 rows...\n",
      "Processing 605000 rows...\n",
      "Processing 610000 rows...\n",
      "Processing 615000 rows...\n",
      "Processing 620000 rows...\n",
      "Processing 625000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Created NER label map from train file coNLL_train2.txt\n",
      "{'B-S_NP_NP_NN': 0, 'B-NN': 1, 'O-NP': 2, 'B-PP_IN': 3, 'B-NP_DT': 4, 'O-PP': 5, 'B-VP_VBZ': 6, 'B-VP_VBN': 7, 'B-NP_PP_IN': 8, 'B-NP_NP_NN': 9, 'O-VP': 10, 'B-PERIOD': 11, 'O-S': 12, 'B-S_NP_NN': 13, 'B-NNS': 14, 'B-VP_VBP': 15, 'B-NP_NP_JJ': 16, 'B-SBAR_WHNP_WDT': 17, 'O-WHNP': 18, 'B-S_NP_VP_VBP': 19, 'B-VBG': 20, 'B-SBAR_WHNP_S_NP_VP_O-SBAR': 21, 'O-SBAR': 22, 'B-S_NP_NP_DT': 23, 'B-JJ': 24, 'B-NP_NN': 25, 'B-RB': 26, 'B-VP_VB': 27, 'B-NP_NP_NNS': 28, 'B-COMMA': 29, 'B-SBAR_IN': 30, 'B-NP_ADVP_FW': 31, 'B-FW': 32, 'O-ADVP': 33, 'B-NP_JJ': 34, 'B-S_ADVP_RB': 35, 'B-NP_NP_DT': 36, 'B-S_PP_IN': 37, 'B-NP_NP_PP_O-PP': 38, 'B-NP_VP_VBD': 39, 'B-S_NP_VP_TO': 40, 'B-CC': 41, 'B-PRN_LRB': 42, 'B-NP_NNS': 43, 'B-RRB': 44, 'O-PRN': 45, 'B-VP_VBD': 46, 'B-NP_O-VP': 47, 'B-S_S_NP_DT': 48, 'B-ADJP_ADJP_RB': 49, 'B-JJR': 50, 'O-ADJP': 51, 'B-PP_O-ADJP': 52, 'B-COLON': 53, 'B-NP_NP_CD': 54, 'B-S_NP_NP_NP_DT': 55, 'B-S_NP_VP_VBG': 56, 'B-S_S_NP_NP_DT': 57, 'B-NP_O-NP': 58, 'B-NP_PP_O-VP': 59, 'B-S_SBAR_WHADVP_WRB': 60, 'O-WHADVP': 61, 'B-S_NP_VP_O-SBAR': 62, 'B-S_NP_CC': 63, 'B-NP_NP_PP_PP_CC': 64, 'B-NP_NP_PP_PP_O-NP': 65, 'B-ADJP_RB': 66, 'B-S_NP_NP_ADJP_JJ': 67, 'B-S_NP_PRP': 68, 'B-ADVP_RB': 69, 'B-ADJP_NN': 70, 'B-VP_VP_VBP': 71, 'B-VP_MD': 72, 'B-NP_NP_NP_NN': 73, 'B-NP_PP_PP_IN': 74, 'B-CONJP_CC': 75, 'O-CONJP': 76, 'B-NP_ADJP_ADJP_JJ': 77, 'B-ADJP_JJ': 78, 'B-PP_TO': 79, 'B-S_NP_VP_MD': 80, 'B-VP_VP_VBD': 81, 'B-NP_NP_PP_O-VP': 82, 'B-S_S_NP_NP_NN': 83, 'B-S_NP_VP_VP_VBZ': 84, 'B-PP_O-VP': 85, 'B-S_NP_DT': 86, 'B-NP_NP_NP_DT': 87, 'B-ADJP_ADJP_NN': 88, 'B-ADJP_O-ADJP': 89, 'B-VP_VBG': 90, 'B-S_NP_NP_JJ': 91, 'B-S_NP_NP_NP_NN': 92, 'B-S_SBAR_IN': 93, 'B-S_S_NP_VP_TO': 94, 'B-NP_PRP': 95, 'B-NP_NP_COMMA': 96, 'B-NP_O-PP': 97, 'B-S_NP_VBG': 98, 'B-NP_ADVP_RB': 99, 'B-NP_CD': 100, 'B-S_S_NP_VP_VBG': 101, 'B-TO': 102, 'B-PP_JJ': 103, 'B-NP_NP_CC': 104, 'B-DT': 105, 'B-S_NP_NP_NP_JJ': 106, 'B-NP_NP_NP_JJ': 107, 'B-NP_NP_VBN': 108, 'B-NP_RRB': 109, 'B-S_NP_VP_VBZ': 110, 'B-NP_NP_ADJP_ADJP_VBN': 111, 'B-VP_VP_VBN': 112, 'B-NP_ADVP_ADVP_RB': 113, 'B-NP_QP_CD': 114, 'O-QP': 115, 'B-LQT': 116, 'B-RQT': 117, 'B-NP_LQT': 118, 'B-IN': 119, 'B-VBN': 120, 'B-UCP_ADJP_JJ': 121, 'B-ADJP_VBN': 122, 'O-UCP': 123, 'B-PP_NP_CD': 124, 'B-S_NP_ADJP_JJ': 125, 'B-PP_O-S': 126, 'B-ADVP_FW': 127, 'B-NP_ADJP_RB': 128, 'B-NP_NP_PRN_NP_O-NP': 129, 'B-S_NP_NP_NNS': 130, 'B-NP_S_NP_VP_VBG': 131, 'B-NP_NP_QP_CD': 132, 'B-CD': 133, 'B-LRB': 134, 'B-NP_NP_NP_ADJP_CD': 135, 'B-NP_QP_JJR': 136, 'B-NP_NP_NP_NNS': 137, 'B-NP_ADJP_NN': 138, 'B-NP_JJR': 139, 'B-S_S_PP_IN': 140, 'B-NP_NP_ADJP_RB': 141, 'B-PP_ADVP_RB': 142, 'B-S_S_S_NP_JJ': 143, 'B-S_NP_JJ': 144, 'B-PP_O-NP': 145, 'B-NP_PRPP': 146, 'B-S_PP_NP_CD': 147, 'B-NP_NP_NP_PRN_COMMA': 148, 'B-NP_VBN': 149, 'B-S_NP_VP_VBN': 150, 'B-ADVP_O-VP': 151, 'B-NP_NP_ADJP_ADJP_JJ': 152, 'B-NP_NP_QP_RB': 153, 'B-VP_O-S': 154, 'B-VP_ADVP_RB': 155, 'B-NP_COMMA': 156, 'B-NP_CC': 157, 'B-ADJP_ADJP_JJ': 158, 'B-ADJP_VBG': 159, 'B-VP_TO': 160, 'B-NP_NP_ADJP_ADJP_CC': 161, 'B-NP_NP_PP_PP_O-PP': 162, 'B-S_S_NP_ADJP_RB': 163, 'B-PP_VBN': 164, 'B-NP_PRN_LRB': 165, 'B-NP_ADJP_QP_CD': 166, 'B-S_NP_NNS': 167, 'B-S_NP_NP_PRPP': 168, 'B-NP_ADJP_CC': 169, 'B-S_NP_VP_VB': 170, 'B-VP_ADVP_VB': 171, 'B-S_NP_ADJP_RB': 172, 'B-S_O-NP': 173, 'B-NP_PP_TO': 174, 'B-VB': 175, 'B-NP_NP_NP_NP_DT': 176, 'B-S_NP_NP_CD': 177, 'B-ADJP_O-VP': 178, 'B-SBAR_WHPP_IN': 179, 'B-WHNP_WDT': 180, 'O-WHPP': 181, 'B-S_NP_NP_VBG': 182, 'B-NP_EX': 183, 'B-SBAR_WHADVP_S_NP_VP_TO': 184, 'B-NP_S_NP_VP_TO': 185, 'B-NP_VP_VBG': 186, 'B-NP_PP_O-NP': 187, 'B-S_S_LST_LRB': 188, 'B-LS': 189, 'O-LST': 190, 'B-NP_NP_VP_O-PP': 191, 'B-S_LST_LRB': 192, 'B-ADJP_ADJP_VBN': 193, 'B-ADJP_ADJP_RBR': 194, 'B-NP_O-ADJP': 195, 'B-S_S_NP_NN': 196, 'B-NP_ADJP_ADJP_CC': 197, 'B-ADJP_NNS': 198, 'B-PP_NP_RB': 199, 'B-NP_NP_NP_CD': 200, 'B-PP_NP_JJ': 201, 'B-PP_PP_IN': 202, 'B-S_NP_VP_ADVP_RB': 203, 'B-VBD': 204, 'B-NP_NP_NP_NP_NN': 205, 'B-NP_NP_NP_VBG': 206, 'B-CONJP_RB': 207, 'B-S_NP_PRPP': 208, 'B-SBAR_SBAR_IN': 209, 'B-NP_NP_PRP': 210, 'B-SBAR_O-NP': 211, 'B-SBAR_WHADVP_WRB': 212, 'B-VP_VP_VBZ': 213, 'B-S_NP_VP_O-PP': 214, 'B-NP_NP_NP_NP_JJ': 215, 'B-S_NP_VP_VBD': 216, 'B-ADJP_RBR': 217, 'B-ADJP_ADJP_ADVP_RB': 218, 'B-RBR': 219, 'B-S_NP_ADVP_ADJP_O-VP': 220, 'B-NP_NP_VBG': 221, 'B-SBAR_WHADVP_IN': 222, 'B-SBAR_WHPP_S_O-NP': 223, 'B-S_NP_NP_VBN': 224, 'B-NP_NP_NP_NP_JJR': 225, 'B-NP_NP_JJR': 226, 'B-VP_JJ': 227, 'B-ADJP_CD': 228, 'B-S_S_NP_NP_JJ': 229, 'B-NP_ADJP_FW': 230, 'B-NP_NP_ADJP_FW': 231, 'B-PP_ADVP_IN': 232, 'B-JJS': 233, 'B-NP_FW': 234, 'B-S_S_S_NP_NP_JJ': 235, 'B-S_NP_FW': 236, 'B-NP_NP_ADJP_O-PP': 237, 'B-VP_VP_VP_VBD': 238, 'B-S_NP_EX': 239, 'B-NP_VBG': 240, 'B-NP_ADJP_NP_NP_NN': 241, 'B-S_NP_ADVP_RBR': 242, 'B-S_O-ADJP': 243, 'B-S_S_S_NP_NN': 244, 'B-S_S_NP_VP_VBN': 245, 'B-S_NP_NP_ADJP_NN': 246, 'B-NP_NP_NP_QP_NN': 247, 'B-NP_NP_QP_NN': 248, 'B-NP_NP_NP_PRN_CC': 249, 'B-NP_NP_PRPP': 250, 'B-S_NP_NP_NP_CC': 251, 'B-ADJP_RBS': 252, 'B-VP_ADVP_ADVP_RB': 253, 'B-PP_NP_CC': 254, 'B-FRAG_PP_IN': 255, 'O-FRAG': 256, 'B-NP_PP_ADVP_RB': 257, 'B-NP_NP_QP_JJ': 258, 'B-NP_NP_NP_PP_PRN_LRB': 259, 'B-NP_PP_JJ': 260, 'B-S_S_NP_QP_CD': 261, 'B-S_NP_CD': 262, 'B-S_S_NP_NP_CD': 263, 'B-NP_QP_IN': 264, 'B-NP_NNP': 265, 'B-S_NP_VBN': 266, 'B-NNP': 267, 'B-NP_NP_ADJP_JJ': 268, 'B-S_O-VP': 269, 'B-S_PP_JJ': 270, 'B-PP_CC': 271, 'B-VP_VP_VP_VBN': 272, 'B-S_S_NP_QP_IN': 273, 'B-ADJP_FW': 274, 'B-NP_ADJP_ADJP_NN': 275, 'B-PRN_PRN_LRB': 276, 'B-S_NP_ADVP_RB': 277, 'B-PP_VBG': 278, 'B-PP_ADVP_RBS': 279, 'B-S_NP_NP_ADJP_NP_NP_JJ': 280, 'B-PRN_JJ': 281, 'B-ADJP_ADJP_CC': 282, 'B-ADJP_JJR': 283, 'B-SBAR_WHNP_S_O-NP': 284, 'B-NP_ADJP_JJ': 285, 'B-UCP_NP_NP_NNS': 286, 'B-PP_ADVP_ADVP_IN': 287, 'B-ADJP_NP_NP_JJ': 288, 'B-NP_NP_NP_PP_COMMA': 289, 'B-NP_NP_NP_NP_CD': 290, 'B-NP_NP_NP_PP_CC': 291, 'B-PP_PRN_LRB': 292, 'B-VBZ': 293, 'B-S_S_NP_NP_NP_NP_NN': 294, 'B-ADVP_DT': 295, 'B-NP_RB': 296, 'B-NP_NP_RB': 297, 'B-NP_NP_PRN_O-NP': 298, 'B-NP_NP_NP_CC': 299, 'B-NP_O-S': 300, 'B-VP_VP_VP_VBZ': 301, 'B-ADJP_ADJP_NP_NN': 302, 'B-NP_ADVP_PP_O-VP': 303, 'B-ADJP_NP_NP_NN': 304, 'B-NP_NP_PRN_COMMA': 305, 'B-NP_LST_NN': 306, 'B-NP_NP_ADJP_O-VP': 307, 'B-VP_VP_VB': 308, 'B-WHADVP_S_NP_VP_TO': 309, 'B-SBAR_SBAR_SBAR_SBAR_CC': 310, 'B-SBAR_S_NP_VP_O-SBAR': 311, 'B-S_NP_ADVP_VP_O-SBAR': 312, 'B-VP_O-VP': 313, 'B-S_CC': 314, 'B-S_O-SBAR': 315, 'B-NP_NP_PP_O-NP': 316, 'B-NP_ADJP_CD': 317, 'B-SBAR_COMP_S_NP_NN': 318, 'B-NP_NP_ADJP_NP_NP_JJ': 319, 'B-QP_CD': 320, 'B-S_NP_NP_CC': 321, 'B-NP_S_NP_VP_VP_TO': 322, 'B-NP_NP_ADJP_CD': 323, 'B-VP_VP_ADVP_RB': 324, 'B-VP_VP_RB': 325, 'B-PP_PP_TO': 326, 'B-VP_PP_TO': 327, 'B-S_NP_VP_O-VP': 328, 'B-VP_VP_VP_CC': 329, 'B-VP_NP_NP_SBAR_O-VP': 330, 'B-NP_NP_SBAR_O-PP': 331, 'B-NP_VP_VBN': 332, 'B-NP_ADJP_NNS': 333, 'B-ADJP_JJS': 334, 'B-NP_ADJP_IN': 335, 'B-NP_NP_NP_NP_VBN': 336, 'B-NP_PP_O-S': 337, 'B-PRN_COMMA': 338, 'B-FRAG_VP_VBN': 339, 'B-NP_PP_O-FRAG': 340, 'B-SBAR_S_NP_VP_VBZ': 341, 'B-S_S_NP_NP_PRP': 342, 'B-NP_ADVP_PP_PP_O-VP': 343, 'B-S_ADVP_RBR': 344, 'B-NP_QP_RB': 345, 'B-NP_QP_O-PP': 346, 'B-S_NP_RB': 347, 'B-PP_ADVP_JJ': 348, 'B-S_NP_NP_ADJP_FW': 349, 'B-S_RRB': 350, 'B-PP_PP_CC': 351, 'B-NP_NP_VP_O-NP': 352, 'B-VP_VP_MD': 353, 'B-S_NP_UCP_NP_NN': 354, 'B-S_NP_UCP_ADJP_JJ': 355, 'B-SBAR_COMP_S_NP_NP_DT': 356, 'B-S_NP_NP_PP_VP_JJ': 357, 'B-S_NP_JJS': 358, 'B-ADJP_ADVP_RB': 359, 'B-SBAR_COMP_S_NP_PRP': 360, 'B-SBAR_WHNP_WP': 361, 'B-UCP_ADJP_ADVP_RB': 362, 'B-S_S_S_NP_VP_VBN': 363, 'B-POS': 364, 'B-NP_NP_NP_ADJP_RB': 365, 'B-VBP': 366, 'B-NP_VP_VBP': 367, 'B-NP_VP_MD': 368, 'B-NP_NP_PRN_O-PP': 369, 'B-S_NP_NP_NP_CD': 370, 'B-NP_IN': 371, 'B-S_NP_VP_O-ADJP': 372, 'B-VP_VP_VP_VB': 373, 'B-NP_NP_ADJP_NN': 374, 'B-NP_NP_PDT': 375, 'B-S_NP_NP_NP_NP_NN': 376, 'B-NP_NP_NP_PRN_ADVP_RB': 377, 'B-NP_NP_NP_TO': 378, 'B-PP_PP_PP_IN': 379, 'B-NP_UCP_CONJP_RB': 380, 'B-VP_CC': 381, 'B-VP_VP_TO': 382, 'B-SBAR_WHADVP_S_O-VP': 383, 'B-ADJP_ADJP_COMMA': 384, 'B-NP_ADJP_RBR': 385, 'B-SINV_VBD': 386, 'B-VP_O-SINV': 387, 'B-NP_NP_PP_PP_O-VP': 388, 'B-NP_SBAR_WHADVP_WRB': 389, 'B-NP_NP_LQT': 390, 'B-S_NP_NP_JJR': 391, 'B-NP_NP_NP_PRPP': 392, 'B-S_NP_VP_VP_CC': 393, 'B-NP_JJS': 394, 'B-NP_NP_NP_ADJP_NN': 395, 'B-NP_NP_LRB': 396, 'B-PP_PP_PP_TO': 397, 'B-S_NP_ADJP_CD': 398, 'B-PP_PP_ADVP_RB': 399, 'B-S_NP_NP_FW': 400, 'B-NP_PP_CONJP_RB': 401, 'B-NP_QP_NN': 402, 'B-S_S_S_NP_VP_CC': 403, 'B-S_NP_VP_O-S': 404, 'B-NP_NP_PP_PP_TO': 405, 'B-S_NP_ADJP_NN': 406, 'B-S_NP_NP_PP_VP_VBP': 407, 'B-NP_NP_ADJP_ADJP_COMMA': 408, 'B-S_NP_VP_VP_TO': 409, 'B-NP_SBAR_WHNP_WDT': 410, 'B-NP_NP_NP_VP_COLON': 411, 'B-S_PP_VBG': 412, 'B-NP_UCP_PP_IN': 413, 'B-NP_ADVP_O-VP': 414, 'B-S_S_ADVP_RB': 415, 'B-NP_ADJP_ADJP_VBG': 416, 'B-SBAR_WHNP_WPP': 417, 'B-NP_ADVP_CD': 418, 'B-S_S_S_NP_NP_NN': 419, 'B-NP_UCP_ADJP_CC': 420, 'B-S_S_NP_JJ': 421, 'B-NP_SBAR_IN': 422, 'B-S_ADJP_NP_NP_PRN_JJ': 423, 'B-ADVP_IN': 424, 'B-NP_NP_NP_PRN_NN': 425, 'B-ADVP_ADVP_FW': 426, 'B-ADJP_ADVP_RBS': 427, 'B-NP_UCP_NN': 428, 'B-NP_UCP_ADJP_JJ': 429, 'B-NP_NP_JJS': 430, 'B-NP_NP_NP_FW': 431, 'B-VP_O-NP': 432, 'B-SBAR_WDT': 433, 'B-NP_NP_VBD': 434, 'B-SBAR_WHNP_IN': 435, 'B-VP_NP_DT': 436, 'B-NP_ADJP_VBG': 437, 'B-SBAR_WHNP_NP_NN': 438, 'B-VP_ADVP_RBR': 439, 'B-ADJP_ADVP_O-VP': 440, 'B-VP_VP_CC': 441, 'B-S_ADVP_NP_CD': 442, 'B-NP_NP_COLON': 443, 'B-S_S_COLON': 444, 'B-NP_NP_LST_LS': 445, 'B-NP_LST_LS': 446, 'B-NP_NP_ADJP_DT': 447, 'B-NP_NP_PRN_NP_O-PP': 448, 'B-NP_NP_NP_NP_NNS': 449, 'B-PP_RB': 450, 'B-NP_NP_ADJP_NNS': 451, 'B-PP_PP_JJ': 452, 'B-S_PP_PP_PP_IN': 453, 'B-NP_NP_NP_QP_CD': 454, 'B-NP_S_NP_NP_NN': 455, 'B-VP_NP_PP_IN': 456, 'B-VP_VP_LST_LS': 457, 'B-VP_LST_LS': 458, 'B-MD': 459, 'B-CONJP_NP_O-PP': 460, 'B-S_NP_ADJP_ADJP_NN': 461, 'B-NP_PP_VBG': 462, 'B-NP_NP_UCP_ADJP_CC': 463, 'B-SBAR_O-PP': 464, 'B-UCP_S_NP_VP_VBG': 465, 'B-S_NP_NP_ADJP_RB': 466, 'B-PP_DT': 467, 'B-VP_ADVP_ADVP_IN': 468, 'B-ADVP_ADVP_RB': 469, 'B-ADVP_RBR': 470, 'B-S_S_SBAR_IN': 471, 'B-NP_NP_PP_VP_MD': 472, 'B-SBAR_LST_LRB': 473, 'B-S_NP_NP_PP_VP_MD': 474, 'B-S_NP_NP_PP_PP_VP_MD': 475, 'B-S_S_NP_NP_VBN': 476, 'B-VP_NNS': 477, 'B-SBAR_WHNP_S_NP_VP_TO': 478, 'B-NP_NP_O-PP': 479, 'B-NP_O-ADVP': 480, 'B-PRPP': 481, 'B-S_S_S_NP_ADJP_JJ': 482, 'B-S_NP_NP_ADJP_NP_NP_NN': 483, 'B-PRN_CC': 484, 'B-PRN_NNS': 485, 'B-NP_NP_ADJP_ADJP_RB': 486, 'B-VP_VP_VP_VBP': 487, 'B-NP_NP_ADJP_NP_NP_PRN_JJ': 488, 'B-NP_NP_PP_PP_ADVP_O-VP': 489, 'B-S_S_NP_NP_NP_DT': 490, 'B-S_S_NP_NNS': 491, 'B-S_S_NP_NP_NNS': 492, 'B-SBAR_WHNP_NP_JJ': 493, 'B-WHPP_IN': 494, 'B-VP_CONJP_RB': 495, 'B-NP_CONJP_CC': 496, 'B-UCP_ADVP_CC': 497, 'B-PP_S_NP_VP_O-VP': 498, 'B-S_NP_NP_JJS': 499, 'B-NP_ADJP_ADJP_RB': 500, 'B-SBAR_WHNP_S_COMMA': 501, 'B-NP_NP_ADJP_ADJP_CONJP_ADJP_NN': 502, 'B-NP_COLON': 503, 'B-NP_NP_NP_PP_NP_NP_PRN_O-NP': 504, 'B-S_NP_NP_ADJP_ADJP_NN': 505, 'B-S_NP_VP_VP_COMMA': 506, 'B-VP_NN': 507, 'B-S_LST_LS': 508, 'B-S_LST_FW': 509, 'B-NP_NP_NP_JJR': 510, 'B-VP_JJR': 511, 'B-S_NP_JJR': 512, 'B-S_ADVP_DT': 513, 'B-S_DT': 514, 'B-S_NNS': 515, 'B-WDT': 516, 'B-NP_NP_QP_IN': 517, 'B-NP_NP_NP_VP_PRN_LRB': 518, 'B-NP_PP_PP_O-VP': 519, 'B-NP_SBAR_O-VP': 520, 'B-ADJP_CC': 521, 'B-PP_NP_NN': 522, 'B-UCP_ADJP_NN': 523, 'B-S_NP_NP_PP_VP_VBZ': 524, 'B-ADVP_ADVP_RBR': 525, 'B-SBAR_WHNP_WHNP_DT': 526, 'B-S_ADVP_CC': 527, 'B-NP_SBAR_WHPP_IN': 528, 'B-S_O-S': 529, 'B-NP_NP_QP_JJR': 530, 'B-NP_NP_CONJP_NP_O-PP': 531, 'B-NP_NP_ADJP_ADJP_NN': 532, 'B-S_NP_NP_NP_NP_JJ': 533, 'B-NP_NP_PRN_NN': 534, 'B-PP_FW': 535, 'B-LST_LRB': 536, 'B-NP_ADVP_IN': 537, 'B-PRN_O-PP': 538, 'B-PRN_SBAR_IN': 539, 'B-S_NN': 540, 'B-S_TO': 541, 'B-S_JJ': 542, 'B-UCP_ADVP_RB': 543, 'B-NP_SBAR_WRB': 544, 'B-VP_RB': 545, 'B-PP_NP_O-PP': 546, 'B-NP_NP_NP_VBN': 547, 'B-ADJP_ADJP_JJR': 548, 'B-S_S_CC': 549, 'B-S_ADVP_O-NP': 550, 'B-NP_NP_NP_O-PP': 551, 'B-VP_VP_ADVP_O-VP': 552, 'B-PP_ADJP_JJ': 553, 'B-NP_NP_NP_VB': 554, 'B-S_S_NP_ADVP_RB': 555, 'B-NP_S_NP_VP_O-NP': 556, 'B-VP_NP_O-VP': 557, 'B-S_NP_IN': 558, 'B-NP_NP_NP_PP_PP_IN': 559, 'B-S_NP_VP_RB': 560, 'B-SBAR_S_NP_VP_VBG': 561, 'B-NP_PRN_O-PP': 562, 'B-S_NP_VP_NN': 563, 'B-WRB': 564, 'B-NP_NP_NP_NP_NP_NN': 565, 'B-S_NP_NP_ADJP_NNS': 566, 'B-NP_NP_PP_RRB': 567, 'B-NP_S_NP_NP_NP_JJ': 568, 'B-NP_NP_PP_CC': 569, 'B-S_PP_ADVP_IN': 570, 'B-NP_NP_S_O-VP': 571, 'B-NP_NP_NP_PRN_PP_IN': 572, 'B-S_NP_NP_NP_VBN': 573, 'B-S_S_NP_CC': 574, 'B-S_S_NP_VP_ADJP_JJ': 575, 'B-VP_PP_IN': 576, 'B-SBAR_RB': 577, 'B-S_NP_NP_NP_NNS': 578, 'B-NP_PRN_O-VP': 579, 'B-S_NP_NP_VP_VP_VBZ': 580, 'B-NP_NP_NP_ADJP_ADJP_JJ': 581, 'B-VP_NP_O-S': 582, 'B-S_NP_NP_RB': 583, 'B-NP_NP_NP_NP_PRN_JJ': 584, 'B-NP_CONJP_RB': 585, 'B-NP_PP_PP_CC': 586, 'B-ADVP_PP_O-VP': 587, 'B-NP_NP_VBP': 588, 'B-S_S_NP_NP_NP_NN': 589, 'B-UCP_CC': 590, 'B-ADVP_CC': 591, 'B-VP_IN': 592, 'B-S_NP_NP_PRP': 593, 'B-ADVP_VP_O-S': 594, 'B-ADVP_JJ': 595, 'B-NP_S_O-VP': 596, 'B-RP': 597, 'B-S_ADVP_FW': 598, 'B-NP_NP_O-NP': 599, 'B-S_NP_NP_NP_NP_DT': 600, 'B-NP_NP_NP_NP_CC': 601, 'B-UCP_VP_VBN': 602, 'B-S_NP_ADJP_ADJP_JJ': 603, 'B-ADVP_JJS': 604, 'B-VP_NP_VBZ': 605, 'B-ADVP_ADJP_O-S': 606, 'B-ADJP_NP_JJ': 607, 'B-ADVP_VP_O-VP': 608, 'B-SBAR_SBAR_WHNP_WDT': 609, 'B-PP_CONJP_RB': 610, 'B-SINV_SBAR_IN': 611, 'O-SINV': 612, 'B-NP_PP_ADVP_RBR': 613, 'B-NP_PP_PP_PP_O-NP': 614, 'B-S_S_NP_NP_NP_JJ': 615, 'B-S_NP_ADJP_ADJP_RB': 616, 'B-S_NP_NP_NP_NP_CD': 617, 'B-S_S_NP_NP_CC': 618, 'B-NP_NP_NP_ADJP_ADJP_NN': 619, 'B-S_PP_VBN': 620, 'B-NP_PRN_PP_IN': 621, 'B-FRAG_PP_VBN': 622, 'B-NP_S_ADVP_O-VP': 623, 'B-NP_PRN_COMMA': 624, 'B-S_NP_VP_O-NP': 625, 'B-NP_PP_VBN': 626, 'B-PRN_O-NP': 627, 'B-NP_NP_NNP': 628, 'B-S_S_NP_ADJP_ADJP_JJ': 629, 'B-SBAR_WHNP_S_NP_NN': 630, 'B-CONJP_JJ': 631, 'B-NP_ADVP_ADVP_FW': 632, 'B-VP_VP_JJR': 633, 'B-NP_NP_NP_PP_SBAR_WHNP_S_O-NP': 634, 'B-CONJP_IN': 635, 'B-NP_NP_ADJP_O-NP': 636, 'B-PRN_COLON': 637, 'B-UCP_SBARQ_WHADVP_WRB': 638, 'B-SQ_NP_DT': 639, 'O-SQ': 640, 'O-SBARQ': 641, 'B-SQ_VBP': 642, 'B-S_PP_ADVP_RB': 643, 'B-ADJP_ADJP_IN': 644, 'B-SQ_SBAR_WHADVP_WRB': 645, 'B-NP_ADJP_ADVP_DT': 646, 'B-RBS': 647, 'B-S_RB': 648, 'B-S_NP_VP_VP_VBG': 649, 'B-SINV_PP_ADVP_RB': 650, 'B-S_PP_TO': 651, 'B-NP_VB': 652, 'B-S_NP_NP_NP_NP_NP_NP_NN': 653, 'B-FRAG_ADVP_RB': 654, 'B-NP_NP_PRN_O-VP': 655, 'B-S_NP_VP_VP_VP_VBG': 656, 'B-SBAR_ADVP_RB': 657, 'B-PP_O-PP': 658, 'B-NP_NP_PP_IN': 659, 'B-S_NP_PDT': 660, 'B-NP_NP_NP_ADVP_PP_JJ': 661, 'B-NP_NP_NP_O-NP': 662, 'B-S_S_NP_ADJP_JJ': 663, 'B-PP_NP_PRPP': 664, 'B-PP_ADVP_CC': 665, 'B-VP_ADJP_JJ': 666, 'B-VP_LQT': 667, 'B-VP_DT': 668, 'B-NP_NP_NP_COMMA': 669, 'B-VP_VP_COMMA': 670, 'B-VP_COMMA': 671, 'B-VP_PP_O-VP': 672, 'B-NP_NP_NP_NP_PRN_NN': 673, 'B-NP_S_NP_DT': 674, 'B-NP_NP_LST_LRB': 675, 'B-S_SBAR_SBAR_IN': 676, 'B-SBAR_CC': 677, 'B-SBAR_COMP_S_NP_NP_JJ': 678, 'B-ADJP_O-S': 679, 'B-WHNP_S_NP_VP_O-SBAR': 680, 'B-NP_NP_PP_COMMA': 681, 'B-SBAR_WRB': 682, 'B-NP_PP_RB': 683, 'B-NP_NP_PP_PP_IN': 684, 'B-ADJP_ADJP_CD': 685, 'B-NP_ADVP_ADVP_ADJP_RB': 686, 'B-NP_NP_TO': 687, 'B-S_S_NP_PRP': 688, 'B-NP_SBAR_SBAR_IN': 689, 'B-NP_VP_TO': 690, 'B-NP_ADJP_ADJP_VBN': 691, 'B-NP_PDT': 692, 'B-NP_S_NP_JJ': 693, 'B-S_NP_QP_CD': 694, 'B-NP_PP_NN': 695, 'B-S_NP_ADJP_CC': 696, 'B-VP_VP_JJ': 697, 'B-WHADVP_WRB': 698, 'B-ADVP_O-ADJP': 699, 'B-NP_SBAR_WDT': 700, 'B-NP_NP_NP_PRN_NP_O-NP': 701, 'B-PRT_IN': 702, 'O-PRT': 703, 'B-NP_NP_ADVP_RB': 704, 'B-S_S_NP_VBN': 705, 'B-S_NP_NP_NP_PRN_CC': 706, 'B-S_NP_ADVP_JJ': 707, 'B-UCP_S_NP_NP_NN': 708, 'B-ADJP_NP_CD': 709, 'B-PRN_PP_IN': 710, 'B-ADJP_NP_QP_RB': 711, 'B-S_NP_NP_PP_S_VP_VBZ': 712, 'B-NP_NP_ADVP_O-PP': 713, 'B-NP_NP_NP_LRB': 714, 'B-NP_S_S_NP_NN': 715, 'B-S_NP_NP_PP_IN': 716, 'B-VP_VP_VBG': 717, 'B-S_S_S_NP_VP_TO': 718, 'B-S_SBAR_RB': 719, 'B-NP_NP_SBAR_O-NP': 720, 'B-S_SBAR_WRB': 721, 'B-VP_ADVP_VBG': 722, 'B-NP_NP_NP_NP_PP_PP_IN': 723, 'B-PP_NN': 724, 'B-NP_ADVP_O-S': 725, 'B-NP_PP_ADVP_O-VP': 726, 'B-NP_RBR': 727, 'B-SBAR_WHADVP_RB': 728, 'B-NP_PP_NP_RB': 729, 'B-S_NP_NP_NN_O-NP': 730, 'B-ADJP_ADJP_DT': 731, 'B-S_UCP_S_NP_ADJP_JJ': 732, 'B-NP_S_NP_NNS': 733, 'B-SBAR_WHNP_WHNP_WPP': 734, 'B-SBAR_WHNP_WHNP_WDT': 735, 'B-SBAR_COMMA': 736, 'B-NP_O-WHNP': 737, 'B-SBAR_S_NP_NN': 738, 'B-SBAR_S_NP_NP_NP_JJ': 739, 'B-SBAR_S_NP_NP_NN': 740, 'B-S_O-PP': 741, 'B-NP_TO': 742, 'B-PP_VBP': 743, 'B-NP_ADVP_O-PP': 744, 'B-NP_LST_LRB': 745, 'B-NP_PP_COMMA': 746, 'B-ADVP_O-S': 747, 'B-NP_PP_PP_JJ': 748, 'B-NP_LRB': 749, 'B-PRN_NN': 750, 'B-NP_NP_IN': 751, 'B-NP_ADJP_ADJP_COMMA': 752, 'B-NP_NP_NP_NNP': 753, 'B-NP_NP_ADJP_NP_O-NP': 754, 'B-NP_ADJP_NP_O-NP': 755, 'B-COMP_NP_DT': 756, 'B-NP_PP_CC': 757, 'B-ADJP_ADJP_FW': 758, 'B-NP_NP_VP_O-VP': 759, 'B-NP_SBAR_WHNP_S_O-NP': 760, 'B-NP_NP_ADJP_RBR': 761, 'B-ADVP_NP_O-PP': 762, 'B-NP_ADJP_QP_RB': 763, 'B-CONJP_NP_O-VP': 764, 'B-S_NP_PP_IN': 765, 'B-S_NP_NP_NP_NP_O-NP': 766, 'B-ADJP_NP_O-NP': 767, 'B-COMP_NP_NN': 768, 'B-NP_NP_NP_NP_IN': 769, 'B-S_NP_NP_NP_COMMA': 770, 'B-CONJP_NP_PP_IN': 771, 'B-PP_NP_QP_JJR': 772, 'B-PP_NP_NP_CD': 773, 'B-QP_CC': 774, 'B-PP_NP_QP_IN': 775, 'B-UCP_NP_QP_JJR': 776, 'B-PP_NP_IN': 777, 'B-NP_NP_NP_ADJP_JJ': 778, 'B-PP_NP_O-VP': 779, 'B-ADVP_ADVP_PP_O-VP': 780, 'B-ADVP_VP_RRB': 781, 'B-UCP_NP_NP_DT': 782, 'B-S_S_NP_ADJP_FW': 783, 'B-NP_NP_PP_ADVP_O-VP': 784, 'B-NP_NP_NP_NP_NP_JJ': 785, 'B-NP_NP_NP_NP_PP_COMMA': 786, 'B-NP_NP_PP_CONJP_RB': 787, 'B-S_S_NP_JJS': 788, 'B-SBAR_SBAR_WHADVP_WRB': 789, 'B-FRAG_NP_JJ': 790, 'B-S_NP_VP_VP_VBD': 791, 'B-PRN_O-ADVP': 792, 'B-S_ADVP_JJ': 793, 'B-NP_S_O-NP': 794, 'B-S_LRB': 795, 'B-NP_VP_VB': 796, 'B-S_S_NP_CD': 797, 'B-UCP_SBAR_IN': 798, 'B-COMP_S_NP_DT': 799, 'B-SINV_VBP': 800, 'B-FRAG_RB': 801, 'B-S_S_S_NP_VP_VBG': 802, 'B-COMP_S_NP_PRPP': 803, 'B-PP_PP_COMMA': 804, 'B-S_COMMA': 805, 'B-NP_QP_DT': 806, 'B-VP_SBAR_O-S': 807, 'B-NP_NP_NP_NP_QP_CD': 808, 'B-VP_RRB': 809, 'B-S_NP_NP_ADJP_ADJP_JJ': 810, 'B-NP_ADJP_ADJP_VB': 811, 'B-COMP_S_NP_NP_DT': 812, 'B-S_NP_NP_IN': 813, 'B-COMP_NP_NP_DT': 814, 'B-NP_NP_ADJP_QP_RB': 815, 'B-WHNP_WP': 816, 'B-NP_NP_PRN_CC': 817, 'B-NP_NP_PP_O-WHNP': 818, 'B-ADJP_NP_RB': 819, 'B-NP_NP_ADJP_ADJP_NP_JJ': 820, 'B-NP_NP_ADJP_JJR': 821, 'B-S_NP_NP_NP_ADJP_JJ': 822, 'B-S_NP_PP_VP_O-SBAR': 823, 'B-S_NP_NP_NP_PRN_O-S': 824, 'B-ADJP_COMMA': 825, 'B-S_S_S_NP_PRP': 826, 'B-S_NP_NP_NP_PP_O-S': 827, 'B-S_NP_PP_TO': 828, 'B-S_PP_NP_QP_RB': 829, 'B-ADJP_ADJP_ADJP_RB': 830, 'B-NP_PP_PP_COMMA': 831, 'B-NP_NP_NP_NP_ADJP_JJ': 832, 'B-S_S_S_NP_NP_NP_CD': 833, 'B-NP_NP_ADVP_NP_ADVP_NP_ADJP_ADJP_CC': 834, 'B-ADJP_NP_O-ADJP': 835, 'B-S_S_SBAR_WHADVP_WRB': 836, 'B-NP_NP_NP_PRN_NNS': 837, 'B-NP_ADJP_NP_PP_O-NP': 838, 'B-S_S_NP_VBG': 839, 'B-NP_VP_O-S': 840, 'B-NP_NP_NP_NP_PP_PRN_LRB': 841, 'B-NP_NP_NP_NP_NP_ADJP_JJ': 842, 'B-S_NP_NP_QP_RB': 843, 'B-NP_NP_NP_NP_NNP': 844, 'B-NP_NP_SBAR_O-VP': 845, 'B-NP_NP_PP_NP_PP_NP_NP_O-PP': 846, 'B-NP_NP_PP_NP_PP_NP_NP_CC': 847, 'B-VP_NP_COMMA': 848, 'B-ADVP_PP_TO': 849, 'B-S_S_S_PP_IN': 850, 'B-NP_NP_ADJP_NP_PP_IN': 851, 'B-NP_NP_NP_NP_NP_DT': 852, 'B-NP_NP_VB': 853, 'B-SBAR_O-VP': 854, 'B-S_PP_CC': 855, 'B-NP_NP_NP_PRN_NP_NP_PP_O-NP': 856, 'B-ADVP_ADJP_JJR': 857, 'B-NP_NP_NP_NP_PP_PP_O-NP': 858, 'B-S_S_SINV_CONJP_RB': 859, 'B-NP_NP_RRB': 860, 'B-PP_PP_SBAR_O-PP': 861, 'B-COMP_S_NP_NP_NN': 862, 'B-NP_ADVP_ADVP_RBR': 863, 'B-S_NP_NP_QP_JJR': 864, 'B-NP_SBAR_RB': 865, 'B-NP_PP_NP_QP_NN': 866, 'B-S_NP_QP_IN': 867, 'B-VP_ADVP_CC': 868, 'B-SBAR_WHNP_NP_CD': 869, 'B-FRAG_SBAR_IN': 870, 'B-NP_NP_NP_PRN_PRN_LRB': 871, 'B-FRAG_NP_O-WHNP': 872, 'B-UCP_VP_ADVP_RB': 873, 'B-NP_PP_PP_TO': 874, 'B-PP_NP_QP_CD': 875, 'B-ADJP_ADJP_NP_RB': 876, 'B-SBAR_JJ': 877, 'B-S_NP_NP_PP_O-S': 878, 'B-NP_UCP_O-VP': 879, 'B-NP_NP_SBAR_WHNP_IN': 880, 'B-ADJP_ADVP_ADVP_RB': 881, 'B-SBAR_WHNP_NP_DT': 882, 'B-PP_PP_O-VP': 883, 'B-S_S_NP_NP_PRPP': 884, 'B-NP_NP_NP_NP_NP_NP_NN': 885, 'B-NP_ADJP_NP_NN': 886, 'B-S_NP_ADVP_FW': 887, 'B-NP_SBAR_O-S': 888, 'B-ADJP_QP_CD': 889, 'B-NP_NP_NP_NP_NP_PRN_CC': 890, 'B-NP_PP_PRN_O-NP': 891, 'B-PRN_RRB': 892, 'B-NP_NP_NP_QP_RB': 893, 'B-PP_NP_QP_NNS': 894, 'B-S_NP_ADJP_ADJP_NP_NN': 895, 'B-ADJP_NP_NN': 896, 'B-PP_COMMA': 897, 'B-ADJP_NP_LRB': 898, 'B-NP_UCP_NP_NN': 899, 'B-NP_NP_NP_PP_CONJP_RB': 900, 'B-S_NP_NNP': 901, 'B-NP_PP_PP_O-NP': 902, 'B-VP_VP_NN': 903, 'B-ADVP_RBS': 904, 'B-ADJP_ADVP_JJ': 905, 'B-S_S_S_NP_NP_NP_DT': 906, 'B-S_NP_VP_ADVP_O-VP': 907, 'B-ADJP_PP_O-VP': 908, 'B-S_S_NP_VP_VP_TO': 909, 'B-NP_NP_NP_ADJP_CC': 910, 'B-S_NP_NP_NP_NP_NP_NN': 911, 'B-QP_FW': 912, 'B-S_NP_NP_ADJP_RBR': 913, 'B-ADVP_NP_FW': 914, 'B-S_S_NP_NNP': 915, 'B-NP_NP_NP_SBAR_COMMA': 916, 'B-S_NP_NP_PP_ADJP_JJ': 917, 'B-NP_NP_NP_NP_PRN_NP_O-NP': 918, 'B-NP_NP_NP_NP_O-NP': 919, 'B-NP_NP_ADJP_ADJP_ADJP_NN': 920, 'B-ADJP_ADJP_LQT': 921, 'B-ADJP_LQT': 922, 'B-NP_NP_NP_NP_FW': 923, 'B-NP_NP_FW': 924, 'B-ADJP_ADJP_NP_CD': 925, 'B-NP_UCP_ADJP_NN': 926, 'B-S_S_NP_NP_QP_RB': 927, 'B-S_NP_ADJP_FW': 928, 'B-ADVP_PP_S_O-S': 929, 'B-SBAR_WHPP_TO': 930, 'B-NP_ADJP_ADVP_ADVP_CC': 931, 'B-ADVP_VBN': 932, 'B-NP_ADJP_ADJP_ADJP_ADJP_NNS': 933, 'B-NP_NP_ADJP_CC': 934, 'B-S_NP_QP_DT': 935, 'B-VP_ADVP_ADVP_RBR': 936, 'B-PP_O-ADVP': 937, 'B-NP_RQT': 938, 'B-VP_O-PP': 939, 'B-NP_NP_VP_VBP': 940, 'B-SBAR_WHNP_S_NP_PRPP': 941, 'B-S_S_PP_VBN': 942, 'B-S_NP_NP_ADJP_ADJP_ADJP_NNS': 943, 'B-QP_VBN': 944, 'B-NP_NP_PP_PP_COMMA': 945, 'B-S_NP_VP_VP_VBP': 946, 'B-S_NP_ADJP_ADVP_O-VP': 947, 'B-S_NP_NP_O-VP': 948, 'B-NP_NP_NP_NP_COMMA': 949, 'B-NP_S_NP_VP_O-VP': 950, 'B-ADVP_ADJP_CD': 951, 'B-FRAG_ADJP_RB': 952, 'B-NP_NP_UCP_NP_NN': 953, 'B-NP_ADJP_NP_LRB': 954, 'B-WHNP_IN': 955, 'B-PP_PP_PRN_O-PP': 956, 'B-FRAG_ADJP_JJ': 957, 'B-S_ADJP_JJ': 958, 'B-NP_NP_PRN_NNS': 959, 'B-S_S_NP_NP_ADJP_ADJP_JJ': 960, 'B-NP_NP_NP_COLON': 961, 'B-NP_NP_NP_QP_IN': 962, 'B-S_S_NP_ADJP_ADJP_VBN': 963, 'B-S_NP_ADJP_ADJP_VBN': 964, 'B-S_S_NP_ADJP_CC': 965, 'B-S_IN': 966, 'B-NP_NP_PP_PP_PP_O-PP': 967, 'B-NP_PP_ADVP_IN': 968, 'B-NP_NP_NP_NP_PP_TO': 969, 'B-VP_PP_RB': 970, 'B-NP_VP_VBZ': 971, 'B-SBAR_WHNP_NP_NP_CC': 972, 'B-NP_WHPP_IN': 973, 'B-WHNP_O-WHNP': 974, 'B-NP_ADVP_O-NP': 975, 'B-SBAR_WHADVP_S_NP_NP_JJ': 976, 'B-NP_NP_PRN_JJ': 977, 'B-SBAR_SBAR_CC': 978, 'B-S_NP_NP_NP_ADJP_FW': 979, 'B-S_S_ADVP_RBR': 980, 'B-FRAG_ADJP_ADJP_RB': 981, 'B-NP_PP_PP_ADVP_IN': 982, 'B-PRT_RP': 983, 'B-NP_ADJP_NP_VBD': 984, 'B-S_NP_VP_PP_IN': 985, 'B-ADJP_ADJP_PRN_NP_O-NP': 986, 'B-ADJP_ADJP_PRN_NP_O-ADJP': 987, 'B-NP_ADVP_RBR': 988, 'B-CONJP_NP_RRB': 989, 'B-FRAG_ADVP_JJR': 990, 'B-NP_ADJP_ADJP_NP_NN': 991, 'B-NP_PP_PP_PP_O-VP': 992, 'B-NP_NP_PP_PP_PP_IN': 993, 'B-VP_NP_CD': 994, 'B-NP_ADJP_QP_IN': 995, 'B-S_S_NP_NP_NP_NP_DT': 996, 'B-NP_NP_PP_PP_NP_O-VP': 997, 'B-NP_NP_NP_PP_PP_LRB': 998, 'B-NP_NP_PP_VP_RRB': 999, 'B-NP_NP_PRN_RRB': 1000, 'B-VP_VP_LST_LRB': 1001, 'B-VP_LST_LRB': 1002, 'B-NP_NP_NP_VP_NP_COMMA': 1003, 'B-ADVP_COMMA': 1004, 'B-NP_NP_ADJP_ADJP_NP_NN': 1005, 'B-NP_NP_CONJP_NP_O-VP': 1006, 'B-SBAR_WHNP_S_NP_VP_VP_TO': 1007, 'B-NP_PP_NP_JJ': 1008, 'B-VP_NP_VP_TO': 1009, 'B-S_S_S_NP_DT': 1010, 'B-NP_UCP_CC': 1011, 'B-ADVP_O-NP': 1012, 'B-VP_ADVP_VBN': 1013, 'B-NP_QP_JJ': 1014, 'B-UCP_NP_NN': 1015, 'B-NP_NP_NP_ADJP_FW': 1016, 'B-FRAG_NP_NP_NN': 1017, 'B-VP_ADVP_RBS': 1018, 'B-NP_NP_O-VP': 1019, 'B-S_S_PP_TO': 1020, 'B-PP_PP_O-S': 1021, 'B-S_NP_ADJP_ADJP_ADJP_JJ': 1022, 'B-NP_NP_NP_ADJP_ADVP_IN': 1023, 'B-ADJP_ADVP_QP_RB': 1024, 'B-VP_ADJP_RB': 1025, 'B-S_NP_NP_PDT': 1026, 'B-NP_NP_NP_NP_NP_COMMA': 1027, 'B-NP_NP_ADJP_NP_NN': 1028, 'B-ADJP_DT': 1029, 'B-S_NP_NP_NNP': 1030, 'B-NP_PP_PP_ADVP_RB': 1031, 'B-NP_NP_ADJP_NP_NP_NN': 1032, 'B-NP_ADVP_COMMA': 1033, 'B-PP_ADVP_FW': 1034, 'B-PP_PP_PP_CC': 1035, 'B-NP_ADVP_ADVP_CD': 1036, 'B-ADVP_O-ADVP': 1037, 'B-SINV_PP_IN': 1038, 'B-ADJP_NP_O-PP': 1039, 'B-S_NP_NP_PP_VP_VBD': 1040, 'B-NP_NP_NP_JJS': 1041, 'B-NP_NP_NP_NP_PRN_NNS': 1042, 'B-NP_NP_CONJP_RB': 1043, 'B-NP_NP_ADJP_NP_JJ': 1044, 'B-NP_NP_VP_VBZ': 1045, 'B-VP_SBAR_IN': 1046, 'B-PP_NP_NP_COMMA': 1047, 'B-NP_UCP_NNS': 1048, 'B-SBAR_WHADVP_S_NP_DT': 1049, 'B-SBAR_COMP_S_NP_DT': 1050, 'B-S_PP_RB': 1051, 'B-QP_RRB': 1052, 'B-ADVP_PP_O-S': 1053, 'B-S_S_S_NP_CD': 1054, 'B-NP_NP_NP_ADJP_ADJP_ADJP_JJ': 1055, 'B-PP_PP_RRB': 1056, 'B-NP_NP_QP_RBR': 1057, 'B-ADVP_NN': 1058, 'B-S_PP_PP_IN': 1059, 'B-VP_VP_ADVP_ADVP_RB': 1060, 'B-ADJP_ADVP_RBR': 1061, 'B-ADJP_ADJP_NP_ADJP_O-ADJP': 1062, 'B-PP_RP': 1063, 'B-ADJP_ADJP_ADJP_JJ': 1064, 'B-S_S_NP_EX': 1065, 'B-S_PP_NP_NP_DT': 1066, 'B-NP_ADJP_ADJP_JJR': 1067, 'B-NP_NP_NP_ADJP_NP_CD': 1068, 'B-S_NP_COMMA': 1069, 'B-NP_VP_VP_COMMA': 1070, 'B-NNPS': 1071, 'B-NP_VBD': 1072, 'B-S_NP_NP_NP_ADJP_NP_O-NP': 1073, 'B-NP_ADJP_ADJP_ADJP_CC': 1074, 'B-NP_NP_NP_PP_CONJP_CC': 1075, 'B-NP_NP_QP_NP_O-NP': 1076, 'B-NP_QP_NP_O-NP': 1077, 'B-NP_ADJP_ADJP_ADJP_NN': 1078, 'B-NP_NP_PP_PP_VBN': 1079, 'B-PP_ADVP_O-VP': 1080, 'B-ADJP_ADJP_ADJP_NN': 1081, 'B-ADJP_PP_O-S': 1082, 'B-ADVP_NP_QP_RB': 1083, 'B-NP_PP_S_NP_VP_O-VP': 1084, 'B-ADVP_ADVP_IN': 1085, 'B-NP_NP_S_O-PP': 1086, 'B-NP_NP_NP_NP_ADJP_PP_TO': 1087, 'B-NP_NP_NP_ADJP_PP_TO': 1088, 'B-S_S_NP_RB': 1089, 'B-ADVP_ADVP_ADVP_JJ': 1090, 'B-S_COLON': 1091, 'B-NP_NP_PP_VP_VBD': 1092, 'B-FRAG_NP_DT': 1093, 'B-ADVP_NP_CD': 1094, 'B-S_S_NP_PRPP': 1095, 'B-S_NP_NP_ADJP_CD': 1096, 'B-NP_NP_NP_PP_VP_VBN': 1097, 'B-SBAR_WHNP_COMMA': 1098, 'B-NP_PP_FW': 1099, 'B-ADJP_ADJP_ADVP_ADVP_RB': 1100, 'B-NP_NP_ADJP_QP_JJR': 1101, 'B-NP_ADJP_NP_CD': 1102, 'B-WHNP_S_NP_ADVP_VP_O-SBAR': 1103, 'B-SBAR_WHNP_WHADJP_WRB': 1104, 'O-WHADJP': 1105, 'B-NP_NP_QP_VBG': 1106, 'B-NP_NP_NP_NP_PRN_VP_VBN': 1107, 'B-NP_PP_ADVP_O-NP': 1108, 'B-NP_NP_NP_PRN_VP_VBN': 1109, 'B-S_NP_NP_ADVP_FW': 1110, 'B-PP_RRB': 1111, 'B-S_S_NP_NP_PDT': 1112, 'B-NP_NP_QP_NNS': 1113, 'B-S_S_LST_NP_VP_COLON': 1114, 'B-S_LST_NP_VP_COLON': 1115, 'B-S_LST_PP_COMMA': 1116, 'B-PP_CONJP_PP_COMMA': 1117, 'B-CONJP_PP_O-NP': 1118, 'B-ADJP_NP_VP_VBN': 1119, 'B-S_NP_NP_SBAR_VP_VBZ': 1120, 'B-NP_SBAR_O-NP': 1121, 'B-NP_ADJP_NP_QP_CD': 1122, 'B-NP_NP_NP_PP_PP_JJ': 1123, 'B-NP_NP_PP_PP_PP_VBG': 1124, 'B-NP_PP_O-PP': 1125, 'B-S_NP_NP_ADVP_IN': 1126, 'B-NP_NP_PP_PRN_O-PP': 1127, 'B-NP_NP_NP_ADJP_ADJP_RB': 1128, 'B-ADJP_RRB': 1129, 'B-SINV_VBZ': 1130, 'B-S_S_LST_LS': 1131, 'B-NP_ADJP_NP_CONJP_NP_NP_O-PP': 1132, 'B-NP_NP_NP_PRN_NP_JJ': 1133, 'B-S_S_ADVP_JJ': 1134, 'B-NP_ADJP_ADJP_NP_VBD': 1135, 'B-S_NP_NP_VP_VP_O-S': 1136, 'B-SBAR_WHADJP_WDT': 1137, 'B-ADVP_ADJP_QP_NNS': 1138, 'B-S_NP_NP_NP_RB': 1139, 'B-ADJP_PP_TO': 1140, 'B-S_NP_NP_NP_NN_O-NP': 1141, 'B-S_NP_VP_VP_VB': 1142, 'B-NP_ADVP_RBS': 1143, 'B-S_NP_NP_NP_ADJP_ADJP_JJ': 1144, 'B-S_NP_NP_QP_IN': 1145, 'B-ADJP_O-SINV': 1146, 'B-ADJP_ADJP_PRN_NN': 1147, 'B-ADVP_ADVP_ADVP_RB': 1148, 'B-NP_NP_ADJP_QP_CD': 1149, 'B-NP_ADVP_ADVP_QP_RB': 1150, 'B-NP_NP_ADJP_ADJP_ADJP_ADJP_NNS': 1151, 'B-S_NP_NP_PP_PP_VP_VBZ': 1152, 'B-S_ADVP_O-VP': 1153, 'B-NP_NP_NP_VP_COMMA': 1154, 'B-NP_NP_NP_ADJP_NP_O-NP': 1155, 'B-S_ADVP_RBS': 1156, 'B-ADVP_ADVP_ADVP_RBR': 1157, 'B-S_NP_NP_PP_VP_O-S': 1158, 'B-S_NP_NP_NP_NP_PRN_COMMA': 1159, 'B-NP_NP_ADJP_PP_IN': 1160, 'B-NP_NP_QP_NP_NNS': 1161, 'B-S_NP_O-SBAR': 1162, 'B-NP_NP_NP_PP_COLON': 1163, 'B-NP_NP_PP_PP_ADVP_IN': 1164, 'B-NP_NP_NP_NP_ADJP_ADJP_JJ': 1165, 'B-NP_NP_ADJP_NP_RB': 1166, 'B-S_S_NP_NP_NP_CD': 1167, 'B-ADJP_ADVP_FW': 1168, 'B-NP_NP_ADJP_ADJP_FW': 1169, 'B-NP_NP_NP_NP_PP_CC': 1170, 'B-SBAR_SBAR_SBAR_IN': 1171, 'B-S_S_ADVP_O-S': 1172, 'B-S_NP_NP_NP_PP_CC': 1173, 'B-NP_PRT_IN': 1174, 'B-UCP_ADJP_NP_NN': 1175, 'B-S_NP_ADJP_ADJP_FW': 1176, 'B-NP_NP_NP_NP_VP_PRN_LRB': 1177, 'B-NP_NP_NP_ADJP_PP_IN': 1178, 'B-NP_NP_RBR': 1179, 'B-S_NP_VP_VP_MD': 1180, 'B-NP_NP_PP_SBAR_O-PP': 1181, 'B-S_S_NP_NP_FW': 1182, 'B-S_S_NP_FW': 1183, 'B-NP_ADJP_NP_NNS': 1184, 'B-NP_PP_SBAR_WHNP_S_O-NP': 1185, 'B-S_S_S_SBAR_IN': 1186, 'B-ADJP_ADJP_ADJP_JJR': 1187, 'B-NP_ADJP_JJR': 1188, 'B-S_NP_NP_VP_VP_VBD': 1189, 'B-SBAR_O-S': 1190, 'B-S_NP_QP_JJR': 1191, 'B-NP_NP_NP_PRN_ADVP_JJ': 1192, 'B-NP_ADJP_ADJP_ADJP_NNS': 1193, 'B-S_S_NP_NP_NP_ADJP_FW': 1194, 'B-PRT_RB': 1195, 'B-ADJP_ADVP_ADVP_ADVP_VBN': 1196, 'B-NP_NP_NP_ADJP_NNS': 1197, 'B-S_S_S_NP_NP_DT': 1198, 'B-VP_ADVP_DT': 1199, 'B-S_NP_ADJP_ADVP_JJ': 1200, 'B-NP_NP_ADJP_QP_JJ': 1201, 'B-NP_NP_SBAR_WHNP_S_O-NP': 1202, 'B-ADJP_ADJP_RBS': 1203, 'B-NP_NP_NP_ADJP_RBR': 1204, 'B-FRAG_DT': 1205, 'B-SBAR_WHNP_S_O-VP': 1206, 'B-VP_S_NP_VP_O-VP': 1207, 'B-ADVP_ADVP_NN': 1208, 'B-NP_PP_NP_QP_NP_O-NP': 1209, 'B-VP_NP_PP_TO': 1210, 'B-UCP_PP_IN': 1211, 'B-NP_PRT_RP': 1212, 'B-NP_NP_ADJP_NP_CD': 1213, 'B-NP_ADJP_NP_NP_CC': 1214, 'B-S_S_LST_CD': 1215, 'B-S_LST_CD': 1216, 'B-VP_RP': 1217, 'B-ADVP_NP_NN': 1218, 'B-NP_NP_VP_COMMA': 1219, 'B-CONJP_NP_COMMA': 1220, 'B-NP_NP_ADVP_O-VP': 1221, 'B-NP_NP_PRN_LRB': 1222, 'B-WHNP_S_NP_VP_PP_O-S': 1223, 'B-PP_ADJP_RB': 1224, 'B-SBAR_SBAR_WHADVP_S_NP_VP_TO': 1225, 'B-NP_ADJP_NP_JJ': 1226, 'B-NP_PP_NP_PP_NP_NN': 1227, 'B-PP_NP_QP_RB': 1228, 'B-NP_NP_NP_NP_NP_NNS': 1229, 'B-ADJP_O-PP': 1230, 'B-ADJP_ADJP_ADVP_IN': 1231, 'B-SBAR_DT': 1232, 'B-S_NP_NP_NP_PRN_NNS': 1233, 'B-NP_PP_NP_CD': 1234, 'B-NP_ADVP_CC': 1235, 'B-PP_PP_O-NP': 1236, 'B-S_NP_NP_PP_VP_VBG': 1237, 'B-NP_NP_PP_SBAR_O-VP': 1238, 'B-S_NP_ADVP_VP_O-PP': 1239, 'B-NP_NP_ADJP_COMMA': 1240, 'B-S_NP_NP_ADVP_RB': 1241, 'B-NP_NP_NP_PP_SBAR_IN': 1242, 'B-SBAR_WHPP_PP_S_NP_VP_O-SBAR': 1243, 'B-NP_NP_PRN_ADVP_RB': 1244, 'B-NP_PP_SBAR_O-S': 1245, 'B-S_NP_NP_NP_PRN_LRB': 1246, 'B-SINV_ADJP_JJ': 1247, 'B-NP_O-SINV': 1248, 'B-NP_NP_NP_ADJP_QP_RB': 1249, 'B-S_NP_ADJP_O-VP': 1250, 'B-NP_NP_NP_PP_PP_VBG': 1251, 'B-S_S_NP_NP_NP_VBG': 1252, 'B-COMP_S_NP_NP_CC': 1253, 'B-UCP_UCP_S_NP_VP_VBG': 1254, 'B-S_SQ_VBZ': 1255, 'B-NP_NP_ADJP_RRB': 1256, 'B-ADJP_O-NP': 1257, 'B-VP_ADVP_ADVP_CC': 1258, 'B-NP_PRN_O-NP': 1259, 'B-PP_NP_DT': 1260, 'B-S_S_S_ADVP_RB': 1261, 'B-S_NP_VP_CC': 1262, 'B-SINV_ADJP_ADJP_JJ': 1263, 'B-S_NP_NP_ADJP_QP_CD': 1264, 'B-UCP_ADJP_RB': 1265, 'B-VP_VP_ADVP_VBN': 1266, 'B-NP_PP_PRN_CC': 1267, 'B-VP_ADVP_PP_PRN_VP_O-VP': 1268, 'B-S_NP_NP_RRB': 1269, 'B-VP_NP_NN': 1270, 'B-S_NP_NP_COMMA': 1271, 'B-S_NP_NP_NP_QP_CD': 1272, 'B-SBAR_SBAR_LST_LRB': 1273, 'B-S_S_NP_NP_NP_ADJP_JJ': 1274, 'B-S_NP_VP_VP_ADVP_RB': 1275, 'B-NP_NP_NP_NP_SBAR_WHNP_S_O-NP': 1276, 'B-NP_S_NP_NP_NNS': 1277, 'B-NP_PP_PP_PP_IN': 1278, 'B-UCP_UCP_S_NP_NP_JJ': 1279, 'B-S_S_NP_NP_ADJP_JJ': 1280, 'B-NP_NP_NP_NP_NP_NP_ADJP_JJ': 1281, 'B-NP_NP_NP_NP_NP_O-NP': 1282, 'B-S_S_S_SBAR_WHADVP_WRB': 1283, 'B-NP_PP_NP_QP_CD': 1284, 'B-PP_NP_O-NP': 1285, 'B-ADJP_ADJP_ADJP_QP_RB': 1286, 'B-NP_ADJP_COMMA': 1287, 'B-VP_S_O-S': 1288, 'B-SBAR_WHADVP_S_NP_VP_O-SBAR': 1289, 'B-NP_PP_RRB': 1290, 'B-S_NP_NP_NP_NP_PRN_PP_IN': 1291, 'B-NP_QP_NNS': 1292, 'B-NP_ADJP_ADJP_ADJP_ADJP_PRN_JJ': 1293, 'B-NP_NP_NP_NP_NP_NP_JJ': 1294, 'B-S_NP_ADJP_O-S': 1295, 'B-NP_NP_O-ADJP': 1296, 'B-NP_NP_NP_PP_ADVP_FW': 1297, 'B-S_S_S_S_NP_VP_TO': 1298, 'B-NP_ADVP_PP_O-NP': 1299, 'B-S_S_S_NP_NP_NNS': 1300, 'B-S_NP_VP_RRB': 1301, 'B-NP_LS': 1302, 'B-ADJP_VBD': 1303, 'B-S_PP_FW': 1304, 'B-S_S_NP_VP_NN': 1305, 'B-VP_NP_LST_LS': 1306, 'B-PP_S_NP_VP_VBG': 1307, 'B-ADJP_QP_JJR': 1308, 'B-NP_NP_VBZ': 1309, 'B-SBAR_O-ADJP': 1310, 'B-ADJP_PP_IN': 1311, 'B-NP_VBZ': 1312, 'B-S_NP_ADJP_QP_CD': 1313, 'B-NP_NP_VP_RRB': 1314, 'B-NP_PP_PP_O-S': 1315, 'B-NP_NP_VP_VBG': 1316, 'B-S_NP_NP_NP_ADJP_PP_IN': 1317, 'B-NP_NP_NP_PP_PP_TO': 1318, 'B-S_NP_NP_NP_PP_PP_IN': 1319, 'B-ADVP_ADJP_CC': 1320, 'B-PRN_O-ADJP': 1321, 'B-S_NP_QP_NN': 1322, 'B-NP_NP_NP_PRN_JJ': 1323, 'B-NP_NP_NP_NP_NP_PP_CC': 1324, 'B-NP_NP_PRN_NP_NN': 1325, 'B-SBARQ_WHNP_WP': 1326, 'B-SQ_VBZ': 1327, 'B-NP_O-SQ': 1328, 'B-SBARQ_SBARQ_WHADJP_WRB': 1329, 'B-ADJP_O-SQ': 1330, 'B-SBARQ_WHNP_WDT': 1331, 'B-UCP_SQ_VBP': 1332, 'B-ADJP_NP_DT': 1333, 'B-SBARQ_WHNP_WHNP_WRB': 1334, 'B-SQ_NP_VP_VBP': 1335, 'B-SINV_CONJP_CC': 1336, 'B-S_NP_QP_RB': 1337, 'B-LST_CD': 1338, 'B-NP_RBS': 1339, 'B-NP_NP_PP_SBAR_IN': 1340, 'B-UCP_NP_CC': 1341, 'B-S_SBAR_WHADVP_RB': 1342, 'B-ADVP_NP_NP_DT': 1343, 'B-ADVP_NP_COMMA': 1344, 'B-NP_ADJP_NP_NP_NP_NN': 1345, 'B-NP_UCP_S_NP_VP_VBG': 1346, 'B-S_NP_NP_NP_NP_NNS': 1347, 'B-S_NP_NP_NP_PRN_NN': 1348, 'B-NP_ADVP_NP_JJ': 1349, 'B-SBAR_COMP_S_NP_NP_NN': 1350, 'B-PP_VBD': 1351, 'B-NP_QP_QP_CD': 1352, 'B-NP_NP_VP_ADVP_O-VP': 1353, 'B-NP_NP_SBAR_WHPP_IN': 1354, 'B-NP_NP_NP_NP_VBG': 1355, 'B-VP_ADVP_FW': 1356, 'B-NP_UCP_NP_JJ': 1357, 'B-UCP_UCP_CC': 1358, 'B-NP_QP_QP_QP_CD': 1359, 'B-QP_QP_CD': 1360, 'B-NP_ADJP_NP_NP_JJ': 1361, 'B-NP_NP_UCP_CC': 1362, 'B-NP_ADVP_DT': 1363, 'B-S_NP_UCP_CC': 1364, 'B-UCP_NNS': 1365, 'B-VP_ADVP_IN': 1366, 'B-S_S_S_NP_NP_NP_NN': 1367, 'B-S_NP_NP_PRN_VP_VBP': 1368, 'B-NP_S_NP_VP_PP_O-VP': 1369, 'B-NP_UCP_NP_COMMA': 1370, 'B-S_CONJP_RB': 1371, 'B-S_UCP_S_NP_VP_TO': 1372, 'B-NP_NP_ADJP_PRN_NNS': 1373, 'B-NP_CONJP_NP_CONJP_NP_O-PP': 1374, 'B-ADJP_ADJP_ADJP_CC': 1375, 'B-ADJP_ADJP_CONJP_ADJP_NN': 1376, 'B-S_NP_VP_VP_PP_O-S': 1377, 'B-UCP_NP_NP_JJ': 1378, 'B-SBAR_WHADVP_S_O-NP': 1379, 'B-NP_NP_PRN_PP_TO': 1380, 'B-S_NP_NP_ADJP_CONJP_RB': 1381, 'B-ADVP_O-PP': 1382, 'B-NP_VP_ADVP_VBD': 1383, 'B-S_NP_NP_PP_RRB': 1384, 'B-UCP_NP_CD': 1385, 'B-S_NP_VP_VP_VP_VBZ': 1386, 'B-NP_ADJP_ADVP_ADVP_RB': 1387, 'B-ADJP_ADJP_ADVP_NP_RBR': 1388, 'B-NP_O-FRAG': 1389, 'B-S_NP_NP_NP_ADJP_NP_NP_NN': 1390, 'B-FRAG_ADJP_NN': 1391, 'B-S_S_S_NP_VBN': 1392, 'B-NP_NP_PP_CONJP_CC': 1393, 'B-SBAR_SBAR_WHPP_IN': 1394, 'B-NP_NP_PP_VP_VBZ': 1395, 'B-S_NP_NP_NP_PRPP': 1396, 'B-NP_PP_PP_NP_O-VP': 1397, 'B-S_NP_LQT': 1398, 'B-S_SBAR_NP_VP_O-SBAR': 1399, 'B-QP_RB': 1400, 'B-NP_NP_NP_NP_CONJP_NP_PP_IN': 1401, 'B-ADVP_PRN_LRB': 1402, 'B-NP_ADJP_ADJP_CD': 1403, 'B-ADJP_ADJP_PRN_CC': 1404, 'B-ADJP_ADJP_PRN_O-ADJP': 1405, 'B-NP_NP_PP_S_O-VP': 1406, 'B-S_S_NP_PDT': 1407, 'B-NP_NP_PP_VP_O-PP': 1408, 'B-NP_ADJP_VBN': 1409, 'B-NP_VP_O-SINV': 1410, 'B-S_NP_VP_JJ': 1411, 'B-S_NP_ADJP_ADJP_JJR': 1412, 'B-VP_PP_ADVP_RB': 1413, 'B-FRAG_NP_CD': 1414, 'B-S_S_COMMA': 1415, 'B-VP_NP_NP_NN': 1416, 'B-S_S_NP_VP_COMMA': 1417, 'B-NP_NP_O-S': 1418, 'B-FRAG_VP_ADVP_RB': 1419, 'B-S_NP_NP_ADJP_ADJP_JJR': 1420, 'B-ADJP_ADVP_JJR': 1421, 'B-NP_NP_NP_NNPS': 1422, 'B-FRAG_NP_QP_CD': 1423, 'B-FRAG_NP_NP_COLON': 1424, 'B-FRAG_FRAG_NP_NN': 1425, 'B-FRAG_NP_NNS': 1426, 'B-S_S_S_NP_NNS': 1427, 'B-FRAG_PP_TO': 1428, 'B-NP_NP_NP_PRN_CD': 1429, 'B-NP_ADJP_ADJP_DT': 1430, 'B-FRAG_LRB': 1431, 'B-PRN_LQT': 1432, 'B-NP_NP_CONJP_NP_COMMA': 1433, 'B-S_NP_ADJP_RBR': 1434, 'B-FRAG_NP_NP_QP_RB': 1435, 'B-ADJP_LRB': 1436, 'B-SBAR_WHNP_WHNP_COMMA': 1437, 'B-S_NP_ADJP_PP_JJ': 1438, 'B-NP_NP_UCP_ADJP_JJ': 1439, 'B-WHNP_S_S_CC': 1440, 'B-S_NP_S_VBP': 1441, 'B-VP_VP_VP_ADVP_RB': 1442, 'B-ADJP_ADVP_ADVP_IN': 1443, 'B-NP_NP_UCP_NP_COMMA': 1444, 'B-PDT': 1445, 'B-VP_VP_VP_VP_VBZ': 1446, 'B-S_S_NP_VP_ADVP_RB': 1447, 'B-FRAG_ADJP_DT': 1448, 'B-SBAR_SBAR_WHNP_IN': 1449, 'B-S_NP_NP_NP_ADJP_RB': 1450, 'B-NP_NP_ADJP_PP_RB': 1451, 'B-ADJP_NP_ADJP_O-ADJP': 1452, 'B-NP_NP_ADJP_CONJP_ADJP_COMMA': 1453, 'B-CONJP_ADJP_COMMA': 1454, 'B-NP_NP_NP_PP_VP_VBG': 1455, 'B-FRAG_NP_NP_NP_QP_CD': 1456, 'B-FRAG_ADJP_ADJP_JJ': 1457, 'B-NP_NP_UCP_NP_CC': 1458, 'B-ADVP_NP_QP_CD': 1459, 'B-NP_SBAR_SBAR_WHNP_WDT': 1460, 'B-S_S_NP_NP_SBAR_VP_MD': 1461, 'B-VP_PP_VBN': 1462, 'B-SBAR_WHNP_S_NP_NP_DT': 1463, 'B-NP_NP_NP_CONJP_NP_NP_NN': 1464, 'B-S_SBAR_WHNP_S_O-NP': 1465, 'B-S_PP_NP_VP_O-SBAR': 1466, 'B-ADJP_NP_VBN': 1467, 'B-S_NP_NP_O-NP': 1468, 'B-NP_ADJP_ADJP_NP_ADJP_O-ADJP': 1469, 'B-NP_NP_NP_ADVP_RB': 1470, 'B-SINV_VP_VBZ': 1471, 'B-FRAG_NP_NP_NNP': 1472, 'B-PP_PP_PP_PP_NP_NN': 1473, 'B-NP_ADVP_NP_CD': 1474, 'B-UCP_S_NP_VP_TO': 1475, 'B-SINV_ADJP_DT': 1476, 'B-FRAG_NP_QP_NN': 1477, 'B-NP_NP_NP_NP_SBAR_O-NP': 1478, 'B-NP_O-UCP': 1479, 'B-FRAG_ADVP_O-PP': 1480, 'B-PP_QP_IN': 1481, 'B-SBAR_ADVP_WRB': 1482, 'B-NP_ADJP_QP_QP_COMMA': 1483, 'B-QP_NN': 1484, 'B-S_UCP_PP_IN': 1485, 'B-SBAR_WHNP_S_NP_PRP': 1486, 'B-FRAG_COLON': 1487, 'B-FRAG_O-NP': 1488, 'B-ADJP_ADJP_SBAR_O-VP': 1489, 'B-ADJP_NP_QP_NN': 1490, 'B-NP_NP_PP_ADVP_O-NP': 1491, 'B-RRC_ADJP_JJ': 1492, 'O-RRC': 1493, 'B-NP_PP_PP_PP_TO': 1494, 'B-VP_S_NP_ADJP_JJ': 1495, 'B-FRAG_NP_QP_JJR': 1496, 'B-SQ_MD': 1497, 'B-S_FRAG_PP_IN': 1498, 'B-UCP_ADJP_CC': 1499, 'B-S_NP_S_NP_VP_VBN': 1500, 'B-ADVP_NP_O-VP': 1501, 'B-ADJP_ADJP_ADJP_JJS': 1502, 'B-S_NP_PP_ADVP_RB': 1503, 'B-ADJP_ADJP_VBG': 1504, 'B-NP_S_NP_VP_VBN': 1505, 'B-S_NP_NP_PP_VP_VP_ADVP_O-VP': 1506, 'B-NP_ADJP_ADVP_RB': 1507, 'B-NP_NP_ADJP_ADVP_VBN': 1508, 'B-NP_UCP_ADJP_NNS': 1509, 'B-FRAG_COMMA': 1510, 'B-PRT_NP_O-VP': 1511, 'B-SBAR_WHNP_NP_JJS': 1512, 'B-NP_QP_ADJP_NN': 1513, 'B-NP_QP_NP_CD': 1514, 'B-NP_NP_NP_NP_ADJP_COMMA': 1515, 'B-NP_NP_NP_NP_NP_CD': 1516, 'B-ADJP_ADJP_PP_O-NP': 1517, 'B-S_S_NP_NP_NP_QP_RB': 1518, 'B-NP_NP_PRN_PRN_CC': 1519, 'B-NP_NP_NP_ADJP_COMMA': 1520, 'B-NP_QP_O-S': 1521, 'B-S_NP_ADJP_ADVP_FW': 1522, 'B-FRAG_NP_NP_QP_CD': 1523, 'B-NP_ADJP_DT': 1524, 'B-FRAG_JJ': 1525, 'B-FRAG_ADVP_ADVP_JJR': 1526, 'B-S_S_NP_NP_ADJP_RB': 1527, 'B-VP_VP_PP_PP_O-VP': 1528, 'B-S_NP_PP_COMMA': 1529, 'B-VP_O-SBAR': 1530, 'B-NP_ADVP_PP_IN': 1531, 'B-NP_ADJP_ADVP_FW': 1532, 'B-ADVP_ADVP_JJR': 1533, 'B-NP_NP_PP_ADJP_O-PP': 1534, 'B-S_NP_NP_NP_NP_QP_CD': 1535, 'B-NP_NP_NP_ADJP_QP_CD': 1536, 'B-NP_NP_NP_NP_NP_ADJP_FW': 1537, 'B-SBAR_SBAR_WHNP_NP_CD': 1538, 'B-SBAR_S_NP_NP_DT': 1539, 'B-NP_NP_NP_O-VP': 1540, 'B-SBAR_S_NP_WHNP_WDT': 1541, 'B-PP_O-UCP': 1542, 'B-FRAG_NP_NP_DT': 1543, 'B-UCP_ADVP_ADVP_RB': 1544, 'B-ADJP_NP_CC': 1545, 'B-S_ADVP_NN': 1546, 'B-ADJP_ADJP_ADJP_ADJP_NP_CD': 1547, 'B-VP_NP_NP_QP_CD': 1548, 'B-VP_VP_VP_PP_IN': 1549, 'B-VP_NP_QP_CD': 1550, 'B-NP_NP_ADJP_QP_IN': 1551, 'B-S_ADVP_WRB': 1552, 'B-ADVP_ADVP_CC': 1553, 'B-ADVP_VP_VBN': 1554, 'B-ADVP_NP_NP_PP_PP_O-VP': 1555, 'B-S_NP_UCP_VP_VBN': 1556, 'B-ADVP_O-UCP': 1557, 'B-S_NP_NP_NP_QP_RB': 1558, 'B-PP_NP_TO': 1559, 'B-COMP_S_NP_NN': 1560, 'B-PRP': 1561, 'B-NP_ADVP_JJ': 1562, 'B-S_S_NP_NP_PP_VP_VBD': 1563, 'B-NP_NP_ADVP_VP_NP_CC': 1564, 'B-ADVP_NP_O-S': 1565, 'B-NP_NP_NP_ADJP_VBN': 1566, 'B-S_UCP_S_NP_VP_VBG': 1567, 'B-ADJP_PP_O-ADJP': 1568, 'B-S_NP_NP_ADJP_QP_RB': 1569, 'B-NP_NP_PP_NP_NN': 1570, 'B-S_NP_MD': 1571, 'B-S_NP_ADJP_PP_O-VP': 1572, 'B-NP_NP_ADVP_O-NP': 1573, 'B-NP_VP_PP_O-SBAR': 1574, 'B-SBAR_CONJP_RB': 1575, 'B-S_VP_TO': 1576, 'B-ADJP_PRN_NP_O-NP': 1577, 'B-NP_NP_NP_NP_PRN_LRB': 1578, 'B-ADVP_JJR': 1579, 'B-S_NP_NP_NP_QP_IN': 1580, 'B-S_S_NP_JJR': 1581, 'B-SBAR_SBAR_O-NP': 1582, 'B-WHPP_WDT': 1583, 'B-S_NP_NP_O-PP': 1584, 'B-S_NP_NP_VP_VBZ': 1585, 'B-S_S_NP_NP_NP_NP_JJ': 1586, 'B-COMP_S_NP_NP_PRPP': 1587, 'B-NP_NP_ADJP_JJS': 1588, 'B-NP_NP_PP_NP_NP_PRN_O-NP': 1589, 'B-SINV_VP_VBP': 1590, 'B-NP_NP_PP_S_O-PP': 1591, 'B-PP_PP_ADVP_IN': 1592, 'B-S_NP_CONJP_RB': 1593, 'B-ADJP_ADJP_ADJP_NNS': 1594, 'B-ADJP_ADJP_ADVP_ADJP_NN': 1595, 'B-UCP_ADJP_JJS': 1596, 'B-NP_NP_NP_PP_SBAR_WHPP_S_O-NP': 1597, 'B-S_NP_NP_PRN_VP_VBZ': 1598, 'B-S_ADJP_RB': 1599, 'B-UCP_SBAR_DT': 1600, 'B-SINV_VP_VBG': 1601, 'B-S_S_NP_VP_JJ': 1602, 'B-NP_NP_ADJP_NP_COMMA': 1603, 'B-SQ_SQ_VBZ': 1604, 'B-S_PP_NP_QP_CD': 1605, 'B-ADJP_QP_NN': 1606, 'B-NP_NP_NP_ADVP_O-VP': 1607, 'B-UCP_VP_ADVP_VBN': 1608, 'B-ADJP_ADJP_ADJP_ADVP_RB': 1609, 'B-NP_CONJP_IN': 1610, 'B-NP_UCP_ADVP_RB': 1611, 'B-NP_NP_ADJP_ADVP_ADVP_CC': 1612, 'B-ADVP_RP': 1613, 'B-NP_ADVP_JJR': 1614, 'B-NP_PP_NP_QP_RB': 1615, 'B-PP_PP_RB': 1616, 'B-NP_QP_RRB': 1617, 'B-NP_NP_NP_SBAR_PRN_LRB': 1618, 'B-SBAR_WHNP_S_NP_NP_CC': 1619, 'B-PRT_NP_PP_O-VP': 1620, 'B-ADJP_TO': 1621, 'B-NP_NP_ADJP_O-ADJP': 1622, 'B-SINV_ADVP_RB': 1623, 'B-NP_SBAR_WHNP_IN': 1624, 'B-PP_PP_PP_O-S': 1625, 'B-SBAR_NP_VP_VBG': 1626, 'B-S_S_S_NP_NP_CD': 1627, 'B-COMP_NP_NP_NN': 1628, 'B-ADJP_QP_RB': 1629, 'B-NP_PP_O-UCP': 1630, 'B-FRAG_NP_O-SBAR': 1631, 'B-PP_NP_QP_NN': 1632, 'B-NP_SBAR_WHADVP_S_O-VP': 1633, 'B-NP_ADJP_O-ADJP': 1634, 'B-S_NP_NP_NP_NP_PP_PP_IN': 1635, 'B-FRAG_NP_PRPP': 1636, 'B-S_ADVP_ADVP_RB': 1637, 'B-S_NP_ADJP_JJS': 1638, 'B-ADJP_ADJP_JJS': 1639, 'B-ADVP_ADVP_NP_PP_IN': 1640, 'B-S_NP_NP_NP_NNP': 1641, 'B-NP_ADJP_RBS': 1642, 'B-S_NP_NP_ADJP_ADJP_NP_NN': 1643, 'B-NP_NP_NP_PP_PP_CC': 1644, 'B-SBAR_O-ADVP': 1645, 'B-FRAG_NP_NN': 1646, 'B-NP_NP_QP_QP_CD': 1647, 'B-NP_SBAR_WHADVP_S_O-NP': 1648, 'B-COMP_NP_PRP': 1649, 'B-S_NP_NP_NP_NP_PRN_COLON': 1650, 'B-S_S_IN': 1651, 'B-S_NP_VP_COMMA': 1652, 'B-S_S_NP_ADJP_CD': 1653, 'B-S_S_NP_NP_QP_JJR': 1654, 'B-S_COMP_S_O-VP': 1655, 'B-ADVP_NP_RB': 1656, 'B-NP_NP_NP_NP_NP_PRN_NP_O-NP': 1657, 'B-NP_ADJP_NP_O-PP': 1658, 'B-NP_NP_PRN_PP_IN': 1659, 'B-PP_PP_PRN_O-VP': 1660, 'B-SBAR_WHPP_PP_WHNP_WDT': 1661, 'B-NP_NP_PP_ADJP_O-VP': 1662, 'B-NP_NP_VP_CC': 1663, 'B-NP_NP_NP_NP_VP_VBN': 1664, 'B-S_S_NP_NP_QP_IN': 1665, 'B-ADJP_ADJP_ADJP_NP_NN': 1666, 'B-S_NP_NP_NP_NP_VBN': 1667, 'B-NP_NP_NP_LST_NN': 1668, 'B-NP_LST_JJ': 1669, 'B-NP_NP_ADJP_NP_CC': 1670, 'B-ADJP_ADJP_ADJP_RBS': 1671, 'B-NP_ADJP_ADJP_COLON': 1672, 'B-NP_NP_NP_PRN_SBAR_WHPP_S_O-NP': 1673, 'B-NP_NP_NP_PP_VP_SBAR_WHNP_S_O-NP': 1674, 'B-NP_NP_NP_NP_PRN_CD': 1675, 'B-S_SINV_CONJP_RB': 1676, 'B-ADVP_SBAR_IN': 1677, 'B-ADVP_VP_VBG': 1678, 'B-NP_NP_NP_PP_IN': 1679, 'B-NP_NP_ADVP_FW': 1680, 'B-S_ADVP_NP_NP_NP_DT': 1681, 'B-ADJP_ADJP_NP_QP_RB': 1682, 'B-S_NP_NP_QP_CD': 1683, 'B-NP_ADJP_RRB': 1684, 'B-ADVP_NP_CC': 1685, 'B-S_S_NP_NP_NP_ADJP_ADJP_JJ': 1686, 'B-ADJP_NP_NP_PP_O-VP': 1687, 'B-NP_ADVP_NP_O-VP': 1688, 'B-NP_CONJP_NP_CONJP_NP_O-VP': 1689, 'B-S_PRN_LRB': 1690, 'B-NP_NP_ADVP_IN': 1691, 'B-NP_S_VP_VBN': 1692, 'B-NP_NP_PRN_O-SINV': 1693, 'B-NP_NP_ADJP_NP_NP_O-NP': 1694, 'B-S_NP_NP_ADVP_O-VP': 1695, 'B-UCP_NP_LST_LS': 1696, 'B-SBAR_LST_LS': 1697, 'B-ADJP_PP_PP_O-S': 1698, 'B-SBAR_WHNP_NP_WPP': 1699, 'B-S_NP_NP_PP_COMMA': 1700, 'B-NP_ADVP_JJS': 1701, 'B-ADJP_ADJP_ADJP_NP_JJ': 1702, 'B-WHADVP_O-NP': 1703, 'B-NP_NP_NP_IN': 1704, 'B-NP_NP_SBAR_WHNP_WDT': 1705, 'B-S_NP_ADVP_VP_O-VP': 1706, 'B-NP_NP_PP_VP_O-VP': 1707, 'B-NP_ADJP_NP_VBN': 1708, 'B-VP_VP_VP_JJ': 1709, 'B-VP_ADVP_O-S': 1710, 'B-NP_NP_NP_PRN_O-NP': 1711, 'B-NP_NP_VP_VBN': 1712, 'B-ADVP_ADVP_NP_RB': 1713, 'B-ADVP_NP_PP_O-ADVP': 1714, 'B-NP_PP_PP_RB': 1715, 'B-PP_S_ADVP_RB': 1716, 'B-NP_PP_CONJP_PP_CONJP_PP_O-VP': 1717, 'B-QP_JJ': 1718, 'B-NP_NP_ADVP_VP_O-PP': 1719, 'B-NP_NP_NP_PRN_SBAR_WHNP_S_O-NP': 1720, 'B-ADJP_O-SBAR': 1721, 'B-NP_NP_NP_VP_PP_VBN': 1722, 'B-COMP_S_NP_PRP': 1723, 'B-SBAR_S_IN': 1724, 'B-S_SBAR_NP_VP_VBN': 1725, 'B-NP_NP_NP_NP_PRN_CC': 1726, 'B-ADJP_PP_PP_O-VP': 1727, 'B-SBAR_NP_VP_TO': 1728, 'B-S_S_NP_NP_NP_NNS': 1729, 'B-S_NP_ADJP_ADJP_VBG': 1730, 'B-SINV_VP_VBD': 1731, 'B-NP_NP_VP_PP_IN': 1732, 'B-S_NP_NP_NP_VBG': 1733, 'B-S_NP_NP_ADJP_NP_NN': 1734, 'B-NP_ADVP_PP_O-S': 1735, 'B-NP_NP_NP_NP_NP_VBN': 1736, 'B-NP_QP_RBR': 1737, 'B-VP_ADJP_ADJP_NN': 1738, 'B-VP_NP_RB': 1739, 'B-S_NP_VP_CONJP_RB': 1740, 'B-ADJP_ADJP_NP_JJ': 1741, 'B-S_S_NP_NP_PP_PP_COMMA': 1742, 'B-FRAG_PP_O-PP': 1743, 'B-NP_NP_WDT': 1744, 'B-UCP_UCP_PP_CC': 1745, 'B-ADVP_NP_DT': 1746, 'B-NP_NP_PP_PP_ADJP_O-VP': 1747, 'B-S_NP_VP_VP_VP_TO': 1748, 'B-S_S_S_NP_CC': 1749, 'B-NP_PP_SBAR_O-VP': 1750, 'B-NP_NP_ADJP_ADVP_FW': 1751, 'B-NP_NP_NP_PRN_NP_NP_CC': 1752, 'B-NP_ADJP_LRB': 1753, 'B-NP_NP_NP_NP_ADJP_CD': 1754, 'B-SBARQ_WHADVP_WRB': 1755, 'B-S_UCP_ADVP_RB': 1756, 'B-PP_PP_NP_SBAR_O-NP': 1757, 'B-SBAR_WHNP_NP_WHPP_S_NP_VP_O-SBAR': 1758, 'B-NP_VP_JJ': 1759, 'B-S_NP_VBD': 1760, 'B-NP_NP_CONJP_NP_NN': 1761, 'B-NP_ADVP_NP_RRB': 1762, 'B-NP_S_LST_NN': 1763, 'B-S_LST_NN': 1764, 'B-UCP_UCP_PP_IN': 1765, 'B-VP_ADVP_VP_O-VP': 1766, 'B-SBAR_WHNP_NP_WHPP_COMMA': 1767, 'B-NP_NP_QP_CC': 1768, 'B-NP_QP_O-NP': 1769, 'B-ADJP_PP_JJ': 1770, 'B-NP_NP_NP_PP_ADVP_RB': 1771, 'B-S_NP_NP_NP_PP_ADVP_RB': 1772, 'B-COMP_NP_PRPP': 1773, 'B-S_S_S_S_NP_NP_NN': 1774, 'B-NP_NP_ADJP_RBS': 1775, 'B-NP_NP_VP_VBD': 1776, 'B-NP_NP_ADJP_LQT': 1777, 'B-NP_ADJP_LQT': 1778, 'B-SYM': 1779}\n",
      "label Map NER written at content/data/ner_coNLL_train2_label_map.joblib\n",
      "Created POS label map from train file coNLL_train2.txt\n",
      "{'B-O': 0}\n",
      "label Map POS written at content/data/pos_coNLL_train2_label_map.joblib\n",
      "Max len of sentence:  230\n",
      "Mean len of sentences:  44.89218704192319\n",
      "Median len of sentences:  41.0\n",
      "Making data from file coNLL_testa2.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  176\n",
      "Mean len of sentences:  44.93237639553429\n",
      "Median len of sentences:  42\n",
      "Making data from file coNLL_testb2.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "Processing 15000 rows...\n",
      "Processing 20000 rows...\n",
      "Processing 25000 rows...\n",
      "Processing 30000 rows...\n",
      "Processing 35000 rows...\n",
      "Processing 40000 rows...\n",
      "Processing 45000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 55000 rows...\n",
      "Processing 60000 rows...\n",
      "Processing 65000 rows...\n",
      "Processing 70000 rows...\n",
      "Processing 75000 rows...\n",
      "Processing 80000 rows...\n",
      "Processing 85000 rows...\n",
      "Processing 90000 rows...\n",
      "Processing 95000 rows...\n",
      "Processing 100000 rows...\n",
      "Processing 105000 rows...\n",
      "Processing 110000 rows...\n",
      "Processing 115000 rows...\n",
      "Processing 120000 rows...\n",
      "Processing 125000 rows...\n",
      "Processing 130000 rows...\n",
      "Processing 135000 rows...\n",
      "Processing 140000 rows...\n",
      "Processing 145000 rows...\n",
      "Processing 150000 rows...\n",
      "Processing 155000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  150\n",
      "Mean len of sentences:  44.45361428153351\n",
      "Median len of sentences:  41\n",
      "Making data from file coNLL_testb2_dummy.txt ...\n",
      "Processing 0 rows...\n",
      "Processing 5000 rows...\n",
      "Processing 10000 rows...\n",
      "NER File Written at content/data\n",
      "POS File Written at content/data\n",
      "Max len of sentence:  125\n",
      "Mean len of sentences:  46.00396825396825\n",
      "Median len of sentences:  42.0\n"
     ]
    }
   ],
   "source": [
    "!python ../data_transformations.py \\\n",
    "    --transform_file 'transform_file_conll.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -2 Data Preparation\n",
    "\n",
    "For more details on the data preparation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-data-preparation\">data preparation</a> in documentation.\n",
    "\n",
    "Defining tasks file for training single model for entailment task. The file is already created at ``tasks_file_conll.yml``\n",
    "```\n",
    "conllner:\n",
    "  model_type: BERT\n",
    "  config_name: bert-base-uncased\n",
    "  dropout_prob: 0.2\n",
    "  label_map_or_file: ../../data/ner_coNLL_train_label_map.joblib\n",
    "  metrics:\n",
    "  - seqeval_f1_score\n",
    "  - seqeval_precision\n",
    "  - seqeval_recall\n",
    "  loss_type: NERLoss\n",
    "  task_type: NER\n",
    "  file_names:\n",
    "  - ner_coNLL_train.tsv\n",
    "  - ner_coNLL_testa.tsv\n",
    "  - ner_coNLL_testb.tsv\n",
    "\n",
    "conllpos:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.2\n",
    "    label_map_or_file: ../../data/pos_coNLL_train_label_map.joblib\n",
    "    metrics:\n",
    "    - seqeval_f1_score\n",
    "    - seqeval_precision\n",
    "    - seqeval_recall\n",
    "    loss_type: NERLoss\n",
    "    task_type: NER\n",
    "    file_names:\n",
    "    - pos_coNLL_train.tsv\n",
    "    - pos_coNLL_testa.tsv\n",
    "    - pos_coNLL_testb.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "task object created from task file...\n",
      "Downloading ()solve/main/vocab.txt: 100%|| 232k/232k [00:00<00:00, 488kB/s]\n",
      "Downloading ()okenizer_config.json: 100%|| 28.0/28.0 [00:00<00:00, 4.32kB/s]\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/tokenizer_config.json\n",
      "Downloading ()lve/main/config.json: 100%|| 570/570 [00:00<00:00, 182kB/s]\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "bert model tokenizer loaded for config bert-base-uncased\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_train.tsv\n",
      "Processing Started...\n",
      "Data Size:  41740\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                       | 21/5962 [00:00<00:28, 206.18it/s]\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                       | 27/5962 [00:00<00:22, 267.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  1%|                                       | 56/5962 [00:00<00:20, 289.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                       | 27/5962 [00:00<00:21, 269.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "  1%|                                       | 55/5962 [00:00<00:21, 272.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                       | 21/5962 [00:00<00:28, 205.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  1%|                                       | 86/5962 [00:00<00:20, 288.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                       | 20/5962 [00:00<00:30, 196.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|                                       | 57/5962 [00:00<00:20, 284.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                       | 29/5962 [00:00<00:21, 278.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  1%|                                       | 83/5962 [00:00<00:22, 265.21it/s]\u001b[A\n",
      "\n",
      "  1%|                                       | 64/5962 [00:00<00:28, 209.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 42/5962 [00:00<00:33, 177.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 47/5962 [00:00<00:24, 239.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|                                       | 88/5962 [00:00<00:20, 280.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 60/5962 [00:00<00:19, 295.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|                                      | 115/5962 [00:00<00:25, 232.73it/s]\u001b[A\n",
      "\n",
      "  1%|                                       | 85/5962 [00:00<00:30, 195.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 73/5962 [00:00<00:23, 247.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 61/5962 [00:00<00:39, 149.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|                                      | 117/5962 [00:00<00:24, 237.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 90/5962 [00:00<00:25, 228.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|                                      | 107/5962 [00:00<00:29, 197.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                      | 140/5962 [00:00<00:32, 179.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 77/5962 [00:00<00:42, 137.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|                                      | 137/5962 [00:00<00:32, 179.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "  2%|                                      | 142/5962 [00:00<00:28, 204.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|                                      | 127/5962 [00:00<00:32, 179.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                      | 121/5962 [00:00<00:26, 220.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                      | 161/5962 [00:00<00:33, 171.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                      | 115/5962 [00:00<00:34, 167.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|                                      | 158/5962 [00:00<00:35, 165.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "  3%|                                      | 165/5962 [00:00<00:27, 210.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|                                      | 151/5962 [00:00<00:29, 195.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                      | 147/5962 [00:00<00:25, 229.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 180/5962 [00:00<00:34, 167.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|                                     | 189/5962 [00:00<00:26, 215.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 172/5962 [00:00<00:24, 235.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|                                     | 177/5962 [00:00<00:34, 165.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                      | 135/5962 [00:00<00:36, 158.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|                                      | 171/5962 [00:00<00:31, 182.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 198/5962 [00:01<00:35, 161.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|                                     | 200/5962 [00:01<00:31, 181.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "  4%|                                     | 216/5962 [00:00<00:25, 227.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 196/5962 [00:00<00:26, 217.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                      | 153/5962 [00:00<00:36, 157.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|                                     | 190/5962 [00:01<00:33, 174.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 218/5962 [00:01<00:33, 170.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|                                     | 231/5962 [00:01<00:26, 213.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "  4%|                                     | 242/5962 [00:01<00:24, 236.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|                                     | 208/5962 [00:01<00:35, 163.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                      | 162/5962 [00:01<00:40, 144.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 236/5962 [00:01<00:33, 170.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|                                     | 254/5962 [00:01<00:26, 213.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "  4%|                                     | 267/5962 [00:01<00:24, 228.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 219/5962 [00:01<00:33, 170.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|                                     | 225/5962 [00:01<00:34, 163.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 254/5962 [00:01<00:33, 170.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  5%|                                     | 280/5962 [00:01<00:25, 224.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 185/5962 [00:01<00:43, 131.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|                                     | 291/5962 [00:01<00:25, 218.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|                                     | 242/5962 [00:01<00:34, 164.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 238/5962 [00:01<00:39, 143.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  5%|                                     | 304/5962 [00:01<00:26, 216.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 199/5962 [00:01<00:46, 125.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 272/5962 [00:01<00:37, 150.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|                                     | 259/5962 [00:01<00:39, 146.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|                                     | 314/5962 [00:01<00:31, 178.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 212/5962 [00:01<00:49, 115.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  5%|                                    | 327/5962 [00:01<00:29, 188.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                     | 205/5962 [00:01<00:50, 113.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 288/5962 [00:01<00:43, 131.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|                                     | 278/5962 [00:01<00:36, 157.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|                                    | 335/5962 [00:01<00:30, 185.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 229/5962 [00:01<00:44, 128.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  6%|                                    | 353/5962 [00:01<00:27, 206.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 220/5962 [00:01<00:47, 121.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 309/5962 [00:01<00:37, 149.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|                                     | 303/5962 [00:01<00:31, 180.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|                                    | 358/5962 [00:01<00:28, 194.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 245/5962 [00:01<00:42, 135.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  6%|                                    | 378/5962 [00:01<00:25, 215.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 237/5962 [00:01<00:43, 132.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 335/5962 [00:01<00:31, 177.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|                                    | 333/5962 [00:01<00:26, 210.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|                                    | 382/5962 [00:01<00:27, 204.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 259/5962 [00:01<00:42, 135.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 365/5962 [00:01<00:26, 207.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|                                    | 362/5962 [00:01<00:24, 232.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 331/5962 [00:01<00:31, 180.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|                                    | 405/5962 [00:01<00:26, 209.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "  7%|                                    | 401/5962 [00:01<00:29, 186.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 276/5962 [00:01<00:39, 144.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 394/5962 [00:02<00:24, 229.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|                                    | 397/5962 [00:02<00:21, 264.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 355/5962 [00:01<00:28, 194.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|                                    | 441/5962 [00:01<00:22, 249.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "  7%|                                    | 433/5962 [00:02<00:25, 219.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 314/5962 [00:01<00:26, 209.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 312/5962 [00:02<00:28, 200.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 433/5962 [00:02<00:20, 267.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|                                    | 431/5962 [00:02<00:19, 279.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|                                   | 481/5962 [00:02<00:18, 291.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "  8%|                                    | 462/5962 [00:02<00:23, 237.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 348/5962 [00:02<00:22, 245.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 333/5962 [00:02<00:28, 198.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                    | 461/5962 [00:02<00:20, 266.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|                                   | 520/5962 [00:02<00:17, 317.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "  8%|                                   | 492/5962 [00:02<00:21, 252.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 357/5962 [00:02<00:26, 209.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 374/5962 [00:02<00:24, 229.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|                                    | 460/5962 [00:02<00:24, 224.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                    | 451/5962 [00:02<00:20, 269.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|                                   | 489/5962 [00:02<00:21, 255.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "  9%|                                   | 519/5962 [00:02<00:23, 232.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 518/5962 [00:02<00:20, 264.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                   | 480/5962 [00:02<00:21, 256.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|                                   | 585/5962 [00:02<00:19, 281.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 398/5962 [00:02<00:28, 196.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|                                   | 485/5962 [00:02<00:28, 192.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 550/5962 [00:02<00:19, 279.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 507/5962 [00:02<00:21, 249.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|                                   | 615/5962 [00:02<00:18, 283.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 419/5962 [00:02<00:28, 195.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|                                   | 544/5962 [00:02<00:29, 184.60it/s]\u001b[A\n",
      "\n",
      " 10%|                                   | 579/5962 [00:02<00:19, 269.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 420/5962 [00:02<00:30, 181.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 537/5962 [00:02<00:24, 223.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 440/5962 [00:02<00:31, 176.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|                                  | 644/5962 [00:02<00:21, 244.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  9%|                                   | 527/5962 [00:02<00:33, 161.66it/s]\u001b[A\u001b[A\n",
      "  9%|                                   | 565/5962 [00:02<00:37, 144.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                    | 467/5962 [00:02<00:27, 196.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 561/5962 [00:02<00:24, 216.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|                                   | 607/5962 [00:02<00:25, 213.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 439/5962 [00:02<00:41, 134.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  9%|                                   | 551/5962 [00:02<00:30, 177.43it/s]\u001b[A\u001b[A\n",
      " 10%|                                   | 583/5962 [00:02<00:35, 150.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 584/5962 [00:02<00:24, 217.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|                                  | 698/5962 [00:02<00:21, 247.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 631/5962 [00:03<00:25, 209.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|                                   | 582/5962 [00:03<00:25, 208.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                    | 462/5962 [00:02<00:35, 154.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|                                   | 620/5962 [00:03<00:27, 197.26it/s]\u001b[A\n",
      "\n",
      "\n",
      " 12%|                                  | 735/5962 [00:03<00:18, 279.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 659/5962 [00:03<00:23, 226.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 607/5962 [00:02<00:26, 203.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                   | 492/5962 [00:03<00:28, 188.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|                                   | 618/5962 [00:03<00:21, 244.88it/s]\u001b[A\u001b[A\n",
      " 11%|                                  | 659/5962 [00:03<00:21, 242.82it/s]\u001b[A\n",
      "\n",
      "\n",
      " 13%|                                  | 777/5962 [00:03<00:16, 317.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 542/5962 [00:03<00:23, 228.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 698/5962 [00:03<00:20, 262.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 517/5962 [00:03<00:26, 202.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|                                  | 647/5962 [00:03<00:20, 255.31it/s]\u001b[A\u001b[A\n",
      " 12%|                                  | 694/5962 [00:03<00:19, 269.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 570/5962 [00:03<00:22, 237.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|                                 | 810/5962 [00:03<00:17, 302.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 733/5962 [00:03<00:18, 285.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 545/5962 [00:03<00:24, 222.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|                                  | 678/5962 [00:03<00:19, 269.83it/s]\u001b[A\u001b[A\n",
      " 12%|                                  | 730/5962 [00:03<00:17, 292.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 600/5962 [00:03<00:21, 254.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|                                 | 841/5962 [00:03<00:16, 304.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                  | 767/5962 [00:03<00:17, 294.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 581/5962 [00:03<00:20, 257.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|                                  | 707/5962 [00:03<00:20, 261.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 626/5962 [00:03<00:21, 245.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|                                  | 762/5962 [00:03<00:20, 259.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 716/5962 [00:03<00:21, 241.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|                                 | 798/5962 [00:03<00:17, 287.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 609/5962 [00:03<00:20, 257.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|                                  | 734/5962 [00:03<00:22, 230.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 741/5962 [00:03<00:25, 201.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|                                 | 902/5962 [00:03<00:22, 227.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 828/5962 [00:03<00:23, 221.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|                                 | 791/5962 [00:03<00:28, 178.82it/s]\u001b[A\n",
      "\n",
      " 13%|                                  | 759/5962 [00:03<00:32, 162.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                  | 763/5962 [00:03<00:31, 166.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 853/5962 [00:04<00:27, 182.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 672/5962 [00:03<00:37, 139.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|                                 | 927/5962 [00:03<00:31, 157.91it/s]\u001b[A\u001b[A\u001b[A\n",
      " 14%|                                 | 814/5962 [00:04<00:35, 144.82it/s]\u001b[A\n",
      "\n",
      " 13%|                                  | 779/5962 [00:04<00:36, 141.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 874/5962 [00:04<00:32, 157.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 690/5962 [00:03<00:41, 127.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 657/5962 [00:04<00:46, 115.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|                                | 947/5962 [00:04<00:37, 135.16it/s]\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 892/5962 [00:04<00:34, 148.05it/s]\u001b[A\n",
      "\n",
      " 13%|                                 | 796/5962 [00:04<00:43, 118.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 705/5962 [00:04<00:45, 116.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                 | 798/5962 [00:04<00:48, 106.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 674/5962 [00:04<00:46, 113.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 914/5962 [00:04<00:31, 160.21it/s]\u001b[A\n",
      "\n",
      "\n",
      " 16%|                                | 964/5962 [00:04<00:39, 128.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|                                 | 810/5962 [00:04<00:44, 115.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 723/5962 [00:04<00:40, 128.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 811/5962 [00:04<00:46, 109.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 693/5962 [00:04<00:42, 124.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 867/5962 [00:04<00:36, 137.79it/s]\u001b[A\n",
      "\n",
      "\n",
      " 16%|                                 | 934/5962 [00:04<00:30, 165.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 740/5962 [00:04<00:38, 135.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|                                 | 823/5962 [00:04<00:46, 111.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 829/5962 [00:04<00:41, 123.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 718/5962 [00:04<00:35, 148.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 887/5962 [00:04<00:33, 150.39it/s]\u001b[A\n",
      "\n",
      "\n",
      " 16%|                                | 955/5962 [00:04<00:28, 175.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                  | 762/5962 [00:04<00:33, 154.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 846/5962 [00:04<00:39, 130.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 912/5962 [00:04<00:29, 173.58it/s]\u001b[A\n",
      "\n",
      " 14%|                                 | 836/5962 [00:04<00:48, 106.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 975/5962 [00:04<00:27, 181.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|                               | 1023/5962 [00:04<00:31, 158.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                 | 786/5962 [00:04<00:29, 174.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 863/5962 [00:04<00:36, 138.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|                                | 939/5962 [00:04<00:25, 197.23it/s]\u001b[A\n",
      "\n",
      " 14%|                                 | 848/5962 [00:04<00:47, 107.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                               | 1045/5962 [00:04<00:28, 172.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 995/5962 [00:04<00:27, 180.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 814/5962 [00:04<00:25, 200.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 879/5962 [00:04<00:35, 142.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                               | 1066/5962 [00:04<00:26, 181.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|                               | 1017/5962 [00:04<00:26, 189.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                  | 773/5962 [00:04<00:33, 153.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|                                | 961/5962 [00:04<00:27, 179.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 895/5962 [00:04<00:35, 143.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|                               | 1040/5962 [00:05<00:24, 197.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                               | 1086/5962 [00:04<00:28, 170.98it/s]\u001b[A\u001b[A\u001b[A\n",
      " 16%|                                | 981/5962 [00:05<00:27, 179.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                 | 790/5962 [00:04<00:35, 144.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 861/5962 [00:04<00:24, 204.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 912/5962 [00:04<00:34, 148.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|                               | 1062/5962 [00:05<00:24, 201.73it/s]\u001b[A\u001b[A\n",
      " 17%|                               | 1000/5962 [00:05<00:27, 180.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 808/5962 [00:05<00:33, 153.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|                               | 1104/5962 [00:05<00:30, 160.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 884/5962 [00:05<00:24, 206.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|                                 | 908/5962 [00:05<00:37, 133.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1083/5962 [00:05<00:24, 195.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|                               | 1019/5962 [00:05<00:27, 177.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 829/5962 [00:05<00:30, 166.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|                              | 1122/5962 [00:05<00:30, 158.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 906/5962 [00:05<00:25, 197.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|                               | 1110/5962 [00:05<00:22, 215.92it/s]\u001b[A\u001b[A\n",
      " 17%|                               | 1038/5962 [00:05<00:27, 179.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 943/5962 [00:05<00:38, 129.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|                              | 1145/5962 [00:05<00:27, 176.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|                                | 943/5962 [00:05<00:33, 150.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                              | 1146/5962 [00:05<00:19, 253.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                 | 927/5962 [00:05<00:28, 177.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|                              | 1166/5962 [00:05<00:25, 184.48it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|                               | 1057/5962 [00:05<00:29, 167.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 957/5962 [00:05<00:42, 118.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 863/5962 [00:05<00:35, 142.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|                                | 959/5962 [00:05<00:34, 146.24it/s]\u001b[A\u001b[A\n",
      " 20%|                              | 1172/5962 [00:05<00:23, 207.13it/s]\u001b[A\n",
      "\n",
      "\n",
      " 20%|                              | 1185/5962 [00:05<00:28, 165.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 970/5962 [00:05<00:46, 107.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 946/5962 [00:05<00:38, 130.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 878/5962 [00:05<00:41, 122.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 982/5962 [00:05<00:45, 109.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|                                | 974/5962 [00:05<00:45, 110.69it/s]\u001b[A\u001b[A\n",
      " 18%|                               | 1092/5962 [00:05<00:35, 138.59it/s]\u001b[A\n",
      "\n",
      "\n",
      " 20%|                              | 1195/5962 [00:05<00:26, 181.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 894/5962 [00:05<00:38, 130.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 962/5962 [00:05<00:38, 131.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 996/5962 [00:05<00:42, 116.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|                                | 989/5962 [00:05<00:41, 119.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|                              | 1219/5962 [00:05<00:32, 147.92it/s]\u001b[A\u001b[A\u001b[A\n",
      " 20%|                              | 1215/5962 [00:06<00:27, 175.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 908/5962 [00:05<00:40, 126.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 977/5962 [00:05<00:39, 126.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1014/5962 [00:05<00:37, 132.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|                               | 1008/5962 [00:05<00:36, 135.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|                              | 1240/5962 [00:05<00:29, 162.77it/s]\u001b[A\u001b[A\u001b[A\n",
      " 21%|                              | 1240/5962 [00:06<00:24, 192.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                 | 927/5962 [00:05<00:35, 141.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 995/5962 [00:05<00:36, 136.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|                               | 1027/5962 [00:06<00:33, 149.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1041/5962 [00:05<00:29, 166.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|                              | 1257/5962 [00:06<00:29, 159.66it/s]\u001b[A\u001b[A\u001b[A\n",
      " 21%|                              | 1263/5962 [00:06<00:24, 195.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1012/5962 [00:06<00:35, 140.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1059/5962 [00:06<00:29, 166.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|                               | 1048/5962 [00:06<00:30, 161.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 942/5962 [00:06<00:40, 125.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|                              | 1160/5962 [00:06<00:31, 154.13it/s]\u001b[A\n",
      "\n",
      "\n",
      " 22%|                             | 1288/5962 [00:06<00:22, 208.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1083/5962 [00:06<00:26, 186.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1027/5962 [00:06<00:36, 135.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 959/5962 [00:06<00:36, 135.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|                             | 1303/5962 [00:06<00:24, 191.29it/s]\u001b[A\u001b[A\u001b[A\n",
      " 22%|                             | 1315/5962 [00:06<00:20, 223.15it/s]\u001b[A\n",
      "\n",
      " 18%|                               | 1065/5962 [00:06<00:35, 137.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 1113/5962 [00:06<00:22, 215.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1042/5962 [00:06<00:35, 139.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 979/5962 [00:06<00:32, 151.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|                             | 1324/5962 [00:06<00:23, 196.07it/s]\u001b[A\u001b[A\u001b[A\n",
      " 20%|                              | 1205/5962 [00:06<00:25, 185.54it/s]\u001b[A\n",
      "\n",
      " 18%|                               | 1080/5962 [00:06<00:37, 129.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1338/5962 [00:06<00:23, 194.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1060/5962 [00:06<00:32, 149.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 996/5962 [00:06<00:31, 156.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|                              | 1229/5962 [00:06<00:23, 197.85it/s]\u001b[A\n",
      "\n",
      "\n",
      " 23%|                             | 1344/5962 [00:06<00:27, 167.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1362/5962 [00:06<00:22, 204.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1164/5962 [00:06<00:21, 221.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1013/5962 [00:06<00:31, 158.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|                               | 1094/5962 [00:06<00:39, 122.19it/s]\u001b[A\u001b[A\n",
      " 23%|                             | 1387/5962 [00:06<00:21, 215.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 1105/5962 [00:06<00:26, 184.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|                             | 1362/5962 [00:06<00:28, 163.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                               | 1030/5962 [00:06<00:31, 157.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|                               | 1107/5962 [00:06<00:39, 123.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1187/5962 [00:06<00:22, 214.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|                             | 1279/5962 [00:06<00:21, 221.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 1410/5962 [00:06<00:21, 214.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|                             | 1379/5962 [00:06<00:28, 160.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1048/5962 [00:06<00:30, 160.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1216/5962 [00:06<00:20, 233.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|                              | 1126/5962 [00:06<00:34, 138.77it/s]\u001b[A\u001b[A\n",
      " 22%|                             | 1302/5962 [00:06<00:22, 204.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1166/5962 [00:06<00:19, 242.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1068/5962 [00:06<00:28, 170.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|                            | 1432/5962 [00:07<00:22, 203.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|                             | 1396/5962 [00:06<00:30, 151.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1240/5962 [00:06<00:21, 216.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|                             | 1323/5962 [00:07<00:25, 180.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|                               | 1086/5962 [00:06<00:28, 171.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|                              | 1173/5962 [00:07<00:25, 184.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|                             | 1414/5962 [00:06<00:28, 158.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                            | 1453/5962 [00:07<00:22, 198.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1262/5962 [00:06<00:23, 201.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 1105/5962 [00:07<00:27, 175.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|                              | 1193/5962 [00:07<00:25, 185.07it/s]\u001b[A\u001b[A\n",
      " 23%|                             | 1354/5962 [00:07<00:22, 209.43it/s]\u001b[A\n",
      "\n",
      "\n",
      " 25%|                            | 1474/5962 [00:07<00:22, 197.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1214/5962 [00:07<00:22, 213.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1286/5962 [00:07<00:22, 210.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                              | 1123/5962 [00:07<00:27, 173.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|                             | 1376/5962 [00:07<00:21, 209.05it/s]\u001b[A\n",
      "\n",
      "\n",
      " 24%|                            | 1452/5962 [00:07<00:27, 165.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|                              | 1212/5962 [00:07<00:26, 176.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1236/5962 [00:07<00:23, 202.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1494/5962 [00:07<00:25, 176.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1258/5962 [00:07<00:23, 199.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                              | 1141/5962 [00:07<00:32, 146.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|                             | 1398/5962 [00:07<00:25, 176.75it/s]\u001b[A\n",
      "\n",
      "\n",
      " 25%|                            | 1469/5962 [00:07<00:31, 140.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1513/5962 [00:07<00:27, 162.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|                              | 1230/5962 [00:07<00:33, 142.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1286/5962 [00:07<00:21, 219.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                              | 1157/5962 [00:07<00:33, 145.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|                            | 1484/5962 [00:07<00:31, 142.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|                            | 1530/5962 [00:07<00:28, 154.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1354/5962 [00:07<00:25, 183.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|                             | 1417/5962 [00:07<00:29, 152.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1309/5962 [00:07<00:21, 217.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1177/5962 [00:07<00:30, 159.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|                            | 1504/5962 [00:07<00:28, 157.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|                              | 1274/5962 [00:07<00:26, 175.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1551/5962 [00:07<00:28, 153.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|                            | 1434/5962 [00:07<00:32, 139.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1194/5962 [00:07<00:32, 146.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|                            | 1521/5962 [00:07<00:30, 143.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1331/5962 [00:07<00:25, 181.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|                            | 1574/5962 [00:07<00:25, 169.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1392/5962 [00:07<00:27, 163.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|                            | 1455/5962 [00:07<00:29, 155.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|                              | 1210/5962 [00:07<00:32, 147.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|                            | 1540/5962 [00:07<00:28, 154.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1351/5962 [00:07<00:25, 178.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|                           | 1596/5962 [00:07<00:24, 181.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 1418/5962 [00:07<00:24, 187.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 25%|                            | 1478/5962 [00:07<00:26, 169.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1230/5962 [00:07<00:29, 160.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|                            | 1567/5962 [00:07<00:23, 184.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1370/5962 [00:07<00:25, 179.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|                             | 1337/5962 [00:08<00:25, 183.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                           | 1615/5962 [00:08<00:25, 168.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 25%|                            | 1500/5962 [00:08<00:24, 181.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1248/5962 [00:07<00:28, 164.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|                           | 1593/5962 [00:08<00:21, 204.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 1402/5962 [00:07<00:21, 214.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|                           | 1636/5962 [00:08<00:24, 179.54it/s]\u001b[A\u001b[A\n",
      " 26%|                            | 1528/5962 [00:08<00:21, 206.21it/s]\u001b[A\n",
      "\n",
      "\n",
      " 27%|                           | 1620/5962 [00:08<00:19, 220.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 1269/5962 [00:08<00:27, 173.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1461/5962 [00:08<00:26, 168.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|                             | 1381/5962 [00:08<00:22, 201.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1655/5962 [00:08<00:24, 179.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|                            | 1550/5962 [00:08<00:22, 199.81it/s]\u001b[A\n",
      "\n",
      "\n",
      " 28%|                           | 1643/5962 [00:08<00:19, 222.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1482/5962 [00:08<00:25, 178.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1287/5962 [00:08<00:27, 169.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|                             | 1406/5962 [00:08<00:21, 215.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1680/5962 [00:08<00:21, 196.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|                            | 1578/5962 [00:08<00:19, 219.48it/s]\u001b[A\n",
      "\n",
      "\n",
      " 28%|                           | 1685/5962 [00:08<00:15, 279.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1506/5962 [00:08<00:23, 193.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|                             | 1321/5962 [00:08<00:21, 213.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|                             | 1429/5962 [00:08<00:20, 219.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1707/5962 [00:08<00:19, 217.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|                           | 1721/5962 [00:08<00:14, 302.46it/s]\u001b[A\u001b[A\u001b[A\n",
      " 27%|                           | 1606/5962 [00:08<00:18, 235.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1532/5962 [00:08<00:21, 210.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1343/5962 [00:08<00:21, 213.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|                            | 1455/5962 [00:08<00:19, 227.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1733/5962 [00:08<00:18, 226.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|                           | 1637/5962 [00:08<00:16, 256.43it/s]\u001b[A\n",
      "\n",
      "\n",
      " 30%|                          | 1762/5962 [00:08<00:12, 331.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1554/5962 [00:08<00:20, 212.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1372/5962 [00:08<00:19, 234.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|                            | 1478/5962 [00:08<00:19, 227.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1763/5962 [00:08<00:17, 246.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 28%|                           | 1667/5962 [00:08<00:16, 266.45it/s]\u001b[A\n",
      "\n",
      "\n",
      " 30%|                          | 1796/5962 [00:08<00:12, 322.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1577/5962 [00:08<00:20, 214.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|                             | 1396/5962 [00:08<00:19, 229.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|                          | 1788/5962 [00:08<00:17, 234.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 1581/5962 [00:08<00:16, 270.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 28%|                           | 1695/5962 [00:08<00:16, 262.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 1420/5962 [00:08<00:20, 219.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|                          | 1829/5962 [00:08<00:14, 277.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|                            | 1524/5962 [00:08<00:21, 205.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1814/5962 [00:08<00:17, 239.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                           | 1609/5962 [00:08<00:16, 271.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|                           | 1726/5962 [00:08<00:15, 275.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|                            | 1443/5962 [00:08<00:20, 217.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|                          | 1858/5962 [00:08<00:14, 275.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|                            | 1549/5962 [00:08<00:20, 215.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1842/5962 [00:09<00:16, 251.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                           | 1639/5962 [00:08<00:15, 276.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|                          | 1754/5962 [00:09<00:15, 276.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1465/5962 [00:08<00:21, 205.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1642/5962 [00:08<00:21, 197.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|                          | 1887/5962 [00:09<00:14, 273.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|                          | 1869/5962 [00:09<00:16, 252.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1667/5962 [00:08<00:16, 266.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|                          | 1782/5962 [00:09<00:16, 247.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1664/5962 [00:09<00:21, 203.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 1895/5962 [00:09<00:16, 246.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|                         | 1915/5962 [00:09<00:15, 253.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|                           | 1606/5962 [00:09<00:21, 202.68it/s]\u001b[A\u001b[A\n",
      " 30%|                          | 1808/5962 [00:09<00:17, 238.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1694/5962 [00:09<00:18, 226.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1685/5962 [00:09<00:21, 201.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|                         | 1921/5962 [00:09<00:16, 247.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|                            | 1506/5962 [00:09<00:24, 180.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|                           | 1628/5962 [00:09<00:22, 193.99it/s]\u001b[A\u001b[A\n",
      " 31%|                          | 1833/5962 [00:09<00:17, 234.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1706/5962 [00:09<00:21, 202.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                         | 1953/5962 [00:09<00:14, 268.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1525/5962 [00:09<00:24, 177.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|                         | 1987/5962 [00:09<00:15, 262.23it/s]\u001b[A\u001b[A\u001b[A\n",
      " 31%|                          | 1862/5962 [00:09<00:16, 248.83it/s]\u001b[A\n",
      "\n",
      " 28%|                           | 1650/5962 [00:09<00:21, 199.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1733/5962 [00:09<00:19, 220.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                         | 1980/5962 [00:09<00:15, 251.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1543/5962 [00:09<00:26, 168.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1759/5962 [00:09<00:18, 229.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|                          | 1888/5962 [00:09<00:16, 241.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1765/5962 [00:09<00:18, 224.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|                         | 2006/5962 [00:09<00:15, 249.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 1579/5962 [00:09<00:20, 217.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|                         | 2015/5962 [00:09<00:19, 202.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1789/5962 [00:09<00:16, 246.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1803/5962 [00:09<00:15, 266.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|                           | 1696/5962 [00:09<00:21, 197.93it/s]\u001b[A\u001b[A\n",
      " 34%|                         | 2032/5962 [00:09<00:15, 246.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|                           | 1604/5962 [00:09<00:19, 222.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1814/5962 [00:09<00:17, 236.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1832/5962 [00:09<00:15, 270.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|                           | 1727/5962 [00:09<00:18, 226.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|                        | 2060/5962 [00:09<00:15, 255.84it/s]\u001b[A\u001b[A\u001b[A\n",
      " 32%|                         | 1936/5962 [00:09<00:19, 208.29it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1839/5962 [00:09<00:17, 240.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1860/5962 [00:09<00:15, 268.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|                          | 1755/5962 [00:09<00:17, 239.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2086/5962 [00:10<00:15, 255.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|                         | 1967/5962 [00:09<00:17, 232.69it/s]\u001b[A\n",
      "\n",
      "\n",
      " 35%|                         | 2059/5962 [00:09<00:21, 179.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1864/5962 [00:09<00:17, 236.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|                          | 1783/5962 [00:10<00:16, 249.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2116/5962 [00:10<00:14, 266.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|                         | 2002/5962 [00:10<00:15, 261.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1648/5962 [00:09<00:24, 179.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|                        | 2082/5962 [00:10<00:20, 188.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|                          | 1809/5962 [00:10<00:16, 248.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2143/5962 [00:10<00:14, 263.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|                         | 2033/5962 [00:10<00:14, 271.51it/s]\u001b[A\n",
      "\n",
      "\n",
      " 35%|                        | 2103/5962 [00:10<00:19, 193.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1667/5962 [00:10<00:24, 172.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 1888/5962 [00:10<00:21, 191.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|                          | 1835/5962 [00:10<00:16, 249.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2171/5962 [00:10<00:14, 265.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|                        | 2136/5962 [00:10<00:17, 222.44it/s]\u001b[A\u001b[A\u001b[A\n",
      " 35%|                        | 2065/5962 [00:10<00:15, 252.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|                           | 1685/5962 [00:10<00:29, 146.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                         | 1909/5962 [00:10<00:23, 169.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|                          | 1861/5962 [00:10<00:19, 210.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|                        | 2160/5962 [00:10<00:17, 214.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                         | 1976/5962 [00:10<00:17, 227.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|                        | 2198/5962 [00:10<00:18, 208.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                         | 1932/5962 [00:10<00:22, 183.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1701/5962 [00:10<00:30, 140.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|                        | 2183/5962 [00:10<00:17, 213.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 2004/5962 [00:10<00:16, 240.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|                          | 1884/5962 [00:10<00:20, 197.11it/s]\u001b[A\u001b[A\n",
      " 37%|                       | 2221/5962 [00:10<00:17, 209.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                         | 1956/5962 [00:10<00:20, 196.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1716/5962 [00:10<00:31, 136.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|                        | 2210/5962 [00:10<00:16, 227.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 2034/5962 [00:10<00:15, 251.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|                         | 1912/5962 [00:10<00:18, 216.19it/s]\u001b[A\u001b[A\n",
      " 36%|                        | 2146/5962 [00:10<00:15, 252.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2244/5962 [00:10<00:17, 210.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|                       | 2241/5962 [00:10<00:14, 249.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 1731/5962 [00:10<00:32, 131.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|                         | 1936/5962 [00:10<00:18, 220.35it/s]\u001b[A\u001b[A\n",
      " 36%|                        | 2172/5962 [00:10<00:15, 249.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2268/5962 [00:10<00:16, 218.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2061/5962 [00:10<00:17, 217.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                          | 1750/5962 [00:10<00:29, 145.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|                       | 2291/5962 [00:10<00:16, 217.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|                         | 1959/5962 [00:10<00:20, 199.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 2041/5962 [00:10<00:16, 231.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|                        | 2198/5962 [00:10<00:16, 223.23it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1772/5962 [00:10<00:25, 163.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2085/5962 [00:10<00:19, 195.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|                       | 2319/5962 [00:11<00:15, 233.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|                         | 1980/5962 [00:10<00:19, 201.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2082/5962 [00:10<00:14, 276.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|                       | 2221/5962 [00:11<00:17, 214.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1792/5962 [00:10<00:24, 171.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2106/5962 [00:10<00:20, 183.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|                         | 2006/5962 [00:11<00:18, 215.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|                       | 2343/5962 [00:11<00:17, 206.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2111/5962 [00:10<00:14, 257.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|                       | 2243/5962 [00:11<00:18, 203.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|                          | 1812/5962 [00:11<00:23, 178.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2127/5962 [00:11<00:20, 189.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|                         | 2038/5962 [00:11<00:16, 242.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|                       | 2341/5962 [00:11<00:16, 222.97it/s]\u001b[A\u001b[A\u001b[A\n",
      " 38%|                       | 2270/5962 [00:11<00:16, 218.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1836/5962 [00:11<00:21, 191.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                       | 2365/5962 [00:11<00:18, 195.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2154/5962 [00:11<00:18, 209.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|                        | 2069/5962 [00:11<00:15, 258.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|                       | 2369/5962 [00:11<00:15, 236.32it/s]\u001b[A\u001b[A\u001b[A\n",
      " 39%|                       | 2298/5962 [00:11<00:15, 234.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2165/5962 [00:11<00:15, 251.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 1856/5962 [00:11<00:22, 184.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 2386/5962 [00:11<00:19, 185.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|                        | 2096/5962 [00:11<00:15, 254.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|                      | 2393/5962 [00:11<00:15, 223.70it/s]\u001b[A\u001b[A\u001b[A\n",
      " 39%|                       | 2322/5962 [00:11<00:15, 231.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                        | 2197/5962 [00:11<00:14, 265.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 1883/5962 [00:11<00:19, 206.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                        | 2208/5962 [00:11<00:16, 233.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|                        | 2122/5962 [00:11<00:15, 244.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|                      | 2406/5962 [00:11<00:21, 165.07it/s]\u001b[A\u001b[A\u001b[A\n",
      " 40%|                       | 2355/5962 [00:11<00:14, 257.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                       | 2224/5962 [00:11<00:14, 266.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|                         | 1908/5962 [00:11<00:18, 216.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                       | 2234/5962 [00:11<00:15, 234.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|                        | 2147/5962 [00:11<00:19, 198.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|                      | 2424/5962 [00:11<00:25, 138.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|                         | 1930/5962 [00:11<00:21, 190.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2251/5962 [00:11<00:16, 222.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|                      | 2382/5962 [00:11<00:18, 189.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2258/5962 [00:11<00:20, 184.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|                        | 2169/5962 [00:11<00:21, 176.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|                         | 1950/5962 [00:11<00:25, 157.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|                      | 2404/5962 [00:11<00:19, 185.13it/s]\u001b[A\n",
      "\n",
      "\n",
      " 41%|                      | 2439/5962 [00:12<00:29, 118.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2279/5962 [00:11<00:21, 173.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2275/5962 [00:11<00:21, 174.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|                        | 2188/5962 [00:12<00:22, 166.10it/s]\u001b[A\u001b[A\n",
      " 41%|                      | 2425/5962 [00:12<00:19, 183.42it/s]\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 2476/5962 [00:11<00:23, 146.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2452/5962 [00:12<00:31, 110.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 2298/5962 [00:11<00:23, 156.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|                        | 2206/5962 [00:12<00:23, 161.25it/s]\u001b[A\u001b[A\n",
      " 41%|                      | 2452/5962 [00:12<00:17, 203.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2464/5962 [00:12<00:31, 111.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2295/5962 [00:12<00:26, 136.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 2492/5962 [00:12<00:26, 130.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|                       | 2229/5962 [00:12<00:21, 175.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 2315/5962 [00:12<00:24, 146.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 2005/5962 [00:12<00:24, 162.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|                      | 2484/5962 [00:12<00:26, 129.45it/s]\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 2506/5962 [00:12<00:27, 127.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|                       | 2248/5962 [00:12<00:20, 179.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 2337/5962 [00:12<00:22, 162.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 2028/5962 [00:12<00:21, 179.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 2510/5962 [00:12<00:21, 160.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|                      | 2495/5962 [00:12<00:19, 182.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                       | 2358/5962 [00:12<00:20, 173.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 2520/5962 [00:12<00:28, 120.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                     | 2532/5962 [00:12<00:19, 174.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 2047/5962 [00:12<00:22, 170.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|                      | 2514/5962 [00:12<00:18, 183.58it/s]\u001b[A\n",
      "\n",
      " 38%|                       | 2267/5962 [00:12<00:24, 153.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 2377/5962 [00:12<00:20, 171.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                     | 2536/5962 [00:12<00:26, 127.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2553/5962 [00:12<00:18, 182.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2065/5962 [00:12<00:23, 168.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|                     | 2535/5962 [00:12<00:18, 187.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2573/5962 [00:12<00:18, 183.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                     | 2552/5962 [00:12<00:29, 117.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|                       | 2284/5962 [00:12<00:30, 118.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                       | 2364/5962 [00:12<00:28, 126.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2083/5962 [00:12<00:25, 150.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|                     | 2596/5962 [00:12<00:17, 196.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 2414/5962 [00:12<00:22, 160.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                     | 2578/5962 [00:12<00:22, 149.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|                       | 2306/5962 [00:12<00:26, 139.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 2387/5962 [00:12<00:24, 147.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                        | 2113/5962 [00:12<00:20, 183.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|                     | 2624/5962 [00:12<00:15, 218.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2446/5962 [00:12<00:17, 200.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|                     | 2603/5962 [00:12<00:19, 173.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|                       | 2328/5962 [00:12<00:23, 154.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2137/5962 [00:12<00:19, 197.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 2404/5962 [00:12<00:23, 149.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|                     | 2652/5962 [00:13<00:14, 233.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2474/5962 [00:12<00:15, 219.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|                       | 2360/5962 [00:13<00:18, 192.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|                     | 2622/5962 [00:13<00:20, 164.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 2168/5962 [00:12<00:16, 224.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2420/5962 [00:12<00:23, 149.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|                     | 2677/5962 [00:13<00:13, 237.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 2509/5962 [00:12<00:13, 254.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|                      | 2389/5962 [00:13<00:16, 217.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|                        | 2194/5962 [00:13<00:16, 233.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2439/5962 [00:13<00:22, 158.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|                     | 2640/5962 [00:13<00:20, 159.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                    | 2703/5962 [00:13<00:13, 242.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|                     | 2661/5962 [00:13<00:15, 214.47it/s]\u001b[A\n",
      "\n",
      " 40%|                      | 2413/5962 [00:13<00:17, 207.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2456/5962 [00:13<00:22, 157.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|                       | 2218/5962 [00:13<00:16, 224.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|                     | 2658/5962 [00:13<00:20, 163.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2728/5962 [00:13<00:13, 237.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|                     | 2684/5962 [00:13<00:16, 203.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2473/5962 [00:13<00:21, 158.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2241/5962 [00:13<00:16, 222.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|                     | 2680/5962 [00:13<00:18, 177.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|                    | 2754/5962 [00:13<00:13, 240.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2595/5962 [00:13<00:13, 244.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|                    | 2705/5962 [00:13<00:16, 195.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 2492/5962 [00:13<00:20, 166.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 2782/5962 [00:13<00:12, 251.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|                    | 2699/5962 [00:13<00:19, 167.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|                      | 2458/5962 [00:13<00:18, 189.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2621/5962 [00:13<00:14, 231.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 2517/5962 [00:13<00:18, 190.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 2286/5962 [00:13<00:18, 200.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|                      | 2483/5962 [00:13<00:17, 203.35it/s]\u001b[A\u001b[A\n",
      " 46%|                    | 2725/5962 [00:13<00:19, 161.91it/s]\u001b[A\n",
      "\n",
      "\n",
      " 47%|                    | 2808/5962 [00:13<00:14, 223.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2537/5962 [00:13<00:17, 190.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2645/5962 [00:13<00:15, 216.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 2307/5962 [00:13<00:18, 197.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|                      | 2505/5962 [00:13<00:17, 196.92it/s]\u001b[A\u001b[A\n",
      " 46%|                    | 2743/5962 [00:13<00:21, 150.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 2832/5962 [00:13<00:15, 207.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|                    | 2734/5962 [00:13<00:22, 141.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 2668/5962 [00:13<00:15, 209.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 2327/5962 [00:13<00:18, 195.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|                     | 2531/5962 [00:13<00:16, 211.39it/s]\u001b[A\u001b[A\n",
      " 46%|                    | 2760/5962 [00:13<00:21, 152.23it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2588/5962 [00:13<00:15, 223.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|                    | 2755/5962 [00:13<00:20, 157.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 2854/5962 [00:14<00:15, 197.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 2350/5962 [00:13<00:17, 204.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|                     | 2562/5962 [00:13<00:14, 236.41it/s]\u001b[A\u001b[A\n",
      " 47%|                    | 2783/5962 [00:14<00:18, 169.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2620/5962 [00:13<00:13, 247.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|                   | 2880/5962 [00:14<00:14, 213.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2723/5962 [00:13<00:14, 221.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 2387/5962 [00:13<00:14, 250.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|                     | 2589/5962 [00:14<00:13, 244.63it/s]\u001b[A\u001b[A\n",
      " 47%|                    | 2814/5962 [00:14<00:15, 205.14it/s]\u001b[A\n",
      "\n",
      "\n",
      " 49%|                   | 2904/5962 [00:14<00:14, 218.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2645/5962 [00:14<00:15, 216.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2415/5962 [00:14<00:13, 256.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2746/5962 [00:14<00:15, 207.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|                     | 2614/5962 [00:14<00:14, 236.79it/s]\u001b[A\u001b[A\n",
      " 48%|                    | 2836/5962 [00:14<00:16, 189.83it/s]\u001b[A\n",
      "\n",
      "\n",
      " 49%|                   | 2927/5962 [00:14<00:15, 194.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|                     | 2639/5962 [00:14<00:14, 222.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2768/5962 [00:14<00:16, 190.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 2668/5962 [00:14<00:18, 181.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2441/5962 [00:14<00:16, 209.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|                   | 2856/5962 [00:14<00:17, 182.18it/s]\u001b[A\n",
      "\n",
      "\n",
      " 50%|                   | 2953/5962 [00:14<00:14, 205.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 2788/5962 [00:14<00:16, 189.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                    | 2688/5962 [00:14<00:18, 176.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 2464/5962 [00:14<00:17, 201.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|                     | 2662/5962 [00:14<00:16, 195.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                   | 2979/5962 [00:14<00:13, 219.01it/s]\u001b[A\u001b[A\u001b[A\n",
      " 48%|                   | 2875/5962 [00:14<00:18, 163.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 2808/5962 [00:14<00:16, 191.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|                     | 2683/5962 [00:14<00:16, 195.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                    | 2707/5962 [00:14<00:19, 169.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                  | 3002/5962 [00:14<00:14, 206.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|                   | 2891/5962 [00:14<00:18, 164.42it/s]\u001b[A\u001b[A\u001b[A\n",
      " 49%|                   | 2893/5962 [00:14<00:19, 155.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 2828/5962 [00:14<00:17, 177.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2726/5962 [00:14<00:18, 174.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|                    | 2704/5962 [00:14<00:17, 190.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 2515/5962 [00:14<00:16, 209.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|                  | 3024/5962 [00:14<00:15, 193.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 2848/5962 [00:14<00:17, 180.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|                   | 2908/5962 [00:14<00:20, 147.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2744/5962 [00:14<00:18, 172.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|                    | 2730/5962 [00:14<00:15, 207.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2537/5962 [00:14<00:16, 209.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|                   | 2932/5962 [00:14<00:17, 172.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3044/5962 [00:14<00:15, 186.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|                   | 2940/5962 [00:14<00:15, 189.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2768/5962 [00:14<00:16, 190.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|                    | 2766/5962 [00:14<00:13, 241.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2564/5962 [00:14<00:15, 224.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|                   | 2957/5962 [00:15<00:16, 187.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3067/5962 [00:15<00:14, 194.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                   | 2973/5962 [00:14<00:13, 223.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 2792/5962 [00:14<00:15, 200.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 2590/5962 [00:14<00:14, 232.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|                    | 2791/5962 [00:15<00:13, 231.89it/s]\u001b[A\u001b[A\n",
      " 50%|                   | 2990/5962 [00:15<00:13, 224.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3087/5962 [00:15<00:14, 194.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                  | 3002/5962 [00:15<00:12, 237.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 2815/5962 [00:14<00:15, 207.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2619/5962 [00:15<00:13, 245.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|                    | 2815/5962 [00:15<00:14, 224.67it/s]\u001b[A\u001b[A\n",
      " 51%|                  | 3014/5962 [00:15<00:13, 222.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 2947/5962 [00:15<00:13, 223.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|                  | 3107/5962 [00:15<00:15, 188.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 2838/5962 [00:15<00:14, 211.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 2650/5962 [00:15<00:12, 263.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|                   | 2848/5962 [00:15<00:12, 252.41it/s]\u001b[A\u001b[A\n",
      " 51%|                  | 3051/5962 [00:15<00:11, 263.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 2976/5962 [00:15<00:12, 242.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|                  | 3063/5962 [00:15<00:10, 268.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 2865/5962 [00:15<00:13, 224.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3127/5962 [00:15<00:16, 171.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|                   | 2874/5962 [00:15<00:12, 243.98it/s]\u001b[A\u001b[A\n",
      " 52%|                  | 3080/5962 [00:15<00:10, 268.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                  | 3003/5962 [00:15<00:11, 249.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 2888/5962 [00:15<00:13, 220.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|                  | 3091/5962 [00:15<00:11, 245.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|                   | 2901/5962 [00:15<00:12, 251.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 3145/5962 [00:15<00:18, 155.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3029/5962 [00:15<00:12, 233.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|                  | 3108/5962 [00:15<00:12, 236.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 2911/5962 [00:15<00:15, 193.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2731/5962 [00:15<00:13, 239.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                 | 3161/5962 [00:15<00:18, 155.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3053/5962 [00:15<00:13, 219.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|                  | 3133/5962 [00:15<00:13, 215.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 2931/5962 [00:15<00:15, 193.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|                 | 3178/5962 [00:15<00:17, 157.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 2756/5962 [00:15<00:14, 226.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                  | 3140/5962 [00:15<00:14, 200.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3078/5962 [00:15<00:12, 227.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|                  | 3156/5962 [00:15<00:13, 209.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 2953/5962 [00:15<00:15, 197.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3195/5962 [00:15<00:19, 144.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|                   | 2949/5962 [00:15<00:17, 171.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3102/5962 [00:15<00:12, 226.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                 | 3161/5962 [00:15<00:14, 190.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 2975/5962 [00:15<00:14, 203.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|                 | 3178/5962 [00:15<00:13, 210.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3211/5962 [00:16<00:18, 146.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|                   | 2973/5962 [00:15<00:16, 184.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3129/5962 [00:15<00:12, 235.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                 | 3181/5962 [00:15<00:15, 184.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 2998/5962 [00:15<00:14, 210.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 2837/5962 [00:15<00:12, 244.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|                 | 3233/5962 [00:16<00:16, 165.78it/s]\u001b[A\n",
      "\n",
      " 50%|                   | 3000/5962 [00:16<00:14, 204.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 3154/5962 [00:15<00:11, 235.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|                 | 3203/5962 [00:16<00:14, 192.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3257/5962 [00:16<00:14, 186.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 2863/5962 [00:16<00:12, 242.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|                  | 3033/5962 [00:16<00:12, 235.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|                 | 3178/5962 [00:16<00:12, 228.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|                 | 3220/5962 [00:16<00:15, 174.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3046/5962 [00:16<00:12, 224.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                 | 3280/5962 [00:16<00:14, 190.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|                  | 3059/5962 [00:16<00:13, 222.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 2888/5962 [00:16<00:14, 208.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3201/5962 [00:16<00:14, 196.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|                 | 3242/5962 [00:16<00:16, 165.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3069/5962 [00:16<00:14, 198.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|                 | 3300/5962 [00:16<00:14, 184.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 2911/5962 [00:16<00:14, 211.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3228/5962 [00:16<00:12, 215.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|                  | 3083/5962 [00:16<00:14, 201.55it/s]\u001b[A\u001b[A\n",
      " 55%|                 | 3265/5962 [00:16<00:15, 175.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3099/5962 [00:16<00:13, 219.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|                | 3322/5962 [00:16<00:14, 188.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 2933/5962 [00:16<00:15, 199.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3251/5962 [00:16<00:13, 207.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|                 | 3289/5962 [00:16<00:14, 190.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3342/5962 [00:16<00:13, 191.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                 | 3276/5962 [00:16<00:17, 150.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|                  | 3105/5962 [00:16<00:16, 177.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 2954/5962 [00:16<00:15, 195.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3276/5962 [00:16<00:12, 218.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|                 | 3310/5962 [00:16<00:13, 191.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3363/5962 [00:16<00:13, 196.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                 | 3304/5962 [00:16<00:14, 179.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|                  | 3124/5962 [00:16<00:16, 177.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 2980/5962 [00:16<00:14, 212.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3299/5962 [00:16<00:12, 216.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 57%|                | 3386/5962 [00:16<00:12, 203.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|                 | 3170/5962 [00:16<00:13, 205.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|                | 3336/5962 [00:16<00:12, 213.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|                  | 3143/5962 [00:16<00:15, 180.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                  | 3009/5962 [00:16<00:12, 231.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|                | 3368/5962 [00:16<00:10, 238.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3324/5962 [00:16<00:11, 221.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|                | 3407/5962 [00:17<00:13, 187.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3191/5962 [00:16<00:14, 193.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|                 | 3162/5962 [00:16<00:16, 168.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3033/5962 [00:16<00:12, 232.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 57%|                | 3393/5962 [00:17<00:10, 240.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3350/5962 [00:16<00:11, 230.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|                | 3391/5962 [00:16<00:10, 234.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3217/5962 [00:16<00:13, 209.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|                | 3427/5962 [00:17<00:14, 172.17it/s]\u001b[A\u001b[A\n",
      " 57%|                | 3425/5962 [00:17<00:09, 261.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 3059/5962 [00:16<00:12, 231.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 3375/5962 [00:16<00:11, 233.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|                | 3431/5962 [00:17<00:09, 279.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3243/5962 [00:16<00:12, 220.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|                | 3461/5962 [00:17<00:11, 214.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3087/5962 [00:17<00:11, 244.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|                | 3452/5962 [00:17<00:09, 260.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 3406/5962 [00:17<00:10, 254.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3269/5962 [00:17<00:11, 229.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|                | 3460/5962 [00:17<00:09, 258.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|               | 3484/5962 [00:17<00:11, 214.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 3439/5962 [00:17<00:09, 275.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|               | 3479/5962 [00:17<00:09, 258.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 3112/5962 [00:17<00:12, 226.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3293/5962 [00:17<00:11, 232.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|               | 3489/5962 [00:17<00:09, 265.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3507/5962 [00:17<00:11, 211.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|               | 3506/5962 [00:17<00:09, 256.39it/s]\u001b[A\n",
      "\n",
      " 54%|                 | 3235/5962 [00:17<00:18, 147.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3320/5962 [00:17<00:11, 239.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|               | 3528/5962 [00:17<00:08, 299.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 3136/5962 [00:17<00:14, 196.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3532/5962 [00:17<00:11, 217.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|               | 3532/5962 [00:17<00:09, 249.50it/s]\u001b[A\n",
      "\n",
      " 55%|                 | 3251/5962 [00:17<00:18, 148.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3346/5962 [00:17<00:10, 245.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|               | 3559/5962 [00:17<00:08, 284.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3568/5962 [00:17<00:09, 253.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|               | 3559/5962 [00:17<00:09, 255.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 3157/5962 [00:17<00:16, 174.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|                 | 3283/5962 [00:17<00:13, 192.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 3378/5962 [00:17<00:09, 266.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3569/5962 [00:17<00:08, 280.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|                 | 3312/5962 [00:17<00:12, 218.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|                 | 3176/5962 [00:17<00:15, 174.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|               | 3589/5962 [00:17<00:09, 252.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3594/5962 [00:17<00:10, 223.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|               | 3585/5962 [00:17<00:11, 211.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3600/5962 [00:17<00:08, 284.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3206/5962 [00:17<00:13, 205.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|                | 3341/5962 [00:17<00:11, 233.10it/s]\u001b[A\u001b[A\n",
      " 61%|               | 3613/5962 [00:17<00:10, 226.16it/s]\u001b[A\n",
      "\n",
      "\n",
      " 61%|               | 3618/5962 [00:17<00:11, 197.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 3443/5962 [00:17<00:10, 238.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 3634/5962 [00:17<00:07, 299.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 3231/5962 [00:17<00:12, 217.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|                | 3365/5962 [00:17<00:11, 218.09it/s]\u001b[A\u001b[A\n",
      " 61%|              | 3637/5962 [00:18<00:10, 223.03it/s]\u001b[A\n",
      "\n",
      "\n",
      " 61%|              | 3639/5962 [00:18<00:12, 190.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3254/5962 [00:17<00:12, 220.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 3665/5962 [00:17<00:08, 270.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 3469/5962 [00:17<00:11, 216.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|              | 3663/5962 [00:18<00:09, 231.60it/s]\u001b[A\n",
      "\n",
      " 57%|                | 3388/5962 [00:18<00:12, 204.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|              | 3669/5962 [00:18<00:10, 217.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 3286/5962 [00:18<00:10, 246.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 3701/5962 [00:18<00:07, 290.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3493/5962 [00:18<00:11, 211.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|                | 3412/5962 [00:18<00:11, 213.05it/s]\u001b[A\u001b[A\n",
      " 62%|              | 3693/5962 [00:18<00:09, 248.55it/s]\u001b[A\n",
      "\n",
      "\n",
      " 62%|              | 3703/5962 [00:18<00:09, 247.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                 | 3312/5962 [00:18<00:11, 232.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3731/5962 [00:18<00:07, 287.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3519/5962 [00:18<00:10, 222.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|                | 3436/5962 [00:18<00:11, 219.71it/s]\u001b[A\u001b[A\n",
      " 62%|              | 3719/5962 [00:18<00:09, 248.88it/s]\u001b[A\n",
      "\n",
      "\n",
      " 63%|              | 3731/5962 [00:18<00:08, 254.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3336/5962 [00:18<00:12, 206.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3543/5962 [00:18<00:11, 212.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3761/5962 [00:18<00:08, 248.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|                | 3459/5962 [00:18<00:12, 196.35it/s]\u001b[A\u001b[A\n",
      " 63%|              | 3745/5962 [00:18<00:09, 223.29it/s]\u001b[A\n",
      "\n",
      "\n",
      " 63%|              | 3773/5962 [00:18<00:09, 220.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3758/5962 [00:18<00:10, 207.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 3358/5962 [00:18<00:13, 199.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|              | 3769/5962 [00:18<00:09, 224.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 3788/5962 [00:18<00:08, 244.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|               | 3480/5962 [00:18<00:13, 182.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|              | 3782/5962 [00:18<00:10, 215.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3607/5962 [00:18<00:08, 262.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 3820/5962 [00:18<00:08, 262.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|             | 3792/5962 [00:18<00:09, 217.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 3379/5962 [00:18<00:14, 179.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|               | 3507/5962 [00:18<00:12, 204.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|             | 3807/5962 [00:18<00:09, 223.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 3635/5962 [00:18<00:08, 264.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3849/5962 [00:18<00:07, 270.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|             | 3820/5962 [00:18<00:09, 229.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 3409/5962 [00:18<00:12, 208.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|               | 3534/5962 [00:18<00:11, 219.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|             | 3858/5962 [00:18<00:08, 258.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 3667/5962 [00:18<00:08, 277.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3879/5962 [00:18<00:07, 277.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 3438/5962 [00:18<00:11, 229.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|             | 3844/5962 [00:18<00:09, 225.74it/s]\u001b[A\n",
      "\n",
      " 64%|             | 3831/5962 [00:18<00:10, 196.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|             | 3891/5962 [00:18<00:07, 273.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 3908/5962 [00:18<00:07, 264.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 3696/5962 [00:18<00:09, 247.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3852/5962 [00:19<00:10, 194.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|             | 3867/5962 [00:19<00:09, 211.29it/s]\u001b[A\n",
      "\n",
      " 60%|               | 3587/5962 [00:19<00:10, 222.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|             | 3873/5962 [00:19<00:10, 195.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 3722/5962 [00:18<00:09, 237.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3489/5962 [00:19<00:10, 226.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 3935/5962 [00:18<00:08, 230.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|               | 3610/5962 [00:19<00:11, 208.72it/s]\u001b[A\u001b[A\n",
      " 65%|             | 3889/5962 [00:19<00:10, 196.02it/s]\u001b[A\n",
      "\n",
      "\n",
      " 65%|             | 3896/5962 [00:19<00:10, 203.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3748/5962 [00:19<00:09, 241.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3513/5962 [00:19<00:11, 216.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|            | 3960/5962 [00:19<00:08, 228.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|             | 3909/5962 [00:19<00:10, 186.92it/s]\u001b[A\n",
      "\n",
      " 61%|              | 3632/5962 [00:19<00:11, 198.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|            | 3980/5962 [00:19<00:07, 273.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 3917/5962 [00:19<00:10, 188.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 3984/5962 [00:19<00:08, 228.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|              | 3655/5962 [00:19<00:11, 204.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|            | 4008/5962 [00:19<00:07, 271.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 3536/5962 [00:19<00:13, 182.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|             | 3928/5962 [00:19<00:12, 157.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 3937/5962 [00:19<00:12, 164.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|              | 3676/5962 [00:19<00:12, 189.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 3803/5962 [00:19<00:11, 195.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|            | 4036/5962 [00:19<00:08, 227.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3556/5962 [00:19<00:15, 154.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|            | 3955/5962 [00:19<00:12, 163.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|            | 3945/5962 [00:19<00:14, 135.20it/s]\u001b[A\n",
      "\n",
      " 62%|              | 3696/5962 [00:19<00:12, 180.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|            | 4061/5962 [00:19<00:08, 227.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 3825/5962 [00:19<00:11, 182.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 3585/5962 [00:19<00:12, 183.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4056/5962 [00:19<00:08, 217.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|            | 3974/5962 [00:19<00:11, 167.78it/s]\u001b[A\n",
      "\n",
      " 62%|              | 3721/5962 [00:19<00:11, 198.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|            | 4097/5962 [00:19<00:07, 260.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3848/5962 [00:19<00:11, 192.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|               | 3623/5962 [00:19<00:10, 227.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4079/5962 [00:19<00:08, 217.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|            | 3995/5962 [00:19<00:11, 177.54it/s]\u001b[A\n",
      "\n",
      " 63%|              | 3754/5962 [00:19<00:09, 230.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|           | 4136/5962 [00:19<00:06, 294.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3874/5962 [00:19<00:09, 208.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 3655/5962 [00:19<00:09, 249.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 4016/5962 [00:20<00:10, 186.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|            | 4022/5962 [00:19<00:09, 199.11it/s]\u001b[A\n",
      "\n",
      " 64%|             | 3787/5962 [00:19<00:08, 257.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3900/5962 [00:19<00:09, 219.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|           | 4167/5962 [00:19<00:06, 273.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 3682/5962 [00:19<00:09, 249.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4035/5962 [00:20<00:10, 181.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|            | 4044/5962 [00:20<00:09, 197.99it/s]\u001b[A\n",
      "\n",
      " 64%|             | 3814/5962 [00:20<00:10, 198.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 3923/5962 [00:20<00:11, 174.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 3709/5962 [00:20<00:10, 216.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4147/5962 [00:20<00:09, 184.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|            | 4054/5962 [00:20<00:12, 151.10it/s]\u001b[A\u001b[A\u001b[A\n",
      " 68%|            | 4065/5962 [00:20<00:11, 169.58it/s]\u001b[A\n",
      "\n",
      " 65%|             | 3850/5962 [00:20<00:08, 235.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|            | 3944/5962 [00:20<00:11, 182.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4176/5962 [00:20<00:08, 210.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|           | 4227/5962 [00:20<00:07, 234.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3733/5962 [00:20<00:10, 207.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|            | 4075/5962 [00:20<00:11, 164.50it/s]\u001b[A\n",
      "\n",
      " 65%|             | 3877/5962 [00:20<00:08, 238.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 3967/5962 [00:20<00:10, 188.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|            | 4093/5962 [00:20<00:11, 168.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|           | 4253/5962 [00:20<00:07, 231.37it/s]\u001b[A\u001b[A\u001b[A\n",
      " 69%|           | 4129/5962 [00:20<00:08, 203.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3755/5962 [00:20<00:12, 181.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 3988/5962 [00:20<00:10, 192.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 4225/5962 [00:20<00:07, 222.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|           | 4112/5962 [00:20<00:10, 172.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|          | 4283/5962 [00:20<00:06, 246.51it/s]\u001b[A\u001b[A\u001b[A\n",
      " 70%|           | 4158/5962 [00:20<00:08, 222.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 3775/5962 [00:20<00:12, 181.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 4014/5962 [00:20<00:09, 209.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 4134/5962 [00:20<00:09, 183.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|          | 4310/5962 [00:20<00:07, 232.30it/s]\u001b[A\u001b[A\u001b[A\n",
      " 70%|           | 4189/5962 [00:20<00:07, 243.98it/s]\u001b[A\n",
      "\n",
      " 66%|             | 3927/5962 [00:20<00:10, 185.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 3803/5962 [00:20<00:10, 202.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4036/5962 [00:20<00:09, 211.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4156/5962 [00:20<00:09, 191.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|           | 4216/5962 [00:20<00:07, 244.67it/s]\u001b[A\n",
      "\n",
      "\n",
      " 73%|          | 4335/5962 [00:20<00:07, 222.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|            | 3948/5962 [00:20<00:10, 191.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 3828/5962 [00:20<00:10, 212.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4176/5962 [00:20<00:09, 191.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4058/5962 [00:20<00:10, 183.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|            | 3971/5962 [00:20<00:09, 199.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|          | 4359/5962 [00:20<00:07, 222.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3852/5962 [00:20<00:09, 218.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|           | 4196/5962 [00:21<00:09, 186.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4329/5962 [00:20<00:08, 200.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|          | 4382/5962 [00:20<00:07, 221.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3876/5962 [00:20<00:09, 221.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|            | 3993/5962 [00:21<00:10, 193.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4078/5962 [00:20<00:11, 159.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|           | 4215/5962 [00:21<00:09, 182.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4351/5962 [00:20<00:07, 201.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 3905/5962 [00:21<00:08, 240.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|          | 4405/5962 [00:21<00:07, 202.89it/s]\u001b[A\u001b[A\u001b[A\n",
      " 71%|           | 4236/5962 [00:21<00:09, 190.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|            | 4096/5962 [00:21<00:12, 152.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|            | 4014/5962 [00:21<00:11, 170.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 3932/5962 [00:21<00:08, 241.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4373/5962 [00:21<00:08, 186.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 4120/5962 [00:21<00:10, 172.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|          | 4256/5962 [00:21<00:09, 184.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|            | 4033/5962 [00:21<00:11, 171.58it/s]\u001b[A\u001b[A\n",
      " 73%|          | 4323/5962 [00:21<00:07, 209.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 4393/5962 [00:21<00:08, 182.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4145/5962 [00:21<00:09, 190.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4275/5962 [00:21<00:09, 178.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|         | 4446/5962 [00:21<00:08, 180.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|            | 4051/5962 [00:21<00:11, 166.89it/s]\u001b[A\u001b[A\n",
      " 73%|          | 4345/5962 [00:21<00:08, 197.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 4412/5962 [00:21<00:08, 179.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4301/5962 [00:21<00:08, 201.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 3979/5962 [00:21<00:09, 204.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|            | 4069/5962 [00:21<00:11, 169.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|         | 4465/5962 [00:21<00:09, 165.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|         | 4431/5962 [00:21<00:08, 181.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4186/5962 [00:21<00:09, 181.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|            | 4087/5962 [00:21<00:11, 170.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4322/5962 [00:21<00:08, 184.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|          | 4366/5962 [00:21<00:10, 159.15it/s]\u001b[A\n",
      "\n",
      "\n",
      " 75%|         | 4482/5962 [00:21<00:09, 161.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4450/5962 [00:21<00:08, 181.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|           | 4110/5962 [00:21<00:09, 186.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4345/5962 [00:21<00:08, 193.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 4205/5962 [00:21<00:10, 167.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|          | 4387/5962 [00:21<00:09, 167.98it/s]\u001b[A\n",
      "\n",
      "\n",
      " 75%|         | 4499/5962 [00:21<00:09, 160.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4469/5962 [00:21<00:08, 170.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|           | 4138/5962 [00:21<00:08, 211.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4368/5962 [00:21<00:07, 202.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 4224/5962 [00:21<00:10, 172.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|          | 4408/5962 [00:21<00:08, 176.22it/s]\u001b[A\n",
      "\n",
      "\n",
      " 76%|         | 4518/5962 [00:21<00:08, 168.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4487/5962 [00:21<00:08, 172.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|           | 4160/5962 [00:21<00:08, 212.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 4063/5962 [00:21<00:09, 197.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 4243/5962 [00:21<00:09, 175.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|          | 4389/5962 [00:22<00:08, 179.72it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|         | 4427/5962 [00:22<00:09, 160.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4505/5962 [00:21<00:08, 165.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|            | 4084/5962 [00:21<00:09, 198.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4270/5962 [00:21<00:08, 201.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|           | 4182/5962 [00:22<00:09, 192.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|          | 4411/5962 [00:22<00:08, 188.57it/s]\u001b[A\u001b[A\u001b[A\n",
      " 75%|         | 4444/5962 [00:22<00:09, 156.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4530/5962 [00:21<00:07, 188.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4293/5962 [00:22<00:07, 208.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 4105/5962 [00:22<00:09, 199.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|           | 4210/5962 [00:22<00:08, 216.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|        | 4588/5962 [00:22<00:06, 209.69it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|         | 4431/5962 [00:22<00:08, 178.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4556/5962 [00:22<00:06, 205.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4315/5962 [00:22<00:07, 210.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 4131/5962 [00:22<00:08, 214.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|           | 4239/5962 [00:22<00:07, 235.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|        | 4610/5962 [00:22<00:07, 188.44it/s]\u001b[A\u001b[A\u001b[A\n",
      " 75%|         | 4490/5962 [00:22<00:08, 183.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4450/5962 [00:22<00:08, 174.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|          | 4265/5962 [00:22<00:06, 242.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4337/5962 [00:22<00:08, 200.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4153/5962 [00:22<00:09, 196.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|        | 4630/5962 [00:22<00:07, 184.88it/s]\u001b[A\u001b[A\u001b[A\n",
      " 76%|         | 4510/5962 [00:22<00:07, 183.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4468/5962 [00:22<00:08, 168.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|          | 4290/5962 [00:22<00:06, 239.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4368/5962 [00:22<00:06, 229.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 4180/5962 [00:22<00:08, 216.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|        | 4655/5962 [00:22<00:06, 200.94it/s]\u001b[A\u001b[A\u001b[A\n",
      " 76%|         | 4535/5962 [00:22<00:07, 200.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4486/5962 [00:22<00:08, 164.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 4395/5962 [00:22<00:06, 240.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 4209/5962 [00:22<00:07, 232.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|          | 4315/5962 [00:22<00:07, 216.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|        | 4677/5962 [00:22<00:06, 204.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4671/5962 [00:22<00:04, 268.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|         | 4503/5962 [00:22<00:09, 154.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|         | 4420/5962 [00:22<00:06, 220.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 4233/5962 [00:22<00:07, 219.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|        | 4698/5962 [00:22<00:06, 188.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4698/5962 [00:22<00:05, 252.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|         | 4519/5962 [00:22<00:09, 145.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|          | 4256/5962 [00:22<00:09, 178.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4534/5962 [00:23<00:11, 121.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|        | 4718/5962 [00:22<00:09, 138.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4724/5962 [00:22<00:06, 177.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|        | 4576/5962 [00:23<00:12, 111.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4555/5962 [00:23<00:09, 140.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|          | 4358/5962 [00:23<00:13, 121.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4745/5962 [00:22<00:06, 179.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4462/5962 [00:22<00:10, 144.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|       | 4734/5962 [00:23<00:09, 135.07it/s]\u001b[A\u001b[A\u001b[A\n",
      " 77%|        | 4573/5962 [00:23<00:09, 150.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4294/5962 [00:23<00:11, 150.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|          | 4379/5962 [00:23<00:11, 137.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|       | 4756/5962 [00:23<00:07, 152.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 4480/5962 [00:23<00:09, 149.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4766/5962 [00:23<00:06, 182.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|        | 4613/5962 [00:23<00:10, 124.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 4312/5962 [00:23<00:10, 156.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|        | 4589/5962 [00:23<00:09, 145.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|       | 4785/5962 [00:23<00:06, 184.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4506/5962 [00:23<00:08, 174.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4796/5962 [00:23<00:05, 210.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|        | 4637/5962 [00:23<00:08, 147.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4354/5962 [00:23<00:07, 220.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|        | 4605/5962 [00:23<00:09, 147.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|       | 4815/5962 [00:23<00:05, 213.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4531/5962 [00:23<00:07, 192.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|       | 4823/5962 [00:23<00:05, 225.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|        | 4662/5962 [00:23<00:07, 170.71it/s]\u001b[A\n",
      "\n",
      " 75%|         | 4447/5962 [00:23<00:07, 189.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 4378/5962 [00:23<00:07, 213.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|       | 4843/5962 [00:23<00:04, 230.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|       | 4847/5962 [00:23<00:04, 225.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4621/5962 [00:23<00:09, 139.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|        | 4682/5962 [00:23<00:07, 161.48it/s]\u001b[A\n",
      "\n",
      " 75%|         | 4469/5962 [00:23<00:08, 181.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 4871/5962 [00:23<00:04, 222.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|       | 4868/5962 [00:23<00:05, 218.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4636/5962 [00:23<00:10, 130.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 4590/5962 [00:23<00:07, 195.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|        | 4701/5962 [00:23<00:07, 166.75it/s]\u001b[A\n",
      "\n",
      " 75%|         | 4494/5962 [00:23<00:07, 197.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 4898/5962 [00:23<00:04, 231.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|      | 4893/5962 [00:23<00:04, 225.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4651/5962 [00:23<00:09, 133.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 4612/5962 [00:23<00:06, 194.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|       | 4732/5962 [00:23<00:06, 202.32it/s]\u001b[A\n",
      "\n",
      " 76%|         | 4531/5962 [00:23<00:05, 242.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 4947/5962 [00:23<00:03, 302.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|      | 4928/5962 [00:23<00:04, 255.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4665/5962 [00:23<00:09, 134.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4633/5962 [00:23<00:06, 193.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|       | 4764/5962 [00:23<00:05, 233.51it/s]\u001b[A\n",
      "\n",
      " 77%|         | 4563/5962 [00:23<00:05, 263.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|      | 4958/5962 [00:23<00:03, 267.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4680/5962 [00:24<00:09, 137.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 4979/5962 [00:23<00:03, 249.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|       | 4789/5962 [00:24<00:04, 237.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4654/5962 [00:23<00:06, 194.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|        | 4595/5962 [00:24<00:04, 278.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 4991/5962 [00:24<00:03, 284.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4694/5962 [00:24<00:09, 130.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4677/5962 [00:24<00:06, 203.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|       | 4814/5962 [00:24<00:05, 225.32it/s]\u001b[A\n",
      "\n",
      " 78%|        | 4624/5962 [00:24<00:05, 262.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 5020/5962 [00:24<00:03, 281.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 4548/5962 [00:24<00:05, 272.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4708/5962 [00:24<00:09, 125.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4707/5962 [00:24<00:05, 226.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|        | 4652/5962 [00:24<00:04, 265.48it/s]\u001b[A\u001b[A\n",
      " 81%|       | 4843/5962 [00:24<00:04, 239.51it/s]\u001b[A\n",
      "\n",
      "\n",
      " 85%|     | 5049/5962 [00:24<00:03, 280.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|       | 4731/5962 [00:24<00:08, 152.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|       | 4735/5962 [00:24<00:05, 241.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 5031/5962 [00:24<00:04, 201.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|     | 5078/5962 [00:24<00:03, 282.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 4609/5962 [00:24<00:04, 283.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|       | 4754/5962 [00:24<00:07, 172.16it/s]\u001b[A\u001b[A\n",
      " 82%|       | 4868/5962 [00:24<00:05, 200.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4765/5962 [00:24<00:04, 256.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5053/5962 [00:24<00:04, 199.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|     | 5107/5962 [00:24<00:03, 279.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 4645/5962 [00:24<00:04, 303.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|        | 4706/5962 [00:24<00:05, 230.98it/s]\u001b[A\u001b[A\n",
      " 80%|       | 4776/5962 [00:24<00:06, 182.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4792/5962 [00:24<00:04, 256.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5075/5962 [00:24<00:04, 203.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|     | 5140/5962 [00:24<00:02, 293.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4681/5962 [00:24<00:04, 319.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|       | 4733/5962 [00:24<00:05, 234.60it/s]\u001b[A\u001b[A\n",
      " 80%|       | 4798/5962 [00:24<00:06, 184.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|       | 4818/5962 [00:24<00:04, 238.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5097/5962 [00:24<00:04, 204.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 4714/5962 [00:24<00:04, 304.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|     | 5170/5962 [00:24<00:02, 265.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|       | 4818/5962 [00:24<00:06, 188.28it/s]\u001b[A\u001b[A\n",
      " 83%|      | 4938/5962 [00:24<00:04, 206.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5122/5962 [00:24<00:04, 209.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|       | 4843/5962 [00:24<00:04, 231.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4745/5962 [00:24<00:04, 294.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|    | 5198/5962 [00:24<00:03, 250.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|       | 4837/5962 [00:24<00:06, 183.40it/s]\u001b[A\u001b[A\n",
      " 83%|      | 4964/5962 [00:24<00:04, 216.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5152/5962 [00:24<00:03, 233.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 4867/5962 [00:24<00:04, 233.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 4775/5962 [00:24<00:04, 287.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|    | 5224/5962 [00:24<00:02, 248.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|       | 4866/5962 [00:25<00:05, 210.57it/s]\u001b[A\u001b[A\n",
      " 84%|      | 5000/5962 [00:25<00:03, 252.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 5181/5962 [00:24<00:03, 248.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 4900/5962 [00:24<00:04, 258.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 4888/5962 [00:25<00:05, 197.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|    | 5207/5962 [00:24<00:03, 235.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|       | 4852/5962 [00:25<00:04, 229.75it/s]\u001b[A\u001b[A\n",
      " 84%|      | 5026/5962 [00:25<00:04, 221.34it/s]\u001b[A\n",
      "\n",
      "\n",
      " 88%|    | 5250/5962 [00:25<00:03, 203.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 4927/5962 [00:25<00:04, 229.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 4914/5962 [00:25<00:04, 213.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|       | 4877/5962 [00:25<00:04, 234.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5232/5962 [00:25<00:03, 232.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|     | 5057/5962 [00:25<00:03, 243.84it/s]\u001b[A\n",
      "\n",
      "\n",
      " 89%|    | 5278/5962 [00:25<00:03, 220.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 4863/5962 [00:25<00:04, 271.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 4951/5962 [00:25<00:04, 210.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|      | 4936/5962 [00:25<00:05, 198.78it/s]\u001b[A\u001b[A\n",
      " 85%|     | 5083/5962 [00:25<00:03, 239.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5256/5962 [00:25<00:03, 221.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|    | 5309/5962 [00:25<00:02, 238.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 4974/5962 [00:25<00:04, 213.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 4891/5962 [00:25<00:04, 250.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|      | 4963/5962 [00:25<00:04, 217.24it/s]\u001b[A\u001b[A\n",
      " 86%|     | 5108/5962 [00:25<00:03, 233.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5279/5962 [00:25<00:03, 218.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 4997/5962 [00:25<00:04, 216.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|    | 5335/5962 [00:25<00:02, 220.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|      | 4969/5962 [00:25<00:03, 269.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 4991/5962 [00:25<00:04, 233.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|     | 5135/5962 [00:25<00:03, 239.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5302/5962 [00:25<00:03, 209.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 5036/5962 [00:25<00:03, 262.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|   | 5359/5962 [00:25<00:02, 204.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|      | 5006/5962 [00:25<00:03, 296.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 5017/5962 [00:25<00:03, 239.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 87%|     | 5169/5962 [00:25<00:02, 266.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5328/5962 [00:25<00:02, 223.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5069/5962 [00:25<00:03, 279.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5043/5962 [00:25<00:03, 243.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|   | 5381/5962 [00:25<00:02, 200.80it/s]\u001b[A\u001b[A\u001b[A\n",
      " 87%|    | 5207/5962 [00:25<00:02, 295.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 5359/5962 [00:25<00:02, 245.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5099/5962 [00:25<00:03, 284.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|      | 5037/5962 [00:25<00:03, 249.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5069/5962 [00:25<00:03, 247.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|   | 5402/5962 [00:25<00:02, 193.09it/s]\u001b[A\u001b[A\u001b[A\n",
      " 88%|    | 5246/5962 [00:25<00:02, 313.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5129/5962 [00:25<00:02, 287.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 5384/5962 [00:25<00:02, 234.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5099/5962 [00:26<00:03, 260.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|     | 5064/5962 [00:26<00:04, 197.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5408/5962 [00:25<00:02, 224.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|   | 5422/5962 [00:25<00:03, 169.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 5083/5962 [00:25<00:02, 304.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 5159/5962 [00:25<00:03, 252.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|     | 5126/5962 [00:26<00:03, 210.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5114/5962 [00:26<00:02, 304.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|    | 5278/5962 [00:26<00:03, 213.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5431/5962 [00:26<00:02, 206.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|     | 5087/5962 [00:26<00:05, 170.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 5186/5962 [00:26<00:03, 220.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|     | 5155/5962 [00:26<00:03, 228.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 5147/5962 [00:26<00:02, 311.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5453/5962 [00:26<00:02, 208.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|    | 5304/5962 [00:26<00:03, 203.29it/s]\u001b[A\n",
      "\n",
      " 86%|     | 5112/5962 [00:26<00:04, 186.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5221/5962 [00:26<00:02, 250.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|     | 5185/5962 [00:26<00:03, 245.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5475/5962 [00:26<00:02, 205.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 5179/5962 [00:26<00:02, 281.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|    | 5329/5962 [00:26<00:02, 212.73it/s]\u001b[A\n",
      "\n",
      " 86%|     | 5147/5962 [00:26<00:03, 222.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|    | 5211/5962 [00:26<00:03, 246.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5248/5962 [00:26<00:03, 232.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5496/5962 [00:26<00:02, 186.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|    | 5353/5962 [00:26<00:02, 205.21it/s]\u001b[A\n",
      "\n",
      " 87%|     | 5172/5962 [00:26<00:03, 221.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|  | 5561/5962 [00:26<00:01, 268.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5237/5962 [00:26<00:03, 226.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5273/5962 [00:26<00:03, 205.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 5515/5962 [00:26<00:02, 178.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|    | 5211/5962 [00:26<00:02, 262.60it/s]\u001b[A\u001b[A\n",
      " 90%|   | 5376/5962 [00:26<00:02, 205.81it/s]\u001b[A\n",
      "\n",
      "\n",
      " 94%|  | 5592/5962 [00:26<00:01, 279.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 5263/5962 [00:26<00:02, 234.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5299/5962 [00:26<00:03, 215.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 5539/5962 [00:26<00:02, 193.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|  | 5625/5962 [00:26<00:01, 291.14it/s]\u001b[A\u001b[A\u001b[A\n",
      " 91%|   | 5398/5962 [00:26<00:02, 205.23it/s]\u001b[A\n",
      "\n",
      " 88%|    | 5240/5962 [00:26<00:02, 257.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5290/5962 [00:26<00:02, 241.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5333/5962 [00:26<00:02, 245.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 5565/5962 [00:26<00:01, 211.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|  | 5657/5962 [00:26<00:01, 298.79it/s]\u001b[A\u001b[A\u001b[A\n",
      " 91%|   | 5420/5962 [00:26<00:02, 207.33it/s]\u001b[A\n",
      "\n",
      " 88%|    | 5268/5962 [00:26<00:02, 252.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 5318/5962 [00:26<00:02, 250.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 5368/5962 [00:26<00:02, 273.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5591/5962 [00:26<00:01, 222.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%| | 5692/5962 [00:26<00:00, 312.11it/s]\u001b[A\u001b[A\u001b[A\n",
      " 91%|   | 5442/5962 [00:27<00:02, 195.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|    | 5352/5962 [00:27<00:02, 274.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|    | 5295/5962 [00:27<00:02, 237.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5397/5962 [00:26<00:02, 260.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%| | 5724/5962 [00:27<00:00, 300.49it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|   | 5463/5962 [00:27<00:02, 197.59it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 5393/5962 [00:27<00:01, 310.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|    | 5326/5962 [00:27<00:02, 255.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 5359/5962 [00:27<00:02, 267.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%| | 5755/5962 [00:27<00:00, 302.72it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|   | 5488/5962 [00:27<00:02, 210.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5424/5962 [00:27<00:02, 237.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|    | 5353/5962 [00:27<00:02, 251.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5425/5962 [00:27<00:01, 290.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 5387/5962 [00:27<00:02, 252.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|   | 5510/5962 [00:27<00:02, 188.96it/s]\u001b[A\n",
      "\n",
      " 90%|   | 5379/5962 [00:27<00:02, 227.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%| | 5786/5962 [00:27<00:00, 237.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5413/5962 [00:27<00:02, 233.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|  | 5655/5962 [00:27<00:01, 163.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5449/5962 [00:27<00:02, 191.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 91%|   | 5455/5962 [00:27<00:02, 223.58it/s]\u001b[A\n",
      "\n",
      " 91%|   | 5406/5962 [00:27<00:02, 237.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 5440/5962 [00:27<00:02, 238.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5471/5962 [00:27<00:02, 195.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%| | 5673/5962 [00:27<00:01, 158.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%| | 5813/5962 [00:27<00:00, 211.55it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|   | 5480/5962 [00:27<00:02, 212.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5492/5962 [00:27<00:02, 197.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|   | 5431/5962 [00:27<00:02, 210.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5465/5962 [00:27<00:02, 217.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 94%|  | 5582/5962 [00:27<00:01, 202.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5504/5962 [00:27<00:02, 208.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|| 5837/5962 [00:27<00:00, 186.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|  | 5513/5962 [00:27<00:02, 195.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|   | 5453/5962 [00:27<00:02, 212.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5488/5962 [00:27<00:02, 212.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 94%|  | 5607/5962 [00:27<00:01, 214.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5705/5962 [00:27<00:01, 136.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|  | 5527/5962 [00:27<00:02, 208.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|   | 5482/5962 [00:27<00:02, 231.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 5538/5962 [00:27<00:02, 208.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 5510/5962 [00:27<00:02, 199.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|  | 5552/5962 [00:27<00:01, 218.18it/s]\u001b[A\n",
      "\n",
      "\n",
      " 99%|| 5880/5962 [00:27<00:00, 194.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 5562/5962 [00:27<00:01, 216.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|  | 5516/5962 [00:27<00:01, 259.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5720/5962 [00:27<00:01, 129.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5575/5962 [00:28<00:01, 220.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5585/5962 [00:27<00:01, 219.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|  | 5543/5962 [00:28<00:01, 259.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5737/5962 [00:27<00:01, 138.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|  | 5650/5962 [00:28<00:01, 186.55it/s]\u001b[A\n",
      "\n",
      "\n",
      " 99%|| 5901/5962 [00:27<00:00, 190.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5599/5962 [00:28<00:01, 221.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5608/5962 [00:27<00:01, 221.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 5761/5962 [00:27<00:01, 163.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|| 5922/5962 [00:28<00:00, 195.28it/s]\u001b[A\u001b[A\u001b[A\n",
      " 95%| | 5670/5962 [00:28<00:01, 176.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5575/5962 [00:28<00:01, 205.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|  | 5624/5962 [00:28<00:01, 228.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 5779/5962 [00:28<00:01, 168.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 5943/5962 [00:28<00:00, 191.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5631/5962 [00:28<00:01, 207.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%| | 5688/5962 [00:28<00:01, 161.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5596/5962 [00:28<00:01, 194.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|  | 5648/5962 [00:28<00:01, 222.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 5962/5962 [00:28<00:00, 210.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|  | 5653/5962 [00:28<00:01, 197.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%| | 5708/5962 [00:28<00:01, 170.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 5626/5962 [00:28<00:01, 220.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%| | 5676/5962 [00:28<00:01, 237.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|  | 5620/5962 [00:28<00:01, 216.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%| | 5682/5962 [00:28<00:01, 216.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%| | 5729/5962 [00:28<00:01, 178.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5702/5962 [00:28<00:01, 243.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 5842/5962 [00:28<00:00, 191.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|  | 5647/5962 [00:28<00:01, 228.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5731/5962 [00:28<00:00, 290.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%| | 5764/5962 [00:28<00:00, 225.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5743/5962 [00:28<00:00, 287.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 5864/5962 [00:28<00:00, 197.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%| | 5685/5962 [00:28<00:01, 267.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 5765/5962 [00:28<00:00, 302.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%| | 5779/5962 [00:28<00:00, 306.25it/s]\u001b[A\n",
      "\n",
      " 96%| | 5714/5962 [00:28<00:00, 272.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|| 5884/5962 [00:28<00:00, 195.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5704/5962 [00:28<00:01, 221.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 98%| | 5823/5962 [00:28<00:00, 256.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 5811/5962 [00:28<00:00, 307.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%| | 5742/5962 [00:28<00:00, 271.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|| 5916/5962 [00:28<00:00, 228.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5727/5962 [00:28<00:01, 206.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 98%|| 5852/5962 [00:28<00:00, 265.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 5842/5962 [00:29<00:00, 301.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%| | 5772/5962 [00:28<00:00, 274.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 5962/5962 [00:28<00:00, 206.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|| 5879/5962 [00:29<00:00, 263.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 5749/5962 [00:28<00:01, 198.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|| 5876/5962 [00:29<00:00, 311.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%| | 5803/5962 [00:29<00:00, 284.22it/s]\u001b[A\u001b[A\n",
      " 99%|| 5921/5962 [00:29<00:00, 307.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 5790/5962 [00:29<00:00, 252.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|| 5915/5962 [00:29<00:00, 330.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|| 5842/5962 [00:29<00:00, 314.04it/s]\u001b[A\u001b[A\n",
      "100%|| 5956/5962 [00:29<00:00, 318.23it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 5962/5962 [00:29<00:00, 203.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 5944/5962 [00:29<00:00, 317.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 5962/5962 [00:29<00:00, 204.51it/s]\u001b[A\u001b[A\n",
      "100%|| 5962/5962 [00:29<00:00, 202.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 5855/5962 [00:29<00:00, 280.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|| 5919/5962 [00:29<00:00, 335.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 5962/5962 [00:29<00:00, 202.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 5962/5962 [00:29<00:00, 202.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_train.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa.tsv\n",
      "Processing Started...\n",
      "Data Size:  9202\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 18/1314 [00:00<00:07, 178.04it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|                                       | 30/1314 [00:00<00:04, 294.58it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  1%|                                       | 18/1314 [00:00<00:07, 179.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|                                       | 26/1314 [00:00<00:05, 256.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 25/1314 [00:00<00:05, 249.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                      | 39/1314 [00:00<00:06, 184.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 21/1314 [00:00<00:06, 194.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|                                       | 36/1314 [00:00<00:07, 162.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|                                      | 52/1314 [00:00<00:05, 238.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                      | 50/1314 [00:00<00:06, 204.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                      | 46/1314 [00:00<00:06, 194.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|                                      | 57/1314 [00:00<00:06, 181.92it/s]\u001b[A\u001b[A\n",
      "  4%|                                      | 58/1314 [00:00<00:07, 158.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                      | 41/1314 [00:00<00:07, 159.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|                                     | 76/1314 [00:00<00:05, 230.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                     | 81/1314 [00:00<00:05, 246.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                     | 88/1314 [00:00<00:04, 285.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|                                     | 90/1314 [00:00<00:05, 237.95it/s]\u001b[A\u001b[A\n",
      "  6%|                                     | 83/1314 [00:00<00:06, 184.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                     | 76/1314 [00:00<00:08, 153.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|                                   | 107/1314 [00:00<00:04, 253.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "  8%|                                   | 106/1314 [00:00<00:06, 197.67it/s]\u001b[A\n",
      "\n",
      "  9%|                                   | 115/1314 [00:00<00:05, 235.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 118/1314 [00:00<00:04, 251.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                    | 102/1314 [00:00<00:06, 186.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                   | 107/1314 [00:00<00:06, 198.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|                                   | 138/1314 [00:00<00:04, 272.26it/s]\u001b[A\u001b[A\u001b[A\n",
      " 10%|                                   | 128/1314 [00:00<00:05, 204.38it/s]\u001b[A\n",
      "\n",
      " 11%|                                  | 139/1314 [00:00<00:05, 227.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                   | 138/1314 [00:00<00:04, 282.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 142/1314 [00:00<00:04, 251.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|                                  | 166/1314 [00:00<00:04, 266.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 129/1314 [00:00<00:06, 184.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|                                  | 172/1314 [00:00<00:04, 258.61it/s]\u001b[A\u001b[A\n",
      " 11%|                                  | 150/1314 [00:00<00:05, 194.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                 | 174/1314 [00:00<00:04, 263.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                  | 171/1314 [00:00<00:04, 239.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 156/1314 [00:00<00:05, 205.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|                                 | 193/1314 [00:00<00:04, 237.77it/s]\u001b[A\u001b[A\u001b[A\n",
      " 13%|                                 | 177/1314 [00:00<00:05, 214.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 179/1314 [00:00<00:05, 212.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 201/1314 [00:00<00:04, 229.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|                                 | 199/1314 [00:00<00:05, 213.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 195/1314 [00:00<00:04, 233.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|                                | 218/1314 [00:00<00:04, 228.65it/s]\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 200/1314 [00:00<00:05, 204.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 208/1314 [00:00<00:04, 232.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 220/1314 [00:00<00:04, 226.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|                               | 243/1314 [00:01<00:03, 275.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 224/1314 [00:00<00:04, 243.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                               | 242/1314 [00:01<00:05, 209.58it/s]\u001b[A\u001b[A\u001b[A\n",
      " 17%|                                | 225/1314 [00:01<00:05, 216.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 256/1314 [00:01<00:04, 264.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|                              | 278/1314 [00:01<00:03, 295.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 251/1314 [00:01<00:04, 248.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|                                | 233/1314 [00:01<00:05, 207.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|                               | 272/1314 [00:01<00:04, 233.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                              | 293/1314 [00:01<00:03, 293.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|                               | 248/1314 [00:01<00:05, 203.06it/s]\u001b[A\n",
      "\n",
      " 24%|                             | 309/1314 [00:01<00:03, 296.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 279/1314 [00:01<00:04, 255.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 255/1314 [00:01<00:05, 198.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|                              | 302/1314 [00:01<00:04, 246.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|                             | 331/1314 [00:01<00:03, 293.45it/s]\u001b[A\u001b[A\n",
      " 20%|                               | 269/1314 [00:01<00:05, 196.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                             | 340/1314 [00:01<00:03, 289.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 323/1314 [00:01<00:03, 255.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 278/1314 [00:01<00:05, 206.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|                             | 329/1314 [00:01<00:03, 250.05it/s]\u001b[A\u001b[A\u001b[A\n",
      " 22%|                              | 290/1314 [00:01<00:05, 196.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 352/1314 [00:01<00:03, 306.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|                            | 370/1314 [00:01<00:03, 278.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|                              | 300/1314 [00:01<00:04, 204.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 350/1314 [00:01<00:03, 241.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|                            | 355/1314 [00:01<00:04, 232.08it/s]\u001b[A\u001b[A\u001b[A\n",
      " 24%|                             | 311/1314 [00:01<00:05, 198.00it/s]\u001b[A\n",
      "\n",
      " 30%|                           | 392/1314 [00:01<00:03, 284.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                           | 400/1314 [00:01<00:03, 281.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 327/1314 [00:01<00:04, 221.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 383/1314 [00:01<00:03, 264.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|                           | 379/1314 [00:01<00:04, 220.06it/s]\u001b[A\u001b[A\u001b[A\n",
      " 25%|                             | 333/1314 [00:01<00:04, 202.70it/s]\u001b[A\n",
      "\n",
      " 32%|                          | 421/1314 [00:01<00:03, 284.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                          | 434/1314 [00:01<00:02, 297.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                          | 413/1314 [00:01<00:03, 273.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 357/1314 [00:01<00:03, 240.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|                           | 404/1314 [00:01<00:04, 224.71it/s]\u001b[A\u001b[A\u001b[A\n",
      " 27%|                            | 356/1314 [00:01<00:04, 208.72it/s]\u001b[A\n",
      "\n",
      " 35%|                         | 457/1314 [00:01<00:02, 303.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                         | 469/1314 [00:01<00:02, 310.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 382/1314 [00:01<00:03, 241.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 445/1314 [00:01<00:03, 277.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|                           | 378/1314 [00:01<00:04, 191.22it/s]\u001b[A\n",
      "\n",
      "\n",
      " 32%|                          | 427/1314 [00:01<00:04, 199.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|                        | 488/1314 [00:01<00:03, 249.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 478/1314 [00:01<00:03, 239.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                         | 474/1314 [00:01<00:03, 242.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                        | 501/1314 [00:01<00:03, 243.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|                           | 404/1314 [00:01<00:04, 207.91it/s]\u001b[A\n",
      "\n",
      "\n",
      " 34%|                         | 448/1314 [00:01<00:04, 201.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|                       | 526/1314 [00:02<00:02, 279.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                        | 508/1314 [00:01<00:03, 267.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|                       | 528/1314 [00:02<00:03, 235.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|                         | 470/1314 [00:02<00:04, 204.11it/s]\u001b[A\u001b[A\u001b[A\n",
      " 32%|                          | 426/1314 [00:02<00:04, 207.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                        | 505/1314 [00:02<00:03, 218.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|                      | 556/1314 [00:02<00:02, 265.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                         | 462/1314 [00:02<00:03, 231.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                      | 565/1314 [00:02<00:02, 268.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|                        | 494/1314 [00:02<00:03, 212.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                       | 535/1314 [00:02<00:03, 235.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|                         | 448/1314 [00:02<00:04, 198.96it/s]\u001b[A\n",
      "\n",
      " 45%|                     | 591/1314 [00:02<00:02, 286.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 597/1314 [00:02<00:02, 278.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|                       | 524/1314 [00:02<00:03, 235.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                      | 566/1314 [00:02<00:02, 252.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|                        | 481/1314 [00:02<00:03, 234.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                      | 563/1314 [00:02<00:03, 223.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|                    | 621/1314 [00:02<00:02, 288.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 627/1314 [00:02<00:02, 280.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 548/1314 [00:02<00:03, 229.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                     | 602/1314 [00:02<00:02, 280.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|                        | 505/1314 [00:02<00:03, 235.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 587/1314 [00:02<00:03, 223.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|                   | 651/1314 [00:02<00:02, 287.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 659/1314 [00:02<00:02, 285.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|                      | 572/1314 [00:02<00:03, 225.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 611/1314 [00:02<00:03, 225.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 632/1314 [00:02<00:02, 249.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|                       | 529/1314 [00:02<00:03, 198.87it/s]\u001b[A\n",
      "\n",
      " 52%|                  | 689/1314 [00:02<00:02, 251.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|                      | 573/1314 [00:02<00:03, 209.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|                     | 595/1314 [00:02<00:03, 197.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 635/1314 [00:02<00:03, 202.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 659/1314 [00:02<00:02, 231.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|                      | 551/1314 [00:02<00:03, 191.59it/s]\u001b[A\n",
      "\n",
      " 54%|                  | 709/1314 [00:02<00:02, 224.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 657/1314 [00:02<00:03, 203.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|                 | 716/1314 [00:02<00:02, 218.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 596/1314 [00:02<00:04, 179.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 684/1314 [00:02<00:03, 206.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|                    | 616/1314 [00:02<00:04, 160.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|                 | 733/1314 [00:02<00:02, 220.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                 | 740/1314 [00:02<00:02, 217.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 625/1314 [00:02<00:03, 203.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                  | 709/1314 [00:02<00:02, 215.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|                    | 641/1314 [00:02<00:03, 179.23it/s]\u001b[A\u001b[A\u001b[A\n",
      " 46%|                     | 605/1314 [00:02<00:03, 188.86it/s]\u001b[A\n",
      "\n",
      " 58%|                | 766/1314 [00:02<00:02, 245.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 768/1314 [00:03<00:02, 231.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 653/1314 [00:03<00:03, 218.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                 | 738/1314 [00:02<00:02, 232.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|                   | 668/1314 [00:03<00:03, 200.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|               | 792/1314 [00:03<00:02, 244.04it/s]\u001b[A\u001b[A\n",
      " 48%|                    | 626/1314 [00:03<00:03, 181.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 793/1314 [00:03<00:02, 235.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 681/1314 [00:03<00:02, 232.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                  | 693/1314 [00:03<00:02, 211.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 763/1314 [00:03<00:02, 230.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|              | 829/1314 [00:03<00:01, 277.22it/s]\u001b[A\u001b[A\n",
      " 50%|                   | 651/1314 [00:03<00:03, 197.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 818/1314 [00:03<00:02, 221.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                 | 720/1314 [00:03<00:03, 187.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 787/1314 [00:03<00:02, 192.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                  | 706/1314 [00:03<00:03, 181.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|             | 858/1314 [00:03<00:01, 238.77it/s]\u001b[A\u001b[A\n",
      " 64%|              | 841/1314 [00:03<00:02, 202.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 792/1314 [00:03<00:02, 187.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|                | 748/1314 [00:03<00:02, 208.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 819/1314 [00:03<00:02, 222.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                 | 736/1314 [00:03<00:02, 207.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|            | 892/1314 [00:03<00:01, 262.46it/s]\u001b[A\u001b[A\n",
      " 66%|             | 867/1314 [00:03<00:02, 213.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 817/1314 [00:03<00:02, 201.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|               | 784/1314 [00:03<00:02, 245.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 848/1314 [00:03<00:01, 234.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 766/1314 [00:03<00:02, 227.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|           | 920/1314 [00:03<00:01, 260.14it/s]\u001b[A\u001b[A\n",
      " 54%|                 | 716/1314 [00:03<00:03, 186.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|              | 844/1314 [00:03<00:02, 217.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|            | 889/1314 [00:03<00:02, 193.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|             | 879/1314 [00:03<00:01, 253.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 791/1314 [00:03<00:02, 233.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|          | 948/1314 [00:03<00:01, 262.40it/s]\u001b[A\u001b[A\n",
      " 56%|                 | 737/1314 [00:03<00:03, 189.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 872/1314 [00:03<00:01, 232.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|            | 913/1314 [00:03<00:01, 202.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 914/1314 [00:03<00:01, 271.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 820/1314 [00:03<00:02, 242.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|          | 979/1314 [00:03<00:01, 273.00it/s]\u001b[A\u001b[A\n",
      " 58%|                | 764/1314 [00:03<00:02, 211.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|           | 946/1314 [00:03<00:01, 232.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|           | 945/1314 [00:03<00:01, 279.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 853/1314 [00:03<00:01, 263.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|         | 1007/1314 [00:03<00:01, 269.07it/s]\u001b[A\u001b[A\n",
      " 60%|               | 790/1314 [00:03<00:02, 219.83it/s]\u001b[A\n",
      "\n",
      "\n",
      " 67%|             | 878/1314 [00:03<00:01, 242.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 974/1314 [00:04<00:01, 244.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 975/1314 [00:03<00:01, 283.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 886/1314 [00:03<00:01, 280.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|              | 814/1314 [00:04<00:02, 223.76it/s]\u001b[A\n",
      "\n",
      " 79%|        | 1035/1314 [00:04<00:01, 261.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|            | 906/1314 [00:04<00:01, 248.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 1008/1314 [00:04<00:01, 269.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 916/1314 [00:04<00:01, 285.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 1014/1314 [00:04<00:00, 306.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|              | 841/1314 [00:04<00:02, 235.44it/s]\u001b[A\n",
      "\n",
      " 81%|       | 1062/1314 [00:04<00:01, 250.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 997/1314 [00:04<00:01, 287.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|        | 1041/1314 [00:04<00:00, 284.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 1045/1314 [00:04<00:00, 295.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|             | 865/1314 [00:04<00:01, 234.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|           | 946/1314 [00:04<00:01, 261.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|      | 1094/1314 [00:04<00:00, 268.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 1037/1314 [00:04<00:00, 319.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|       | 1070/1314 [00:04<00:00, 281.43it/s]\u001b[A\u001b[A\u001b[A\n",
      " 68%|            | 896/1314 [00:04<00:01, 255.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 973/1314 [00:04<00:01, 263.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|     | 1122/1314 [00:04<00:00, 266.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 1075/1314 [00:04<00:00, 267.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 1080/1314 [00:04<00:00, 338.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 1099/1314 [00:04<00:00, 270.68it/s]\u001b[A\u001b[A\u001b[A\n",
      " 70%|           | 922/1314 [00:04<00:01, 253.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 1000/1314 [00:04<00:01, 259.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|    | 1157/1314 [00:04<00:00, 288.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 1103/1314 [00:04<00:00, 252.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 1115/1314 [00:04<00:00, 311.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|     | 1128/1314 [00:04<00:00, 276.03it/s]\u001b[A\u001b[A\u001b[A\n",
      " 72%|          | 951/1314 [00:04<00:01, 260.34it/s]\u001b[A\n",
      "\n",
      " 90%|   | 1188/1314 [00:04<00:00, 290.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 1027/1314 [00:04<00:01, 241.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 1131/1314 [00:04<00:00, 257.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|       | 1042/1314 [00:04<00:01, 248.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 1157/1314 [00:04<00:00, 278.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|         | 984/1314 [00:04<00:01, 279.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 1052/1314 [00:04<00:01, 243.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|  | 1218/1314 [00:04<00:00, 269.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|    | 1178/1314 [00:04<00:00, 299.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|   | 1186/1314 [00:04<00:00, 277.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 1158/1314 [00:04<00:00, 235.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|        | 1013/1314 [00:04<00:01, 278.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 1083/1314 [00:04<00:00, 261.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 1220/1314 [00:04<00:00, 331.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|   | 1214/1314 [00:04<00:00, 260.06it/s]\u001b[A\n",
      "\n",
      "\n",
      " 83%|      | 1094/1314 [00:04<00:00, 231.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 1183/1314 [00:04<00:00, 217.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|  | 1246/1314 [00:04<00:00, 223.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 1125/1314 [00:04<00:00, 305.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|     | 1118/1314 [00:04<00:00, 221.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%| | 1254/1314 [00:04<00:00, 264.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|      | 1077/1314 [00:05<00:01, 236.79it/s]\u001b[A\n",
      "\n",
      " 94%|  | 1241/1314 [00:05<00:00, 194.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 1206/1314 [00:05<00:00, 164.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|     | 1141/1314 [00:05<00:00, 199.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 1157/1314 [00:05<00:00, 218.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|| 1294/1314 [00:05<00:00, 203.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%| | 1283/1314 [00:05<00:00, 244.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%| | 1264/1314 [00:05<00:00, 194.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|| 1314/1314 [00:05<00:00, 252.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 1225/1314 [00:05<00:00, 161.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|     | 1131/1314 [00:05<00:00, 233.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 1314/1314 [00:05<00:00, 251.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 1185/1314 [00:05<00:00, 208.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 1183/1314 [00:05<00:00, 187.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 1262/1314 [00:05<00:00, 207.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 1314/1314 [00:05<00:00, 243.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 1228/1314 [00:05<00:00, 267.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 1207/1314 [00:05<00:00, 198.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 1289/1314 [00:05<00:00, 221.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 1314/1314 [00:05<00:00, 242.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|  | 1242/1314 [00:05<00:00, 232.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%| | 1256/1314 [00:05<00:00, 266.70it/s]\u001b[A\u001b[A\u001b[A\n",
      " 95%|  | 1244/1314 [00:05<00:00, 331.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%| | 1283/1314 [00:05<00:00, 273.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 1314/1314 [00:05<00:00, 234.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 1314/1314 [00:05<00:00, 232.96it/s]\u001b[A\n",
      "100%|| 1314/1314 [00:05<00:00, 230.07it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb.tsv\n",
      "Processing Started...\n",
      "Data Size:  18406\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  1%|                                       | 27/2629 [00:00<00:09, 268.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  1%|                                       | 21/2629 [00:00<00:12, 204.75it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  1%|                                       | 26/2629 [00:00<00:10, 258.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|                                       | 26/2629 [00:00<00:10, 255.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 21/2629 [00:00<00:12, 202.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 54/2629 [00:00<00:10, 242.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                       | 22/2629 [00:00<00:12, 211.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|                                       | 51/2629 [00:00<00:10, 252.96it/s]\u001b[A\n",
      "\n",
      "  2%|                                       | 60/2629 [00:00<00:08, 305.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|                                       | 56/2629 [00:00<00:09, 276.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 44/2629 [00:00<00:12, 208.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 43/2629 [00:00<00:11, 219.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                       | 48/2629 [00:00<00:10, 235.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|                                      | 79/2629 [00:00<00:11, 229.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "  3%|                                      | 84/2629 [00:00<00:09, 267.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|                                      | 91/2629 [00:00<00:09, 258.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                       | 67/2629 [00:00<00:11, 215.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                       | 66/2629 [00:00<00:11, 222.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                       | 73/2629 [00:00<00:10, 240.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|                                     | 103/2629 [00:00<00:11, 227.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "  5%|                                     | 128/2629 [00:00<00:07, 331.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|                                     | 127/2629 [00:00<00:08, 294.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 102/2629 [00:00<00:09, 266.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 102/2629 [00:00<00:09, 274.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                     | 100/2629 [00:00<00:10, 250.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  5%|                                     | 130/2629 [00:00<00:10, 240.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "  6%|                                    | 168/2629 [00:00<00:06, 354.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|                                    | 164/2629 [00:00<00:07, 318.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 131/2629 [00:00<00:09, 273.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 130/2629 [00:00<00:09, 273.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                     | 136/2629 [00:00<00:08, 286.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|                                    | 171/2629 [00:00<00:08, 293.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "  8%|                                    | 204/2629 [00:00<00:07, 329.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 160/2629 [00:00<00:08, 280.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|                                    | 197/2629 [00:00<00:08, 298.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 159/2629 [00:00<00:09, 253.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                    | 165/2629 [00:00<00:08, 284.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  8%|                                    | 201/2629 [00:00<00:08, 294.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "  9%|                                   | 240/2629 [00:00<00:07, 337.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 189/2629 [00:00<00:09, 269.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                    | 185/2629 [00:00<00:10, 242.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 235/2629 [00:00<00:07, 299.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|                                   | 231/2629 [00:00<00:09, 261.84it/s]\u001b[A\n",
      "\n",
      "  9%|                                   | 228/2629 [00:00<00:10, 231.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                   | 217/2629 [00:00<00:10, 239.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|                                   | 275/2629 [00:00<00:08, 265.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                    | 210/2629 [00:00<00:12, 194.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|                                   | 266/2629 [00:01<00:09, 244.09it/s]\u001b[A\n",
      "\n",
      " 10%|                                   | 254/2629 [00:01<00:10, 218.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 234/2629 [00:00<00:10, 227.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                   | 244/2629 [00:00<00:09, 245.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|                                  | 304/2629 [00:01<00:08, 258.91it/s]\u001b[A\u001b[A\u001b[A\n",
      " 11%|                                  | 290/2629 [00:01<00:09, 249.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 300/2629 [00:01<00:08, 267.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|                                  | 282/2629 [00:01<00:10, 233.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 262/2629 [00:01<00:09, 239.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                   | 270/2629 [00:01<00:10, 229.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|                                 | 347/2629 [00:01<00:07, 302.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                  | 340/2629 [00:01<00:07, 300.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|                                  | 307/2629 [00:01<00:09, 235.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 292/2629 [00:01<00:09, 254.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 12%|                                  | 317/2629 [00:01<00:10, 225.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 294/2629 [00:01<00:10, 230.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|                                 | 383/2629 [00:01<00:07, 312.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                  | 280/2629 [00:01<00:11, 209.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|                                 | 372/2629 [00:01<00:07, 288.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 320/2629 [00:01<00:09, 246.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|                                  | 345/2629 [00:01<00:09, 236.24it/s]\u001b[A\n",
      "\n",
      "\n",
      " 16%|                                | 418/2629 [00:01<00:06, 321.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 318/2629 [00:01<00:10, 220.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                 | 411/2629 [00:01<00:07, 315.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                 | 352/2629 [00:01<00:08, 262.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 382/2629 [00:01<00:09, 236.32it/s]\u001b[A\n",
      "\n",
      " 14%|                                 | 362/2629 [00:01<00:11, 192.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|                                | 452/2629 [00:01<00:07, 274.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                  | 324/2629 [00:01<00:12, 184.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 444/2629 [00:01<00:08, 271.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 380/2629 [00:01<00:09, 238.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|                                 | 407/2629 [00:01<00:09, 236.01it/s]\u001b[A\n",
      "\n",
      " 15%|                                 | 384/2629 [00:01<00:11, 196.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                               | 483/2629 [00:01<00:07, 283.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 364/2629 [00:01<00:11, 191.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|                                | 477/2629 [00:01<00:07, 286.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 405/2629 [00:01<00:09, 235.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|                                | 447/2629 [00:01<00:07, 278.14it/s]\u001b[A\n",
      "\n",
      " 15%|                                 | 406/2629 [00:01<00:11, 194.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                 | 372/2629 [00:01<00:11, 198.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 510/2629 [00:01<00:07, 293.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|                               | 513/2629 [00:01<00:08, 247.10it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|                                | 476/2629 [00:01<00:07, 272.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 430/2629 [00:01<00:09, 222.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|                                | 432/2629 [00:01<00:10, 209.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 548/2629 [00:01<00:06, 316.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                 | 394/2629 [00:01<00:11, 199.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|                               | 540/2629 [00:01<00:08, 243.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                                | 463/2629 [00:01<00:08, 248.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|                               | 504/2629 [00:01<00:08, 261.33it/s]\u001b[A\n",
      "\n",
      " 17%|                                | 458/2629 [00:01<00:09, 221.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                              | 581/2629 [00:02<00:06, 316.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                | 415/2629 [00:01<00:11, 193.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|                              | 575/2629 [00:02<00:07, 269.29it/s]\u001b[A\u001b[A\u001b[A\n",
      " 21%|                               | 541/2629 [00:02<00:07, 289.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 489/2629 [00:01<00:08, 240.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|                               | 484/2629 [00:02<00:09, 229.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 435/2629 [00:02<00:11, 194.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                                | 479/2629 [00:02<00:09, 238.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|                              | 613/2629 [00:02<00:06, 293.15it/s]\u001b[A\u001b[A\u001b[A\n",
      " 22%|                              | 578/2629 [00:02<00:06, 310.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                              | 614/2629 [00:02<00:07, 277.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|                               | 508/2629 [00:02<00:09, 223.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                | 456/2629 [00:02<00:10, 198.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 508/2629 [00:02<00:08, 249.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|                             | 647/2629 [00:02<00:06, 305.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 646/2629 [00:02<00:06, 287.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|                               | 537/2629 [00:02<00:08, 240.40it/s]\u001b[A\u001b[A\n",
      " 23%|                              | 610/2629 [00:02<00:07, 288.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|                                | 477/2629 [00:02<00:10, 198.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|                               | 534/2629 [00:02<00:08, 234.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                             | 679/2629 [00:02<00:06, 294.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|                             | 679/2629 [00:02<00:06, 287.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|                              | 562/2629 [00:02<00:08, 239.32it/s]\u001b[A\u001b[A\n",
      " 24%|                             | 640/2629 [00:02<00:06, 289.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                               | 498/2629 [00:02<00:10, 196.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                              | 600/2629 [00:02<00:08, 250.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|                            | 710/2629 [00:02<00:07, 251.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|                              | 587/2629 [00:02<00:09, 209.76it/s]\u001b[A\u001b[A\n",
      " 25%|                             | 670/2629 [00:02<00:07, 249.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                              | 559/2629 [00:02<00:11, 187.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|                               | 518/2629 [00:02<00:11, 182.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|                              | 613/2629 [00:02<00:09, 220.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 626/2629 [00:02<00:09, 216.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|                            | 699/2629 [00:02<00:07, 255.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|                              | 584/2629 [00:02<00:10, 201.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|                            | 737/2629 [00:02<00:08, 235.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|                            | 736/2629 [00:02<00:08, 219.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|                             | 643/2629 [00:02<00:08, 241.32it/s]\u001b[A\u001b[A\n",
      " 28%|                            | 726/2629 [00:02<00:07, 256.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                              | 606/2629 [00:02<00:09, 203.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 764/2629 [00:02<00:07, 243.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|                           | 760/2629 [00:02<00:08, 222.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 649/2629 [00:02<00:11, 177.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|                             | 668/2629 [00:02<00:08, 225.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 628/2629 [00:02<00:09, 207.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|                           | 790/2629 [00:02<00:07, 241.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|                              | 581/2629 [00:02<00:11, 173.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|                           | 784/2629 [00:02<00:08, 214.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                             | 673/2629 [00:02<00:10, 191.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|                           | 782/2629 [00:03<00:07, 251.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                           | 815/2629 [00:03<00:08, 219.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|                            | 692/2629 [00:03<00:10, 193.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|                              | 599/2629 [00:03<00:12, 161.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|                           | 807/2629 [00:03<00:09, 186.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 694/2629 [00:03<00:11, 174.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 670/2629 [00:03<00:10, 185.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|                           | 808/2629 [00:03<00:08, 224.52it/s]\u001b[A\n",
      "\n",
      " 27%|                            | 716/2629 [00:03<00:09, 204.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 838/2629 [00:03<00:09, 191.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|                          | 829/2629 [00:03<00:09, 191.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 690/2629 [00:03<00:11, 175.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 713/2629 [00:03<00:12, 152.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|                             | 635/2629 [00:03<00:13, 152.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|                          | 832/2629 [00:03<00:09, 196.48it/s]\u001b[A\n",
      "\n",
      " 28%|                            | 738/2629 [00:03<00:10, 176.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|                          | 859/2629 [00:03<00:10, 175.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 709/2629 [00:03<00:10, 176.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                            | 732/2629 [00:03<00:11, 159.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 651/2629 [00:03<00:13, 147.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|                           | 759/2629 [00:03<00:10, 184.02it/s]\u001b[A\u001b[A\n",
      " 32%|                          | 853/2629 [00:03<00:09, 192.96it/s]\u001b[A\n",
      "\n",
      "\n",
      " 34%|                          | 881/2629 [00:03<00:09, 182.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                            | 731/2629 [00:03<00:10, 187.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 765/2629 [00:03<00:09, 197.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|                           | 789/2629 [00:03<00:08, 213.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|                             | 667/2629 [00:03<00:13, 148.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|                         | 891/2629 [00:03<00:08, 193.30it/s]\u001b[A\u001b[A\u001b[A\n",
      " 34%|                         | 901/2629 [00:03<00:09, 181.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                           | 787/2629 [00:03<00:09, 199.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 751/2629 [00:03<00:10, 182.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|                            | 690/2629 [00:03<00:11, 168.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|                         | 917/2629 [00:03<00:08, 208.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|                           | 812/2629 [00:03<00:09, 200.32it/s]\u001b[A\u001b[A\n",
      " 35%|                         | 920/2629 [00:03<00:09, 180.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                           | 810/2629 [00:03<00:09, 199.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|                            | 720/2629 [00:03<00:09, 204.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 770/2629 [00:03<00:11, 160.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|                         | 942/2629 [00:03<00:08, 189.91it/s]\u001b[A\u001b[A\u001b[A\n",
      " 35%|                         | 917/2629 [00:03<00:09, 182.07it/s]\u001b[A\n",
      "\n",
      " 32%|                          | 834/2629 [00:03<00:09, 185.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 831/2629 [00:03<00:09, 198.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                           | 794/2629 [00:03<00:10, 177.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|                        | 965/2629 [00:03<00:08, 197.42it/s]\u001b[A\u001b[A\u001b[A\n",
      " 36%|                         | 944/2629 [00:03<00:08, 202.10it/s]\u001b[A\n",
      "\n",
      " 33%|                          | 863/2629 [00:03<00:08, 210.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|                            | 741/2629 [00:03<00:11, 166.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 852/2629 [00:03<00:08, 197.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                           | 815/2629 [00:03<00:09, 184.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|                        | 996/2629 [00:03<00:06, 237.47it/s]\u001b[A\u001b[A\u001b[A\n",
      " 38%|                        | 986/2629 [00:04<00:08, 185.79it/s]\u001b[A\n",
      "\n",
      " 34%|                         | 886/2629 [00:04<00:08, 205.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                          | 873/2629 [00:03<00:08, 199.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                           | 760/2629 [00:03<00:11, 160.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 837/2629 [00:03<00:09, 193.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|                       | 1027/2629 [00:04<00:06, 257.17it/s]\u001b[A\u001b[A\u001b[A\n",
      " 38%|                        | 998/2629 [00:04<00:07, 211.14it/s]\u001b[A\n",
      "\n",
      " 35%|                         | 908/2629 [00:04<00:08, 204.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                         | 894/2629 [00:04<00:08, 192.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 1005/2629 [00:04<00:09, 163.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                          | 857/2629 [00:04<00:09, 193.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|                      | 1054/2629 [00:04<00:06, 242.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|                         | 936/2629 [00:04<00:07, 221.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                         | 914/2629 [00:04<00:08, 194.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 39%|                       | 1020/2629 [00:04<00:08, 200.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|                           | 796/2629 [00:04<00:11, 164.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 1022/2629 [00:04<00:10, 154.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|                        | 965/2629 [00:04<00:06, 238.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                         | 935/2629 [00:04<00:08, 196.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|                           | 816/2629 [00:04<00:10, 171.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|                      | 1079/2629 [00:04<00:07, 203.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                         | 908/2629 [00:04<00:07, 220.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|                      | 1048/2629 [00:04<00:08, 181.15it/s]\u001b[A\n",
      "\n",
      " 38%|                        | 995/2629 [00:04<00:06, 254.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                        | 962/2629 [00:04<00:07, 216.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|                          | 841/2629 [00:04<00:09, 189.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 1103/2629 [00:04<00:07, 212.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                         | 940/2629 [00:04<00:06, 248.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|                      | 1080/2629 [00:04<00:07, 217.51it/s]\u001b[A\n",
      "\n",
      " 39%|                       | 1025/2629 [00:04<00:06, 266.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                        | 990/2629 [00:04<00:07, 233.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|                      | 1091/2629 [00:04<00:07, 214.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                        | 966/2629 [00:04<00:06, 243.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|                          | 861/2629 [00:04<00:09, 184.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                      | 1107/2629 [00:04<00:06, 230.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 1019/2629 [00:04<00:06, 248.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|                      | 1052/2629 [00:04<00:06, 250.18it/s]\u001b[A\u001b[A\n",
      " 43%|                     | 1122/2629 [00:04<00:06, 239.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|                          | 884/2629 [00:04<00:08, 196.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                     | 1139/2629 [00:04<00:05, 250.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                        | 991/2629 [00:04<00:07, 206.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 1056/2629 [00:04<00:05, 281.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|                      | 1080/2629 [00:04<00:06, 256.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                         | 908/2629 [00:04<00:08, 207.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|                     | 1168/2629 [00:04<00:05, 259.97it/s]\u001b[A\n",
      "\n",
      "\n",
      " 45%|                     | 1175/2629 [00:04<00:06, 210.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 1102/2629 [00:04<00:04, 330.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 1019/2629 [00:04<00:07, 219.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|                    | 1195/2629 [00:05<00:06, 212.77it/s]\u001b[A\u001b[A\n",
      " 45%|                     | 1171/2629 [00:05<00:08, 180.15it/s]\u001b[A\n",
      "\n",
      "\n",
      " 46%|                    | 1197/2629 [00:04<00:08, 165.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                         | 929/2629 [00:04<00:11, 151.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                       | 1042/2629 [00:04<00:08, 184.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|                    | 1220/2629 [00:05<00:06, 219.98it/s]\u001b[A\u001b[A\n",
      " 45%|                    | 1196/2629 [00:05<00:07, 195.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 1136/2629 [00:05<00:06, 230.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|                        | 955/2629 [00:05<00:09, 175.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|                    | 1216/2629 [00:05<00:08, 163.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 1062/2629 [00:05<00:08, 186.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|                   | 1254/2629 [00:05<00:05, 249.21it/s]\u001b[A\u001b[A\n",
      " 47%|                    | 1230/2629 [00:05<00:06, 231.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 1165/2629 [00:05<00:06, 243.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|                        | 981/2629 [00:05<00:08, 196.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                      | 1093/2629 [00:05<00:07, 216.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|                    | 1234/2629 [00:05<00:08, 163.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|                   | 1281/2629 [00:05<00:05, 226.87it/s]\u001b[A\u001b[A\n",
      " 48%|                   | 1255/2629 [00:05<00:06, 215.87it/s]\u001b[A\n",
      "\n",
      "\n",
      " 48%|                    | 1253/2629 [00:05<00:08, 164.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                       | 1003/2629 [00:05<00:08, 180.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|                    | 1207/2629 [00:05<00:06, 226.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                    | 1193/2629 [00:05<00:07, 204.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                     | 1116/2629 [00:05<00:09, 162.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|                       | 1023/2629 [00:05<00:09, 178.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|                   | 1279/2629 [00:05<00:07, 180.10it/s]\u001b[A\n",
      "\n",
      " 47%|                    | 1231/2629 [00:05<00:06, 199.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                   | 1305/2629 [00:05<00:07, 179.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                    | 1217/2629 [00:05<00:07, 187.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 1135/2629 [00:05<00:10, 142.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|                   | 1286/2629 [00:05<00:10, 127.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|                       | 1042/2629 [00:05<00:11, 139.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|                    | 1252/2629 [00:05<00:08, 159.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 1239/2629 [00:05<00:08, 162.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|                  | 1326/2629 [00:05<00:09, 136.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 1152/2629 [00:05<00:11, 124.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|                   | 1300/2629 [00:05<00:12, 103.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|                      | 1058/2629 [00:05<00:13, 117.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 1258/2629 [00:05<00:09, 152.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|                   | 1316/2629 [00:05<00:09, 135.91it/s]\u001b[A\n",
      "\n",
      " 51%|                  | 1343/2629 [00:05<00:09, 132.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 1172/2629 [00:05<00:10, 136.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                   | 1315/2629 [00:05<00:11, 113.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 1275/2629 [00:05<00:08, 153.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 1076/2629 [00:05<00:12, 127.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|                   | 1286/2629 [00:06<00:10, 124.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 1359/2629 [00:06<00:10, 124.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|                  | 1329/2629 [00:06<00:11, 117.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                      | 1091/2629 [00:06<00:11, 129.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|                  | 1332/2629 [00:06<00:11, 112.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 1292/2629 [00:06<00:08, 150.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|                   | 1300/2629 [00:06<00:10, 127.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 1374/2629 [00:06<00:09, 129.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|                  | 1350/2629 [00:06<00:09, 138.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|                     | 1117/2629 [00:06<00:09, 159.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|                  | 1353/2629 [00:06<00:09, 130.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 1314/2629 [00:06<00:07, 166.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|                   | 1314/2629 [00:06<00:10, 128.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                 | 1396/2629 [00:06<00:08, 147.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 1228/2629 [00:06<00:09, 148.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|                     | 1135/2629 [00:06<00:09, 160.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 1334/2629 [00:06<00:07, 169.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|                  | 1369/2629 [00:06<00:09, 131.67it/s]\u001b[A\n",
      "\n",
      " 54%|                 | 1413/2629 [00:06<00:08, 150.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 1244/2629 [00:06<00:09, 149.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|                  | 1380/2629 [00:06<00:09, 136.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|                     | 1156/2629 [00:06<00:08, 172.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 1355/2629 [00:06<00:07, 177.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|                  | 1387/2629 [00:06<00:08, 140.79it/s]\u001b[A\n",
      "\n",
      " 54%|                 | 1431/2629 [00:06<00:07, 154.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 1261/2629 [00:06<00:09, 150.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|                     | 1177/2629 [00:06<00:08, 180.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|                 | 1395/2629 [00:06<00:09, 133.43it/s]\u001b[A\u001b[A\u001b[A\n",
      " 54%|                 | 1411/2629 [00:06<00:07, 160.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 1374/2629 [00:06<00:07, 160.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|                 | 1448/2629 [00:06<00:07, 156.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 1277/2629 [00:06<00:09, 142.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|                    | 1196/2629 [00:06<00:09, 158.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 1391/2629 [00:06<00:08, 144.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|                 | 1409/2629 [00:06<00:11, 104.98it/s]\u001b[A\u001b[A\u001b[A\n",
      " 54%|                 | 1429/2629 [00:06<00:09, 132.63it/s]\u001b[A\n",
      "\n",
      " 52%|                  | 1376/2629 [00:06<00:10, 123.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                   | 1293/2629 [00:06<00:09, 137.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 1465/2629 [00:06<00:09, 123.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 1410/2629 [00:06<00:07, 155.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|                 | 1421/2629 [00:06<00:11, 103.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 1307/2629 [00:06<00:10, 128.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|                 | 1444/2629 [00:06<00:10, 118.45it/s]\u001b[A\n",
      "\n",
      " 53%|                  | 1389/2629 [00:06<00:11, 107.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 1479/2629 [00:06<00:09, 120.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|                    | 1234/2629 [00:06<00:09, 143.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                 | 1433/2629 [00:06<00:11, 106.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 1321/2629 [00:06<00:10, 126.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|                 | 1461/2629 [00:07<00:09, 127.82it/s]\u001b[A\n",
      "\n",
      " 53%|                 | 1401/2629 [00:07<00:11, 109.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 1493/2629 [00:07<00:09, 121.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                 | 1456/2629 [00:07<00:08, 137.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                    | 1250/2629 [00:07<00:09, 138.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 1341/2629 [00:07<00:08, 145.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|                | 1479/2629 [00:07<00:08, 139.49it/s]\u001b[A\n",
      "\n",
      " 54%|                 | 1419/2629 [00:07<00:09, 126.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                | 1467/2629 [00:07<00:06, 171.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|                | 1506/2629 [00:07<00:09, 118.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|                   | 1273/2629 [00:07<00:08, 158.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 57%|                | 1497/2629 [00:07<00:07, 148.01it/s]\u001b[A\n",
      "\n",
      " 55%|                 | 1446/2629 [00:07<00:07, 163.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 1369/2629 [00:07<00:07, 175.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 1496/2629 [00:07<00:05, 203.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|                | 1511/2629 [00:07<00:05, 199.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 1519/2629 [00:07<00:09, 115.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|                | 1515/2629 [00:07<00:07, 155.56it/s]\u001b[A\n",
      "\n",
      " 56%|                | 1469/2629 [00:07<00:06, 180.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 1517/2629 [00:07<00:05, 205.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 1387/2629 [00:07<00:07, 163.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|               | 1533/2629 [00:07<00:09, 121.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|                   | 1313/2629 [00:07<00:07, 172.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|               | 1538/2629 [00:07<00:06, 175.30it/s]\u001b[A\n",
      "\n",
      " 57%|                | 1491/2629 [00:07<00:05, 191.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 1538/2629 [00:07<00:05, 204.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|               | 1563/2629 [00:07<00:04, 214.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 1548/2629 [00:07<00:08, 128.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|                  | 1333/2629 [00:07<00:07, 174.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|               | 1559/2629 [00:07<00:05, 183.71it/s]\u001b[A\n",
      "\n",
      " 58%|                | 1520/2629 [00:07<00:05, 217.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 1561/2629 [00:07<00:05, 207.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 1571/2629 [00:07<00:06, 156.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|               | 1586/2629 [00:07<00:05, 201.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                  | 1360/2629 [00:07<00:06, 198.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|               | 1578/2629 [00:07<00:05, 184.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 1583/2629 [00:07<00:04, 210.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|               | 1543/2629 [00:07<00:05, 198.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|               | 1591/2629 [00:07<00:06, 167.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|              | 1625/2629 [00:07<00:04, 250.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|                  | 1381/2629 [00:07<00:06, 197.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|               | 1597/2629 [00:07<00:05, 181.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 1606/2629 [00:07<00:04, 215.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|               | 1570/2629 [00:07<00:04, 217.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 1611/2629 [00:07<00:05, 174.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|              | 1661/2629 [00:07<00:03, 279.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                 | 1411/2629 [00:07<00:05, 223.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|              | 1622/2629 [00:07<00:05, 199.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 1639/2629 [00:07<00:03, 248.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|              | 1600/2629 [00:07<00:04, 237.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 1633/2629 [00:07<00:05, 186.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|             | 1690/2629 [00:07<00:03, 273.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|                 | 1437/2629 [00:07<00:05, 230.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 1678/2629 [00:07<00:03, 289.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|              | 1643/2629 [00:08<00:05, 182.90it/s]\u001b[A\n",
      "\n",
      " 63%|              | 1663/2629 [00:08<00:04, 219.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|             | 1718/2629 [00:08<00:03, 269.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                 | 1461/2629 [00:08<00:05, 219.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|               | 1543/2629 [00:08<00:05, 206.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 1708/2629 [00:07<00:03, 292.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|              | 1652/2629 [00:08<00:04, 234.14it/s]\u001b[A\u001b[A\n",
      " 64%|             | 1686/2629 [00:08<00:04, 208.57it/s]\u001b[A\n",
      "\n",
      "\n",
      " 66%|            | 1746/2629 [00:08<00:03, 270.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|                | 1489/2629 [00:08<00:04, 235.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|            | 1745/2629 [00:08<00:02, 313.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 1565/2629 [00:08<00:05, 201.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|             | 1682/2629 [00:08<00:03, 251.23it/s]\u001b[A\u001b[A\n",
      " 64%|             | 1686/2629 [00:08<00:05, 181.31it/s]\u001b[A\n",
      "\n",
      "\n",
      " 65%|             | 1708/2629 [00:08<00:04, 200.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|                | 1516/2629 [00:08<00:04, 243.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 1777/2629 [00:08<00:02, 308.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|               | 1587/2629 [00:08<00:05, 206.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|             | 1708/2629 [00:08<00:03, 253.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|            | 1804/2629 [00:08<00:03, 274.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|             | 1729/2629 [00:08<00:04, 184.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|             | 1705/2629 [00:08<00:06, 147.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|              | 1609/2629 [00:08<00:05, 187.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 1808/2629 [00:08<00:03, 252.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|           | 1832/2629 [00:08<00:02, 271.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|             | 1734/2629 [00:08<00:04, 219.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|            | 1748/2629 [00:08<00:05, 171.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|             | 1731/2629 [00:08<00:05, 173.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 1634/2629 [00:08<00:04, 202.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|           | 1866/2629 [00:08<00:02, 290.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|            | 1757/2629 [00:08<00:03, 218.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 1835/2629 [00:08<00:03, 236.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 1768/2629 [00:08<00:04, 178.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|            | 1766/2629 [00:08<00:04, 215.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 1658/2629 [00:08<00:04, 209.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|          | 1897/2629 [00:08<00:02, 286.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|            | 1786/2629 [00:08<00:03, 233.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 1787/2629 [00:08<00:04, 177.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|            | 1790/2629 [00:08<00:04, 204.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|              | 1622/2629 [00:08<00:04, 202.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|          | 1926/2629 [00:08<00:02, 258.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 1680/2629 [00:08<00:05, 173.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 1894/2629 [00:08<00:02, 253.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|            | 1806/2629 [00:08<00:04, 180.06it/s]\u001b[A\u001b[A\n",
      " 69%|           | 1816/2629 [00:08<00:03, 218.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|              | 1647/2629 [00:08<00:04, 213.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|         | 1954/2629 [00:08<00:02, 262.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 1926/2629 [00:08<00:02, 270.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|           | 1835/2629 [00:08<00:03, 223.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 1829/2629 [00:08<00:04, 192.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|           | 1840/2629 [00:09<00:03, 209.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|             | 1679/2629 [00:08<00:03, 239.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|         | 1987/2629 [00:09<00:02, 280.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|           | 1858/2629 [00:09<00:03, 225.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|         | 1955/2629 [00:08<00:02, 269.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 1855/2629 [00:09<00:03, 210.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|           | 1865/2629 [00:09<00:03, 218.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|             | 1706/2629 [00:09<00:03, 247.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|        | 2016/2629 [00:09<00:02, 274.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|          | 1888/2629 [00:09<00:03, 243.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 1983/2629 [00:09<00:02, 261.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|          | 1877/2629 [00:09<00:03, 195.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|            | 1740/2629 [00:09<00:03, 272.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|          | 1888/2629 [00:09<00:03, 212.56it/s]\u001b[A\n",
      "\n",
      " 73%|          | 1922/2629 [00:09<00:02, 269.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 2011/2629 [00:09<00:02, 262.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 1770/2629 [00:09<00:04, 203.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|          | 1897/2629 [00:09<00:03, 186.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|            | 1769/2629 [00:09<00:03, 274.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|          | 1910/2629 [00:09<00:03, 199.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|            | 1794/2629 [00:09<00:03, 211.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|        | 2074/2629 [00:09<00:02, 263.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 1928/2629 [00:09<00:03, 219.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|         | 1950/2629 [00:09<00:03, 225.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      " 74%|          | 1936/2629 [00:09<00:03, 214.52it/s]\u001b[A\n",
      "\n",
      "\n",
      " 80%|       | 2102/2629 [00:09<00:01, 265.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 1816/2629 [00:09<00:03, 208.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|         | 1951/2629 [00:09<00:03, 218.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|           | 1826/2629 [00:09<00:03, 262.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|         | 1974/2629 [00:09<00:03, 212.72it/s]\u001b[A\u001b[A\n",
      " 74%|         | 1958/2629 [00:09<00:03, 215.81it/s]\u001b[A\n",
      "\n",
      "\n",
      " 81%|       | 2132/2629 [00:09<00:01, 273.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|           | 1841/2629 [00:09<00:03, 219.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 1974/2629 [00:09<00:02, 220.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 1864/2629 [00:09<00:02, 293.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|         | 2006/2629 [00:09<00:02, 239.26it/s]\u001b[A\u001b[A\n",
      " 75%|         | 1984/2629 [00:09<00:02, 227.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|           | 1869/2629 [00:09<00:03, 236.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|      | 2160/2629 [00:09<00:01, 261.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 1997/2629 [00:09<00:03, 204.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|         | 2008/2629 [00:09<00:02, 217.15it/s]\u001b[A\n",
      "\n",
      " 77%|        | 2032/2629 [00:09<00:02, 225.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 1894/2629 [00:09<00:02, 263.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|          | 1893/2629 [00:09<00:03, 226.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|      | 2187/2629 [00:09<00:01, 237.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 2032/2629 [00:09<00:02, 243.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|        | 2060/2629 [00:09<00:02, 239.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 1916/2629 [00:09<00:03, 216.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|          | 1922/2629 [00:09<00:02, 243.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 2165/2629 [00:09<00:02, 222.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|        | 2031/2629 [00:09<00:03, 172.95it/s]\u001b[A\n",
      "\n",
      " 79%|       | 2085/2629 [00:09<00:02, 239.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 2212/2629 [00:09<00:02, 207.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|         | 1948/2629 [00:09<00:02, 239.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|          | 1938/2629 [00:09<00:03, 205.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 2057/2629 [00:10<00:03, 182.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|        | 2057/2629 [00:10<00:03, 189.73it/s]\u001b[A\n",
      "\n",
      "\n",
      " 85%|     | 2234/2629 [00:10<00:01, 206.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|       | 2110/2629 [00:10<00:02, 228.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 1963/2629 [00:10<00:03, 215.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|         | 1973/2629 [00:10<00:02, 225.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 2078/2629 [00:10<00:03, 178.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|       | 2136/2629 [00:10<00:02, 235.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|     | 2260/2629 [00:10<00:01, 218.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 1985/2629 [00:10<00:03, 210.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|        | 2078/2629 [00:10<00:03, 167.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 2000/2629 [00:10<00:02, 235.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 2101/2629 [00:10<00:02, 190.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|      | 2161/2629 [00:10<00:01, 239.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|     | 2286/2629 [00:10<00:01, 228.92it/s]\u001b[A\u001b[A\u001b[A\n",
      " 80%|       | 2102/2629 [00:10<00:02, 183.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 2029/2629 [00:10<00:02, 244.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|       | 2122/2629 [00:10<00:02, 189.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 2278/2629 [00:10<00:01, 233.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|        | 2029/2629 [00:10<00:02, 203.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|       | 2122/2629 [00:10<00:02, 173.57it/s]\u001b[A\n",
      "\n",
      " 82%|       | 2143/2629 [00:10<00:02, 192.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 2306/2629 [00:10<00:01, 244.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|    | 2310/2629 [00:10<00:01, 175.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 2054/2629 [00:10<00:02, 212.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|        | 2052/2629 [00:10<00:02, 209.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|       | 2144/2629 [00:10<00:02, 184.18it/s]\u001b[A\n",
      "\n",
      " 84%|      | 2209/2629 [00:10<00:02, 205.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 2164/2629 [00:10<00:02, 188.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 2080/2629 [00:10<00:02, 222.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|    | 2330/2629 [00:10<00:01, 163.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 2084/2629 [00:10<00:02, 240.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|     | 2235/2629 [00:10<00:01, 219.61it/s]\u001b[A\u001b[A\n",
      " 83%|      | 2184/2629 [00:10<00:02, 189.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 2375/2629 [00:10<00:00, 274.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 2113/2629 [00:10<00:02, 250.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|    | 2354/2629 [00:10<00:01, 179.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|       | 2116/2629 [00:10<00:01, 262.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|     | 2264/2629 [00:10<00:01, 237.86it/s]\u001b[A\u001b[A\n",
      " 84%|      | 2207/2629 [00:10<00:02, 198.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|       | 2139/2629 [00:10<00:01, 246.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 2403/2629 [00:10<00:00, 258.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|   | 2380/2629 [00:10<00:01, 198.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|     | 2291/2629 [00:10<00:01, 245.38it/s]\u001b[A\u001b[A\n",
      " 85%|     | 2228/2629 [00:10<00:02, 199.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 2143/2629 [00:10<00:02, 217.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|      | 2165/2629 [00:10<00:01, 233.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|   | 2406/2629 [00:10<00:01, 212.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 2430/2629 [00:10<00:00, 240.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|    | 2321/2629 [00:10<00:01, 258.78it/s]\u001b[A\u001b[A\n",
      " 86%|     | 2249/2629 [00:11<00:01, 194.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 2169/2629 [00:11<00:02, 225.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 2195/2629 [00:11<00:01, 249.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|  | 2436/2629 [00:11<00:00, 233.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 2455/2629 [00:11<00:00, 241.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|    | 2357/2629 [00:11<00:00, 287.28it/s]\u001b[A\u001b[A\n",
      " 87%|     | 2276/2629 [00:11<00:01, 214.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|      | 2194/2629 [00:11<00:01, 231.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 2221/2629 [00:11<00:01, 244.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|  | 2466/2629 [00:11<00:00, 249.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|   | 2397/2629 [00:11<00:00, 310.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|    | 2298/2629 [00:11<00:01, 212.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 2219/2629 [00:11<00:01, 223.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|     | 2247/2629 [00:11<00:01, 248.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|  | 2497/2629 [00:11<00:00, 265.67it/s]\u001b[A\u001b[A\u001b[A\n",
      " 88%|    | 2304/2629 [00:11<00:01, 197.46it/s]\u001b[A\n",
      "\n",
      " 92%|   | 2429/2629 [00:11<00:00, 278.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 2324/2629 [00:11<00:01, 226.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 2248/2629 [00:11<00:01, 238.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 2278/2629 [00:11<00:01, 265.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|    | 2327/2629 [00:11<00:01, 204.16it/s]\u001b[A\n",
      "\n",
      "\n",
      " 90%|    | 2355/2629 [00:11<00:01, 248.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 2525/2629 [00:11<00:00, 198.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|    | 2309/2629 [00:11<00:01, 276.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 2279/2629 [00:11<00:01, 256.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|  | 2458/2629 [00:11<00:00, 253.58it/s]\u001b[A\u001b[A\n",
      " 89%|    | 2350/2629 [00:11<00:01, 209.73it/s]\u001b[A\n",
      "\n",
      "\n",
      " 97%| | 2551/2629 [00:11<00:00, 248.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 2381/2629 [00:11<00:01, 226.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|  | 2485/2629 [00:11<00:00, 241.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 2337/2629 [00:11<00:01, 250.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|   | 2373/2629 [00:11<00:01, 195.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 2546/2629 [00:11<00:00, 178.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|| 2577/2629 [00:11<00:00, 242.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 2406/2629 [00:11<00:00, 231.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%| | 2511/2629 [00:11<00:00, 245.38it/s]\u001b[A\u001b[A\n",
      " 91%|   | 2400/2629 [00:11<00:01, 213.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 2579/2629 [00:11<00:00, 213.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|| 2602/2629 [00:11<00:00, 242.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 2435/2629 [00:11<00:00, 247.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%| | 2547/2629 [00:11<00:00, 274.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|   | 2362/2629 [00:11<00:01, 239.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|| 2602/2629 [00:11<00:00, 213.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 2627/2629 [00:11<00:00, 237.64it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|   | 2423/2629 [00:11<00:00, 210.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 2629/2629 [00:11<00:00, 222.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 94%|  | 2462/2629 [00:11<00:00, 252.15it/s]\n",
      "\n",
      " 98%|| 2576/2629 [00:11<00:00, 272.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 2388/2629 [00:11<00:00, 243.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 2414/2629 [00:11<00:00, 242.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|  | 2445/2629 [00:11<00:00, 206.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 2629/2629 [00:11<00:00, 221.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|  | 2495/2629 [00:12<00:00, 274.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 2414/2629 [00:11<00:00, 246.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|| 2604/2629 [00:12<00:00, 259.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 2444/2629 [00:12<00:00, 257.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 2629/2629 [00:12<00:00, 216.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 2440/2629 [00:12<00:00, 241.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%| | 2503/2629 [00:12<00:00, 243.29it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 2555/2629 [00:12<00:00, 280.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 2465/2629 [00:12<00:00, 235.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%| | 2529/2629 [00:12<00:00, 246.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 2584/2629 [00:12<00:00, 276.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|  | 2497/2629 [00:12<00:00, 259.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%| | 2554/2629 [00:12<00:00, 244.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|| 2613/2629 [00:12<00:00, 278.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%| | 2546/2629 [00:12<00:00, 324.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 2629/2629 [00:12<00:00, 210.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%| | 2566/2629 [00:12<00:00, 283.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|| 2579/2629 [00:12<00:00, 306.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|| 2613/2629 [00:12<00:00, 245.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 2629/2629 [00:12<00:00, 207.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 2629/2629 [00:12<00:00, 207.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 2629/2629 [00:12<00:00, 204.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_abolish.tsv\n",
      "Processing Started...\n",
      "Data Size:  281\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|                        | 17/40 [00:00<00:00, 159.48it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 45%|                       | 18/40 [00:00<00:00, 174.92it/s]\u001b[A\n",
      "\n",
      " 45%|                       | 18/40 [00:00<00:00, 166.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                        | 17/40 [00:00<00:00, 160.15it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|      | 34/40 [00:00<00:00, 164.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                      | 19/40 [00:00<00:00, 180.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 40/40 [00:00<00:00, 166.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 173.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                             | 12/40 [00:00<00:00, 111.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 167.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 168.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 40/40 [00:00<00:00, 171.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 151.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 137.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_abolish.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_abolish.tsv\n",
      "Processing Started...\n",
      "Data Size:  283\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 40%|                         | 16/40 [00:00<00:00, 153.58it/s]\n",
      " 57%|                 | 23/40 [00:00<00:00, 224.30it/s]\u001b[A\n",
      "\n",
      " 57%|                 | 23/40 [00:00<00:00, 229.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                   | 22/40 [00:00<00:00, 214.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 240.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 254.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 200.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 40/40 [00:00<00:00, 244.68it/s]\n",
      "100%|| 40/40 [00:00<00:00, 242.94it/s]\n",
      "100%|| 40/40 [00:00<00:00, 230.91it/s]\n",
      "100%|| 40/40 [00:00<00:00, 208.04it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_abolish.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_alter.tsv\n",
      "Processing Started...\n",
      "Data Size:  254\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 56%|                  | 20/36 [00:00<00:00, 194.50it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 58%|                 | 21/36 [00:00<00:00, 207.94it/s]\u001b[A\n",
      "\n",
      " 61%|                | 22/36 [00:00<00:00, 210.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 220.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 36/36 [00:00<00:00, 229.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|            | 25/36 [00:00<00:00, 229.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 225.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 36/36 [00:00<00:00, 232.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 259.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 36/36 [00:00<00:00, 286.10it/s]\n",
      "100%|| 36/36 [00:00<00:00, 308.45it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_alter.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_alter.tsv\n",
      "Processing Started...\n",
      "Data Size:  255\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 67%|              | 24/36 [00:00<00:00, 236.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 58%|                 | 21/36 [00:00<00:00, 204.23it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 244.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 247.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 262.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 36/36 [00:00<00:00, 267.56it/s]\n",
      "100%|| 36/36 [00:00<00:00, 264.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 32/36 [00:00<00:00, 311.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 36/36 [00:00<00:00, 326.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 36/36 [00:00<00:00, 334.29it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_alter.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_begin.tsv\n",
      "Processing Started...\n",
      "Data Size:  683\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 24%|                                | 23/97 [00:00<00:00, 226.40it/s]\n",
      " 21%|                                 | 20/97 [00:00<00:00, 196.87it/s]\u001b[A\n",
      "\n",
      " 16%|                                   | 16/97 [00:00<00:00, 152.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                                  | 17/97 [00:00<00:00, 168.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                   | 15/97 [00:00<00:00, 135.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                     | 11/97 [00:00<00:00, 104.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                   | 16/97 [00:00<00:00, 154.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|                            | 32/97 [00:00<00:00, 130.35it/s]\u001b[A\u001b[A\n",
      " 41%|                        | 40/97 [00:00<00:00, 156.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|                             | 29/97 [00:00<00:00, 137.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                                | 22/97 [00:00<00:00, 106.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|                      | 46/97 [00:00<00:00, 154.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                            | 32/97 [00:00<00:00, 136.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|                | 60/97 [00:00<00:00, 173.03it/s]\u001b[A\n",
      "\n",
      " 52%|                    | 50/97 [00:00<00:00, 150.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|                     | 48/97 [00:00<00:00, 157.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|              | 63/97 [00:00<00:00, 158.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|                | 59/97 [00:00<00:00, 179.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                  | 55/97 [00:00<00:00, 174.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|      | 83/97 [00:00<00:00, 192.39it/s]\u001b[A\n",
      "\n",
      " 70%|            | 68/97 [00:00<00:00, 160.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|            | 68/97 [00:00<00:00, 172.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|               | 61/97 [00:00<00:00, 164.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 192.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 184.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 166.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|      | 82/97 [00:00<00:00, 180.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 179.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 97/97 [00:00<00:00, 163.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 174.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 97/97 [00:00<00:00, 166.25it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_begin.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_begin.tsv\n",
      "Processing Started...\n",
      "Data Size:  684\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 29%|                              | 28/97 [00:00<00:00, 277.10it/s]\n",
      " 19%|                                  | 18/97 [00:00<00:00, 177.04it/s]\u001b[A\n",
      "\n",
      " 22%|                                 | 21/97 [00:00<00:00, 207.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|                                 | 20/97 [00:00<00:00, 195.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|                               | 25/97 [00:00<00:00, 242.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|                                  | 17/97 [00:00<00:00, 157.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                 | 56/97 [00:00<00:00, 271.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|                         | 39/97 [00:00<00:00, 194.95it/s]\u001b[A\n",
      "\n",
      " 43%|                       | 42/97 [00:00<00:00, 204.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                    | 50/97 [00:00<00:00, 223.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                           | 33/97 [00:00<00:00, 149.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|                        | 41/97 [00:00<00:00, 157.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                   | 52/97 [00:00<00:00, 188.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|              | 63/97 [00:00<00:00, 192.85it/s]\u001b[A\u001b[A\n",
      " 87%|     | 84/97 [00:00<00:00, 211.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|          | 73/97 [00:00<00:00, 218.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|                    | 50/97 [00:00<00:00, 158.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 226.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|       | 81/97 [00:00<00:00, 222.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|   | 90/97 [00:00<00:00, 219.78it/s]\u001b[A\u001b[A\n",
      " 89%|    | 86/97 [00:00<00:00, 196.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 224.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 208.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 97/97 [00:00<00:00, 225.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|| 97/97 [00:00<00:00, 195.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 97/97 [00:00<00:00, 192.79it/s]\n",
      "100%|| 97/97 [00:00<00:00, 187.09it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_begin.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_block.tsv\n",
      "Processing Started...\n",
      "Data Size:  362\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 25%|                               | 13/51 [00:00<00:00, 125.83it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 25%|                               | 13/51 [00:00<00:00, 101.09it/s]\n",
      "\n",
      " 20%|                                  | 10/51 [00:00<00:00, 92.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 75%|          | 38/51 [00:00<00:00, 175.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                       | 23/51 [00:00<00:00, 208.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|              | 34/51 [00:00<00:00, 165.34it/s]\u001b[A\n",
      "\n",
      " 49%|                     | 25/51 [00:00<00:00, 124.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|                       | 23/51 [00:00<00:00, 229.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 181.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 51/51 [00:00<00:00, 180.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 170.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 44/51 [00:00<00:00, 190.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 239.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 199.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 51/51 [00:00<00:00, 188.54it/s]\n",
      "100%|| 51/51 [00:00<00:00, 215.63it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_block.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_block.tsv\n",
      "Processing Started...\n",
      "Data Size:  363\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                             | 15/51 [00:00<00:00, 147.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|                              | 14/51 [00:00<00:00, 130.01it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 25%|                               | 13/51 [00:00<00:00, 120.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|                | 31/51 [00:00<00:00, 149.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|                         | 20/51 [00:00<00:00, 193.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                          | 19/51 [00:00<00:00, 187.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                           | 18/51 [00:00<00:00, 178.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|                 | 30/51 [00:00<00:00, 139.83it/s]\u001b[A\n",
      "\n",
      " 55%|                   | 28/51 [00:00<00:00, 135.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|| 50/51 [00:00<00:00, 164.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 160.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|         | 40/51 [00:00<00:00, 184.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 161.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 163.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 51/51 [00:00<00:00, 163.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 51/51 [00:00<00:00, 187.01it/s]\n",
      "100%|| 51/51 [00:00<00:00, 179.96it/s]\n",
      "100%|| 51/51 [00:00<00:00, 168.86it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_block.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_catalyse.tsv\n",
      "Processing Started...\n",
      "Data Size:  195\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                         | 11/27 [00:00<00:00, 107.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 48%|                     | 13/27 [00:00<00:00, 129.34it/s]\u001b[A\n",
      "\n",
      " 63%|               | 17/27 [00:00<00:00, 165.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 149.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|            | 19/27 [00:00<00:00, 158.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 142.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 149.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 27/27 [00:00<00:00, 163.95it/s]\n",
      "100%|| 27/27 [00:00<00:00, 141.82it/s]\n",
      "100%|| 27/27 [00:00<00:00, 150.04it/s]\n",
      "100%|| 27/27 [00:00<00:00, 158.16it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_catalyse.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_catalyse.tsv\n",
      "Processing Started...\n",
      "Data Size:  196\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 61%|                | 17/28 [00:00<00:00, 162.06it/s]\n",
      " 64%|               | 18/28 [00:00<00:00, 172.77it/s]\u001b[A\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 193.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 17/28 [00:00<00:00, 169.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 164.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|                  | 16/28 [00:00<00:00, 143.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 155.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 161.12it/s]\n",
      "100%|| 28/28 [00:00<00:00, 164.50it/s]\n",
      "100%|| 28/28 [00:00<00:00, 165.36it/s]\n",
      "100%|| 28/28 [00:00<00:00, 181.82it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_catalyse.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_confer.tsv\n",
      "Processing Started...\n",
      "Data Size:  317\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 42%|                        | 19/45 [00:00<00:00, 188.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 33%|                            | 15/45 [00:00<00:00, 147.30it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|                        | 19/45 [00:00<00:00, 188.71it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 49%|                     | 22/45 [00:00<00:00, 210.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 42/45 [00:00<00:00, 212.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                          | 17/45 [00:00<00:00, 168.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 45/45 [00:00<00:00, 199.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 45/45 [00:00<00:00, 192.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 38/45 [00:00<00:00, 175.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 37/45 [00:00<00:00, 182.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 45/45 [00:00<00:00, 193.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 45/45 [00:00<00:00, 176.72it/s]\n",
      "100%|| 45/45 [00:00<00:00, 186.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 37/45 [00:00<00:00, 177.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 45/45 [00:00<00:00, 184.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 45/45 [00:00<00:00, 170.10it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_confer.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_confer.tsv\n",
      "Processing Started...\n",
      "Data Size:  319\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 40%|                         | 18/45 [00:00<00:00, 175.84it/s]\n",
      " 42%|                        | 19/45 [00:00<00:00, 188.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|                          | 17/45 [00:00<00:00, 166.76it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 31%|                             | 14/45 [00:00<00:00, 135.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|                         | 18/45 [00:00<00:00, 177.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 37/45 [00:00<00:00, 182.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 84%|      | 38/45 [00:00<00:00, 187.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                             | 13/45 [00:00<00:00, 124.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 45/45 [00:00<00:00, 184.73it/s]\u001b[A\u001b[A\n",
      "100%|| 45/45 [00:00<00:00, 193.17it/s]\n",
      "\n",
      "\n",
      "\n",
      " 78%|         | 35/45 [00:00<00:00, 172.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|        | 36/45 [00:00<00:00, 178.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 45/45 [00:00<00:00, 162.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 45/45 [00:00<00:00, 166.19it/s]\n",
      "100%|| 45/45 [00:00<00:00, 170.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 45/45 [00:00<00:00, 164.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 45/45 [00:00<00:00, 152.82it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_confer.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_decrease.tsv\n",
      "Processing Started...\n",
      "Data Size:  195\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 67%|              | 18/27 [00:00<00:00, 174.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 59%|                 | 16/27 [00:00<00:00, 159.46it/s]\u001b[A\n",
      "\n",
      " 48%|                     | 13/27 [00:00<00:00, 122.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 156.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|               | 17/27 [00:00<00:00, 159.46it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 248.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 143.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 27/27 [00:00<00:00, 210.76it/s]\n",
      "100%|| 27/27 [00:00<00:00, 155.24it/s]\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 115.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 195.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_decrease.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_decrease.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 39%|                         | 11/28 [00:00<00:00, 105.88it/s]\n",
      " 57%|                  | 16/28 [00:00<00:00, 153.07it/s]\u001b[A\n",
      "\n",
      " 50%|                     | 14/28 [00:00<00:00, 133.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|                      | 13/28 [00:00<00:00, 114.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 234.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 161.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 230.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 133.86it/s]\n",
      "100%|| 28/28 [00:00<00:00, 209.90it/s]\n",
      "100%|| 28/28 [00:00<00:00, 144.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 114.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_decrease.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_delete.tsv\n",
      "Processing Started...\n",
      "Data Size:  411\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|                          | 22/58 [00:00<00:00, 218.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 45%|                       | 26/58 [00:00<00:00, 253.06it/s]\u001b[A\n",
      "\n",
      " 41%|                        | 24/58 [00:00<00:00, 237.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|                          | 22/58 [00:00<00:00, 209.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                          | 22/58 [00:00<00:00, 215.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|      | 49/58 [00:00<00:00, 245.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 58/58 [00:00<00:00, 245.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 49/58 [00:00<00:00, 232.36it/s]\u001b[A\u001b[A\n",
      " 90%|    | 52/58 [00:00<00:00, 216.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|| 58/58 [00:00<00:00, 224.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 58/58 [00:00<00:00, 240.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|          | 44/58 [00:00<00:00, 202.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 58/58 [00:00<00:00, 219.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 58/58 [00:00<00:00, 212.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 58/58 [00:00<00:00, 216.14it/s]\n",
      "100%|| 58/58 [00:00<00:00, 211.26it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_delete.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_delete.tsv\n",
      "Processing Started...\n",
      "Data Size:  413\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 39%|                         | 23/59 [00:00<00:00, 207.51it/s]\n",
      " 32%|                            | 19/59 [00:00<00:00, 184.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 20%|                                 | 12/59 [00:00<00:00, 117.76it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 83%|       | 49/59 [00:00<00:00, 234.30it/s]\u001b[A\u001b[A\u001b[A\n",
      " 83%|       | 49/59 [00:00<00:00, 249.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                            | 19/59 [00:00<00:00, 173.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|                      | 27/59 [00:00<00:00, 247.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|                          | 22/59 [00:00<00:00, 211.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|               | 37/59 [00:00<00:00, 189.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 59/59 [00:00<00:00, 231.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 59/59 [00:00<00:00, 213.21it/s]\n",
      "100%|| 59/59 [00:00<00:00, 230.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|               | 37/59 [00:00<00:00, 172.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|        | 47/59 [00:00<00:00, 232.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%| | 57/59 [00:00<00:00, 193.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 59/59 [00:00<00:00, 187.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 59/59 [00:00<00:00, 243.84it/s]\n",
      "100%|| 59/59 [00:00<00:00, 232.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 59/59 [00:00<00:00, 187.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_delete.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_develop.tsv\n",
      "Processing Started...\n",
      "Data Size:  62\n",
      "number of threads:  7\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "100%|| 8/8 [00:00<00:00, 241.20it/s]\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 8/8 [00:00<00:00, 198.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 8/8 [00:00<00:00, 206.08it/s]\n",
      "100%|| 8/8 [00:00<00:00, 180.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 8/8 [00:00<00:00, 181.97it/s]\n",
      "100%|| 8/8 [00:00<00:00, 161.53it/s]\n",
      "100%|| 8/8 [00:00<00:00, 183.21it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_develop.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_develop.tsv\n",
      "Processing Started...\n",
      "Data Size:  62\n",
      "number of threads:  7\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 8/8 [00:00<00:00, 191.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 8/8 [00:00<00:00, 176.94it/s]\n",
      "100%|| 8/8 [00:00<00:00, 166.84it/s]\n",
      "100%|| 8/8 [00:00<00:00, 176.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 8/8 [00:00<00:00, 83.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 8/8 [00:00<00:00, 102.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 8/8 [00:00<00:00, 77.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_develop.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_disrupt.tsv\n",
      "Processing Started...\n",
      "Data Size:  119\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 169.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 162.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 71%|            | 12/17 [00:00<00:00, 111.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 119.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 17/17 [00:00<00:00, 141.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 142.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|          | 13/17 [00:00<00:00, 121.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 131.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 17/17 [00:00<00:00, 118.46it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_disrupt.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_disrupt.tsv\n",
      "Processing Started...\n",
      "Data Size:  120\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|  | 16/17 [00:00<00:00, 156.70it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 17/17 [00:00<00:00, 151.81it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|| 17/17 [00:00<00:00, 170.40it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 173.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                       | 8/17 [00:00<00:00, 77.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 145.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 122.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 17/17 [00:00<00:00, 148.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 17/17 [00:00<00:00, 103.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_disrupt.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_eliminate.tsv\n",
      "Processing Started...\n",
      "Data Size:  191\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 78%|         | 21/27 [00:00<00:00, 203.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 27/27 [00:00<00:00, 214.06it/s]\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 222.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|      | 23/27 [00:00<00:00, 225.20it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 27/27 [00:00<00:00, 208.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 59%|                 | 16/27 [00:00<00:00, 155.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 26/27 [00:00<00:00, 256.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 27/27 [00:00<00:00, 260.68it/s]\n",
      "100%|| 27/27 [00:00<00:00, 168.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 200.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 203.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_eliminate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_eliminate.tsv\n",
      "Processing Started...\n",
      "Data Size:  192\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 70%|            | 19/27 [00:00<00:00, 180.17it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 63%|               | 17/27 [00:00<00:00, 168.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 27/27 [00:00<00:00, 179.44it/s]\n",
      "\n",
      "\n",
      " 59%|                 | 16/27 [00:00<00:00, 158.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 27/27 [00:00<00:00, 189.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 169.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 59%|                 | 16/27 [00:00<00:00, 138.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|                    | 14/27 [00:00<00:00, 139.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 155.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 27/27 [00:00<00:00, 174.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 27/27 [00:00<00:00, 155.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 27/27 [00:00<00:00, 213.37it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_eliminate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_encode.tsv\n",
      "Processing Started...\n",
      "Data Size:  196\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 68%|             | 19/28 [00:00<00:00, 186.78it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|       | 23/28 [00:00<00:00, 224.62it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 28/28 [00:00<00:00, 209.98it/s]\n",
      "100%|| 28/28 [00:00<00:00, 242.64it/s]\n",
      "\n",
      "\n",
      " 68%|             | 19/28 [00:00<00:00, 178.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 200.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 225.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 240.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 203.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 222.35it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_encode.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_encode.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 64%|               | 18/28 [00:00<00:00, 175.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|                | 17/28 [00:00<00:00, 168.93it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 28/28 [00:00<00:00, 181.44it/s]\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 193.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 183.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|               | 18/28 [00:00<00:00, 177.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|         | 22/28 [00:00<00:00, 218.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 167.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 183.77it/s]\n",
      "100%|| 28/28 [00:00<00:00, 158.07it/s]\n",
      "100%|| 28/28 [00:00<00:00, 147.64it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_encode.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_express.tsv\n",
      "Processing Started...\n",
      "Data Size:  395\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 41%|                        | 23/56 [00:00<00:00, 219.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 25%|                               | 14/56 [00:00<00:00, 138.22it/s]\u001b[A\n",
      "\n",
      " 25%|                               | 14/56 [00:00<00:00, 129.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                     | 28/56 [00:00<00:00, 277.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|                        | 23/56 [00:00<00:00, 220.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 51/56 [00:00<00:00, 251.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                           | 20/56 [00:00<00:00, 197.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 56/56 [00:00<00:00, 245.85it/s]\u001b[A\n",
      "\n",
      "\n",
      " 64%|               | 36/56 [00:00<00:00, 173.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|           | 41/56 [00:00<00:00, 204.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 34/56 [00:00<00:00, 161.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|       | 46/56 [00:00<00:00, 190.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 56/56 [00:00<00:00, 222.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 56/56 [00:00<00:00, 166.43it/s]\u001b[A\n",
      "100%|| 56/56 [00:00<00:00, 178.54it/s]\n",
      "100%|| 56/56 [00:00<00:00, 189.75it/s]\n",
      "100%|| 56/56 [00:00<00:00, 202.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 56/56 [00:00<00:00, 177.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_express.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_express.tsv\n",
      "Processing Started...\n",
      "Data Size:  397\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 36%|                           | 20/56 [00:00<00:00, 198.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|                              | 15/56 [00:00<00:00, 149.05it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|                        | 24/56 [00:00<00:00, 239.25it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 32%|                            | 18/56 [00:00<00:00, 174.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|        | 45/56 [00:00<00:00, 227.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|                | 34/56 [00:00<00:00, 171.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 56/56 [00:00<00:00, 229.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                            | 18/56 [00:00<00:00, 174.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|            | 40/56 [00:00<00:00, 194.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|          | 42/56 [00:00<00:00, 213.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|      | 48/56 [00:00<00:00, 179.65it/s]\u001b[A\u001b[A\n",
      " 93%|   | 52/56 [00:00<00:00, 165.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 56/56 [00:00<00:00, 213.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 56/56 [00:00<00:00, 188.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 56/56 [00:00<00:00, 167.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 56/56 [00:00<00:00, 182.72it/s]\n",
      "100%|| 56/56 [00:00<00:00, 190.54it/s]\n",
      "100%|| 56/56 [00:00<00:00, 198.55it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_express.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_generate.tsv\n",
      "Processing Started...\n",
      "Data Size:  370\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 25%|                               | 13/52 [00:00<00:00, 129.01it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 25%|                               | 13/52 [00:00<00:00, 127.43it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|                               | 13/52 [00:00<00:00, 127.90it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 31%|                             | 16/52 [00:00<00:00, 159.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|                 | 31/52 [00:00<00:00, 157.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                          | 19/52 [00:00<00:00, 188.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|             | 35/52 [00:00<00:00, 177.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                           | 18/52 [00:00<00:00, 178.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|                 | 31/52 [00:00<00:00, 156.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%| | 50/52 [00:00<00:00, 163.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 52/52 [00:00<00:00, 162.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 52/52 [00:00<00:00, 167.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|         | 41/52 [00:00<00:00, 208.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 52/52 [00:00<00:00, 177.69it/s]\u001b[A\u001b[A\n",
      "100%|| 52/52 [00:00<00:00, 158.33it/s]\n",
      "100%|| 52/52 [00:00<00:00, 176.18it/s]\n",
      "100%|| 52/52 [00:00<00:00, 182.84it/s]\n",
      "100%|| 52/52 [00:00<00:00, 197.78it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_generate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_generate.tsv\n",
      "Processing Started...\n",
      "Data Size:  371\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 36%|                           | 19/53 [00:00<00:00, 186.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 38%|                          | 20/53 [00:00<00:00, 197.17it/s]\u001b[A\n",
      "\n",
      " 25%|                               | 13/53 [00:00<00:00, 123.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|                           | 19/53 [00:00<00:00, 175.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|          | 40/53 [00:00<00:00, 199.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|                              | 15/53 [00:00<00:00, 147.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                       | 24/53 [00:00<00:00, 211.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|                   | 28/53 [00:00<00:00, 135.93it/s]\u001b[A\u001b[A\n",
      " 75%|          | 40/53 [00:00<00:00, 159.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 179.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|   | 49/53 [00:00<00:00, 219.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 219.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 226.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 53/53 [00:00<00:00, 193.59it/s]\n",
      "100%|| 53/53 [00:00<00:00, 165.61it/s]\n",
      "100%|| 53/53 [00:00<00:00, 210.97it/s]\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 133.47it/s]\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_generate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_inhibit.tsv\n",
      "Processing Started...\n",
      "Data Size:  339\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 58%|                 | 28/48 [00:00<00:00, 277.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 46%|                      | 22/48 [00:00<00:00, 202.96it/s]\u001b[A\n",
      "\n",
      " 48%|                     | 23/48 [00:00<00:00, 228.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|                           | 17/48 [00:00<00:00, 168.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 276.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 241.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                         | 19/48 [00:00<00:00, 172.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%| | 46/48 [00:00<00:00, 206.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 210.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|               | 30/48 [00:00<00:00, 141.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 164.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|         | 37/48 [00:00<00:00, 163.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 144.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 48/48 [00:00<00:00, 136.43it/s]\n",
      "100%|| 48/48 [00:00<00:00, 160.77it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_inhibit.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_inhibit.tsv\n",
      "Processing Started...\n",
      "Data Size:  341\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|                              | 13/48 [00:00<00:00, 111.90it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 31%|                            | 15/48 [00:00<00:00, 123.95it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 23%|                                 | 11/48 [00:00<00:00, 98.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|                                 | 11/48 [00:00<00:00, 87.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                 | 28/48 [00:00<00:00, 129.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|         | 37/48 [00:00<00:00, 174.86it/s]\u001b[A\n",
      "\n",
      "\n",
      " 85%|      | 41/48 [00:00<00:00, 208.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|          | 36/48 [00:00<00:00, 170.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|                       | 21/48 [00:00<00:00, 198.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 178.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 189.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 48/48 [00:00<00:00, 152.94it/s]\n",
      "100%|| 48/48 [00:00<00:00, 164.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|     | 42/48 [00:00<00:00, 201.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 48/48 [00:00<00:00, 165.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 48/48 [00:00<00:00, 213.94it/s]\n",
      "100%|| 48/48 [00:00<00:00, 215.29it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_inhibit.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_initiate.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 86%|      | 24/28 [00:00<00:00, 238.87it/s]\n",
      "100%|| 28/28 [00:00<00:00, 245.34it/s]\u001b[A\n",
      "\n",
      "\n",
      " 86%|      | 24/28 [00:00<00:00, 238.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 241.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 218.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 221.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 220.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 222.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 198.93it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_initiate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_initiate.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|                   | 15/28 [00:00<00:00, 149.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 50%|                     | 14/28 [00:00<00:00, 137.95it/s]\u001b[A\n",
      "\n",
      " 64%|               | 18/28 [00:00<00:00, 175.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|                | 17/28 [00:00<00:00, 167.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 160.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 186.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 164.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 195.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 28/28 [00:00<00:00, 159.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 28/28 [00:00<00:00, 180.47it/s]\n",
      "100%|| 28/28 [00:00<00:00, 155.44it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_initiate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_lead.tsv\n",
      "Processing Started...\n",
      "Data Size:  492\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 34%|                           | 24/70 [00:00<00:00, 238.22it/s]\n",
      " 27%|                              | 19/70 [00:00<00:00, 187.14it/s]\u001b[A\n",
      "\n",
      " 31%|                            | 22/70 [00:00<00:00, 219.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|                              | 19/70 [00:00<00:00, 184.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                          | 26/70 [00:00<00:00, 257.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|                            | 22/70 [00:00<00:00, 208.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|             | 48/70 [00:00<00:00, 151.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|                   | 38/70 [00:00<00:00, 124.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 43/70 [00:00<00:00, 164.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|                   | 38/70 [00:00<00:00, 136.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|               | 44/70 [00:00<00:00, 141.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|             | 47/70 [00:00<00:00, 187.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 208.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|| 69/70 [00:00<00:00, 169.29it/s]\n",
      "100%|| 70/70 [00:00<00:00, 170.98it/s]\u001b[A\n",
      "100%|| 70/70 [00:00<00:00, 190.86it/s]\n",
      "100%|| 70/70 [00:00<00:00, 212.46it/s]\n",
      "\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 169.17it/s]\u001b[A\u001b[A\n",
      "100%|| 70/70 [00:00<00:00, 171.78it/s]\n",
      "100%|| 70/70 [00:00<00:00, 150.87it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_lead.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_lead.tsv\n",
      "Processing Started...\n",
      "Data Size:  493\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 31%|                            | 22/70 [00:00<00:00, 210.67it/s]\n",
      " 24%|                               | 17/70 [00:00<00:00, 162.60it/s]\u001b[A\n",
      "\n",
      " 29%|                              | 20/70 [00:00<00:00, 188.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|                             | 21/70 [00:00<00:00, 194.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|                            | 22/70 [00:00<00:00, 219.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                            | 22/70 [00:00<00:00, 214.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|               | 44/70 [00:00<00:00, 215.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|                   | 37/70 [00:00<00:00, 183.37it/s]\u001b[A\n",
      "\n",
      " 63%|               | 44/70 [00:00<00:00, 211.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|             | 47/70 [00:00<00:00, 229.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|           | 51/70 [00:00<00:00, 255.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|             | 48/70 [00:00<00:00, 238.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 220.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 252.07it/s]\u001b[A\n",
      "\n",
      "\n",
      " 94%|  | 66/70 [00:00<00:00, 203.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 220.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 70/70 [00:00<00:00, 241.37it/s]\n",
      "100%|| 70/70 [00:00<00:00, 204.41it/s]\n",
      "100%|| 70/70 [00:00<00:00, 236.51it/s]\n",
      "100%|| 70/70 [00:00<00:00, 187.97it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_lead.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_lose.tsv\n",
      "Processing Started...\n",
      "Data Size:  286\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                      | 19/40 [00:00<00:00, 188.72it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 48%|                      | 19/40 [00:00<00:00, 181.19it/s]\u001b[A\n",
      "\n",
      " 52%|                    | 21/40 [00:00<00:00, 206.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|                   | 22/40 [00:00<00:00, 213.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|                       | 18/40 [00:00<00:00, 178.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 220.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 200.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 40/40 [00:00<00:00, 209.37it/s]\n",
      "100%|| 40/40 [00:00<00:00, 233.64it/s]\n",
      "100%|| 40/40 [00:00<00:00, 181.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 40/40 [00:00<00:00, 160.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 40/40 [00:00<00:00, 175.39it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_lose.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_lose.tsv\n",
      "Processing Started...\n",
      "Data Size:  287\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                         | 16/41 [00:00<00:00, 156.47it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 39%|                         | 16/41 [00:00<00:00, 151.64it/s]\u001b[A\n",
      "\n",
      " 46%|                      | 19/41 [00:00<00:00, 184.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|                            | 13/41 [00:00<00:00, 129.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 203.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 41/41 [00:00<00:00, 217.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 217.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 239.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 229.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 41/41 [00:00<00:00, 158.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 153.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_lose.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_modify.tsv\n",
      "Processing Started...\n",
      "Data Size:  166\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 70%|            | 16/23 [00:00<00:00, 151.74it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 70%|            | 16/23 [00:00<00:00, 157.05it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 23/23 [00:00<00:00, 162.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 178.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 70%|            | 16/23 [00:00<00:00, 152.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|     | 20/23 [00:00<00:00, 189.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 162.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 188.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 174.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 153.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 138.63it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_modify.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_modify.tsv\n",
      "Processing Started...\n",
      "Data Size:  167\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 23/23 [00:00<00:00, 213.08it/s]\n",
      "\n",
      " 87%|     | 20/23 [00:00<00:00, 198.35it/s]\u001b[A\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 200.86it/s]\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 189.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 21/23 [00:00<00:00, 189.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 185.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 186.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 185.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 147.51it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_modify.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_mutate.tsv\n",
      "Processing Started...\n",
      "Data Size:  244\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 62%|                | 21/34 [00:00<00:00, 201.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 29%|                              | 10/34 [00:00<00:00, 99.74it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 53%|                   | 18/34 [00:00<00:00, 170.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 34/34 [00:00<00:00, 184.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                              | 10/34 [00:00<00:00, 86.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                                   | 7/34 [00:00<00:00, 69.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|                 | 20/34 [00:00<00:00, 96.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 34/34 [00:00<00:00, 145.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|                | 21/34 [00:00<00:00, 98.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|            | 24/34 [00:00<00:00, 99.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|                      | 16/34 [00:00<00:00, 78.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|     | 30/34 [00:00<00:00, 96.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 34/34 [00:00<00:00, 95.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 34/34 [00:00<00:00, 106.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 34/34 [00:00<00:00, 100.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|            | 24/34 [00:00<00:00, 71.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 34/34 [00:00<00:00, 86.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 34/34 [00:00<00:00, 83.48it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_mutate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_mutate.tsv\n",
      "Processing Started...\n",
      "Data Size:  245\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 46%|                      | 16/35 [00:00<00:00, 144.17it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 54%|                   | 19/35 [00:00<00:00, 188.23it/s]\u001b[A\n",
      "\n",
      " 20%|                                   | 7/35 [00:00<00:00, 59.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|                                | 9/35 [00:00<00:00, 86.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 32/35 [00:00<00:00, 151.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|                              | 10/35 [00:00<00:00, 82.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|                            | 11/35 [00:00<00:00, 100.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|| 35/35 [00:00<00:00, 191.48it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 35/35 [00:00<00:00, 145.09it/s]\n",
      "\n",
      "\n",
      " 37%|                           | 13/35 [00:00<00:00, 59.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|                  | 20/35 [00:00<00:00, 97.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                           | 12/35 [00:00<00:00, 118.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|               | 22/35 [00:00<00:00, 102.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|             | 24/35 [00:00<00:00, 116.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 35/35 [00:00<00:00, 109.06it/s]\u001b[A\u001b[A\n",
      "100%|| 35/35 [00:00<00:00, 123.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|| 35/35 [00:00<00:00, 111.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 35/35 [00:00<00:00, 113.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 35/35 [00:00<00:00, 135.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_mutate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_proliferate.tsv\n",
      "Processing Started...\n",
      "Data Size:  162\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|         | 18/23 [00:00<00:00, 178.54it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 96%| | 22/23 [00:00<00:00, 196.83it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 189.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 162.63it/s]\n",
      "\n",
      "\n",
      " 78%|         | 18/23 [00:00<00:00, 171.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|           | 17/23 [00:00<00:00, 166.84it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                      | 11/23 [00:00<00:00, 96.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 142.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 136.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 129.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 112.08it/s]\n",
      "100%|| 23/23 [00:00<00:00, 116.06it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_proliferate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_proliferate.tsv\n",
      "Processing Started...\n",
      "Data Size:  163\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 70%|            | 16/23 [00:00<00:00, 152.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 74%|           | 17/23 [00:00<00:00, 169.87it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 23/23 [00:00<00:00, 150.73it/s]\n",
      "100%|| 23/23 [00:00<00:00, 164.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 74%|           | 17/23 [00:00<00:00, 165.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|                | 14/23 [00:00<00:00, 129.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|              | 15/23 [00:00<00:00, 140.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 122.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 115.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 133.53it/s]\n",
      "100%|| 23/23 [00:00<00:00, 127.95it/s]\n",
      "100%|| 23/23 [00:00<00:00, 132.26it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_proliferate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_recognize.tsv\n",
      "Processing Started...\n",
      "Data Size:  291\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 34%|                           | 14/41 [00:00<00:00, 138.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 34%|                           | 14/41 [00:00<00:00, 137.57it/s]\u001b[A\u001b[A\n",
      " 27%|                               | 11/41 [00:00<00:00, 99.97it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|                          | 15/41 [00:00<00:00, 149.32it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                                | 10/41 [00:00<00:00, 99.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|             | 28/41 [00:00<00:00, 130.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|            | 29/41 [00:00<00:00, 144.07it/s]\u001b[A\n",
      "\n",
      " 78%|         | 32/41 [00:00<00:00, 160.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                         | 16/41 [00:00<00:00, 159.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 185.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 41/41 [00:00<00:00, 154.99it/s]\n",
      "100%|| 41/41 [00:00<00:00, 142.82it/s]\n",
      "100%|| 41/41 [00:00<00:00, 136.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|          | 31/41 [00:00<00:00, 162.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                    | 22/41 [00:00<00:00, 97.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 169.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 41/41 [00:00<00:00, 154.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 124.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_recognize.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_recognize.tsv\n",
      "Processing Started...\n",
      "Data Size:  293\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 24%|                                | 10/41 [00:00<00:00, 95.09it/s]\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      " 32%|                            | 13/41 [00:00<00:00, 126.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 83%|       | 34/41 [00:00<00:00, 172.89it/s]\n",
      "\n",
      " 46%|                      | 19/41 [00:00<00:00, 188.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|                | 25/41 [00:00<00:00, 248.91it/s]\u001b[A\u001b[A\u001b[A\n",
      " 90%|    | 37/41 [00:00<00:00, 191.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|                   | 22/41 [00:00<00:00, 215.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 163.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 41/41 [00:00<00:00, 184.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 224.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 220.06it/s]\u001b[A\u001b[A\n",
      "100%|| 41/41 [00:00<00:00, 209.11it/s]\n",
      "100%|| 41/41 [00:00<00:00, 185.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 41/41 [00:00<00:00, 185.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_recognize.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_result.tsv\n",
      "Processing Started...\n",
      "Data Size:  496\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 24%|                               | 17/70 [00:00<00:00, 160.87it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 20%|                                 | 14/70 [00:00<00:00, 133.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 21%|                                 | 15/70 [00:00<00:00, 144.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|                     | 34/70 [00:00<00:00, 149.93it/s]\u001b[A\u001b[A\n",
      " 44%|                       | 31/70 [00:00<00:00, 152.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                  | 12/70 [00:00<00:00, 117.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                                | 16/70 [00:00<00:00, 159.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|           | 51/70 [00:00<00:00, 155.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                        | 30/70 [00:00<00:00, 129.77it/s]\u001b[A\u001b[A\u001b[A\n",
      " 67%|             | 47/70 [00:00<00:00, 148.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|                          | 26/70 [00:00<00:00, 128.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|                      | 32/70 [00:00<00:00, 122.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|                   | 37/70 [00:00<00:00, 188.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 67/70 [00:00<00:00, 156.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 156.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|    | 63/70 [00:00<00:00, 152.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 43/70 [00:00<00:00, 146.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|             | 48/70 [00:00<00:00, 136.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|            | 49/70 [00:00<00:00, 160.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 154.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 70/70 [00:00<00:00, 186.82it/s]\n",
      "\n",
      "\n",
      "\n",
      " 96%| | 67/70 [00:00<00:00, 161.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|   | 64/70 [00:00<00:00, 169.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 70/70 [00:00<00:00, 174.14it/s]\u001b[A\u001b[A\n",
      "100%|| 70/70 [00:00<00:00, 152.15it/s]\n",
      "100%|| 70/70 [00:00<00:00, 161.39it/s]\n",
      "100%|| 70/70 [00:00<00:00, 146.28it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_result.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_result.tsv\n",
      "Processing Started...\n",
      "Data Size:  497\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 15%|                                    | 11/71 [00:00<00:00, 85.03it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 13%|                                      | 9/71 [00:00<00:00, 87.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|                                       | 8/71 [00:00<00:00, 75.88it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                              | 21/71 [00:00<00:00, 91.37it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|                                        | 6/71 [00:00<00:01, 57.23it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 27%|                               | 19/71 [00:00<00:00, 88.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                      | 9/71 [00:00<00:00, 83.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                         | 5/71 [00:00<00:01, 48.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                       | 8/71 [00:00<00:00, 77.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|                                   | 13/71 [00:00<00:00, 63.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|                        | 31/71 [00:00<00:00, 85.86it/s]\u001b[A\u001b[A\n",
      " 39%|                          | 28/71 [00:00<00:00, 85.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|                               | 20/71 [00:00<00:00, 96.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                                | 17/71 [00:00<00:00, 89.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|                             | 23/71 [00:00<00:00, 78.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|                             | 23/71 [00:00<00:00, 59.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|                  | 40/71 [00:00<00:00, 81.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|                    | 37/71 [00:00<00:00, 79.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|                           | 26/71 [00:00<00:00, 89.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|                        | 30/71 [00:00<00:00, 78.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|                   | 38/71 [00:00<00:00, 104.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|                   | 39/71 [00:00<00:00, 93.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|          | 54/71 [00:00<00:00, 99.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|            | 50/71 [00:00<00:00, 95.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|                 | 42/71 [00:00<00:00, 116.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|         | 55/71 [00:00<00:00, 126.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|                  | 41/71 [00:00<00:00, 86.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|           | 51/71 [00:00<00:00, 101.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 71/71 [00:00<00:00, 101.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|   | 66/71 [00:00<00:00, 115.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 71/71 [00:00<00:00, 119.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 71/71 [00:00<00:00, 102.29it/s]\n",
      "100%|| 71/71 [00:00<00:00, 138.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 71/71 [00:00<00:00, 130.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 71/71 [00:00<00:00, 101.22it/s]\u001b[A\u001b[A\n",
      "100%|| 71/71 [00:00<00:00, 116.74it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_result.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_skip.tsv\n",
      "Processing Started...\n",
      "Data Size:  79\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 111.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 82%|        | 9/11 [00:00<00:00, 78.85it/s]\u001b[A\u001b[A\n",
      "100%|| 11/11 [00:00<00:00, 66.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 72.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 65.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 11/11 [00:00<00:00, 96.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 11/11 [00:00<00:00, 126.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 107.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_skip.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_skip.tsv\n",
      "Processing Started...\n",
      "Data Size:  80\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 164.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|| 11/11 [00:00<00:00, 170.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 177.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|| 11/11 [00:00<00:00, 166.20it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 11/11 [00:00<00:00, 134.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 11/11 [00:00<00:00, 154.99it/s]\n",
      "100%|| 11/11 [00:00<00:00, 88.52it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_skip.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_splice.tsv\n",
      "Processing Started...\n",
      "Data Size:  390\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 27%|                              | 15/55 [00:00<00:00, 148.56it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 20%|                                  | 11/55 [00:00<00:00, 83.85it/s]\u001b[A\n",
      "\n",
      " 20%|                                 | 11/55 [00:00<00:00, 104.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 60%|                | 33/55 [00:00<00:00, 164.99it/s]\n",
      "\n",
      "\n",
      " 25%|                               | 14/55 [00:00<00:00, 130.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|                          | 21/55 [00:00<00:00, 202.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|                               | 14/55 [00:00<00:00, 131.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|                   | 29/55 [00:00<00:00, 147.50it/s]\u001b[A\u001b[A\n",
      " 44%|                       | 24/55 [00:00<00:00, 102.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|                             | 16/55 [00:00<00:00, 156.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|   | 50/55 [00:00<00:00, 161.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 55/55 [00:00<00:00, 160.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|          | 42/55 [00:00<00:00, 183.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|               | 35/55 [00:00<00:00, 173.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|          | 41/55 [00:00<00:00, 128.06it/s]\u001b[A\n",
      "\n",
      " 82%|       | 45/55 [00:00<00:00, 146.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 55/55 [00:00<00:00, 170.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 55/55 [00:00<00:00, 184.64it/s]\n",
      "100%|| 55/55 [00:00<00:00, 170.72it/s]\n",
      "100%|| 55/55 [00:00<00:00, 140.51it/s]\n",
      "100%|| 55/55 [00:00<00:00, 127.04it/s]\n",
      "100%|| 55/55 [00:00<00:00, 184.53it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_splice.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_splice.tsv\n",
      "Processing Started...\n",
      "Data Size:  390\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 29%|                             | 16/55 [00:00<00:00, 159.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 33%|                            | 18/55 [00:00<00:00, 172.38it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 25%|                               | 14/55 [00:00<00:00, 131.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|           | 40/55 [00:00<00:00, 205.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|                           | 19/55 [00:00<00:00, 186.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|                           | 19/55 [00:00<00:00, 188.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|                            | 18/55 [00:00<00:00, 178.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|            | 39/55 [00:00<00:00, 192.72it/s]\u001b[A\n",
      "\n",
      "100%|| 55/55 [00:00<00:00, 201.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|        | 44/55 [00:00<00:00, 204.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 55/55 [00:00<00:00, 192.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|             | 38/55 [00:00<00:00, 188.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 55/55 [00:00<00:00, 204.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 55/55 [00:00<00:00, 198.35it/s]\u001b[A\u001b[A\n",
      "100%|| 55/55 [00:00<00:00, 157.85it/s]\n",
      "100%|| 55/55 [00:00<00:00, 187.52it/s]\n",
      "100%|| 55/55 [00:00<00:00, 181.78it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_splice.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_transcribe.tsv\n",
      "Processing Started...\n",
      "Data Size:  799\n",
      "number of threads:  7\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 18%|                                 | 21/114 [00:00<00:00, 185.22it/s]\n",
      " 15%|                                   | 17/114 [00:00<00:00, 161.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|                                   | 16/114 [00:00<00:00, 148.88it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                  | 18/114 [00:00<00:00, 171.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                        | 6/114 [00:00<00:01, 56.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                        | 45/114 [00:00<00:00, 214.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|                          | 42/114 [00:00<00:00, 197.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|                                  | 19/114 [00:00<00:00, 187.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|                          | 42/114 [00:00<00:00, 206.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|                          | 41/114 [00:00<00:00, 205.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                                 | 22/114 [00:00<00:00, 113.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|                         | 44/114 [00:00<00:00, 215.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|                | 69/114 [00:00<00:00, 225.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                        | 46/114 [00:00<00:00, 235.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|               | 70/114 [00:00<00:00, 232.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|                 | 67/114 [00:00<00:00, 170.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|                          | 42/114 [00:00<00:00, 151.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|                 | 66/114 [00:00<00:00, 185.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|        | 92/114 [00:00<00:00, 208.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|                  | 64/114 [00:00<00:00, 177.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|       | 94/114 [00:00<00:00, 220.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 87/114 [00:00<00:00, 174.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|       | 94/114 [00:00<00:00, 204.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 208.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 114/114 [00:00<00:00, 213.86it/s]\n",
      "100%|| 114/114 [00:00<00:00, 215.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|  | 106/114 [00:00<00:00, 174.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 181.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 205.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|    | 101/114 [00:00<00:00, 174.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 195.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 114/114 [00:00<00:00, 161.77it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_transcribe.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_transcribe.tsv\n",
      "Processing Started...\n",
      "Data Size:  801\n",
      "number of threads:  7\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 25%|                               | 28/114 [00:00<00:00, 275.84it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 23%|                               | 26/114 [00:00<00:00, 253.95it/s]\u001b[A\n",
      "\n",
      " 20%|                                | 23/114 [00:00<00:00, 220.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|                                | 25/114 [00:00<00:00, 230.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|                                 | 22/114 [00:00<00:00, 210.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|                               | 26/114 [00:00<00:00, 254.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                    | 56/114 [00:00<00:00, 202.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                       | 49/114 [00:00<00:00, 212.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|                        | 46/114 [00:00<00:00, 183.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|                         | 44/114 [00:00<00:00, 200.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|                      | 52/114 [00:00<00:00, 189.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|                        | 46/114 [00:00<00:00, 229.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|             | 78/114 [00:00<00:00, 208.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|              | 73/114 [00:00<00:00, 224.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|                 | 66/114 [00:00<00:00, 203.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|               | 72/114 [00:00<00:00, 186.31it/s]\u001b[A\n",
      "\n",
      " 57%|                 | 65/114 [00:00<00:00, 154.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 69/114 [00:00<00:00, 198.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|     | 100/114 [00:00<00:00, 194.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|        | 92/114 [00:00<00:00, 187.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|         | 87/114 [00:00<00:00, 190.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 96/114 [00:00<00:00, 195.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|         | 89/114 [00:00<00:00, 179.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|        | 90/114 [00:00<00:00, 193.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 207.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 114/114 [00:00<00:00, 201.18it/s]\n",
      "100%|| 114/114 [00:00<00:00, 210.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 214.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 114/114 [00:00<00:00, 192.06it/s]\n",
      "100%|| 114/114 [00:00<00:00, 198.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 114/114 [00:00<00:00, 206.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_transcribe.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_transform.tsv\n",
      "Processing Started...\n",
      "Data Size:  375\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 38%|                          | 20/53 [00:00<00:00, 198.38it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 53%|                   | 28/53 [00:00<00:00, 267.53it/s]\u001b[A\n",
      "\n",
      " 49%|                     | 26/53 [00:00<00:00, 249.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|                     | 26/53 [00:00<00:00, 258.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|                       | 23/53 [00:00<00:00, 226.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|                           | 19/53 [00:00<00:00, 188.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 275.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 53/53 [00:00<00:00, 242.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 46/53 [00:00<00:00, 219.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%| | 51/53 [00:00<00:00, 203.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|            | 38/53 [00:00<00:00, 187.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 224.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 53/53 [00:00<00:00, 206.29it/s]\n",
      "100%|| 53/53 [00:00<00:00, 230.75it/s]\n",
      "100%|| 53/53 [00:00<00:00, 251.80it/s]\n",
      "100%|| 53/53 [00:00<00:00, 204.82it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_transform.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_transform.tsv\n",
      "Processing Started...\n",
      "Data Size:  377\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 25%|                               | 13/53 [00:00<00:00, 128.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 32%|                            | 17/53 [00:00<00:00, 167.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 40%|                         | 21/53 [00:00<00:00, 205.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                       | 23/53 [00:00<00:00, 224.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|             | 36/53 [00:00<00:00, 187.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|                       | 24/53 [00:00<00:00, 229.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|          | 40/53 [00:00<00:00, 191.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 197.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|        | 43/53 [00:00<00:00, 207.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|     | 46/53 [00:00<00:00, 226.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 184.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 218.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 53/53 [00:00<00:00, 246.33it/s]\n",
      "100%|| 53/53 [00:00<00:00, 201.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 53/53 [00:00<00:00, 154.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 53/53 [00:00<00:00, 130.48it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_transform.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_translate.tsv\n",
      "Processing Started...\n",
      "Data Size:  678\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 14%|                                    | 13/96 [00:00<00:00, 129.28it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 11%|                                     | 11/96 [00:00<00:00, 104.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|                                      | 11/96 [00:00<00:00, 99.12it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                   | 14/96 [00:00<00:00, 133.76it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 31%|                            | 30/96 [00:00<00:00, 150.20it/s]\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                      | 11/96 [00:00<00:00, 91.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|                               | 25/96 [00:00<00:00, 124.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                    | 12/96 [00:00<00:00, 116.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|                             | 28/96 [00:00<00:00, 129.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                    | 13/96 [00:00<00:00, 122.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|                            | 32/96 [00:00<00:00, 159.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|                      | 45/96 [00:00<00:00, 143.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|                     | 48/96 [00:00<00:00, 169.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|                            | 31/96 [00:00<00:00, 156.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|                   | 51/96 [00:00<00:00, 170.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|                           | 33/96 [00:00<00:00, 160.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|              | 62/96 [00:00<00:00, 151.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|                      | 45/96 [00:00<00:00, 147.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|             | 65/96 [00:00<00:00, 161.46it/s]\u001b[A\n",
      "\n",
      " 72%|           | 69/96 [00:00<00:00, 168.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|     | 83/96 [00:00<00:00, 170.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|                     | 47/96 [00:00<00:00, 121.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|        | 77/96 [00:00<00:00, 161.01it/s]\u001b[A\u001b[A\u001b[A\n",
      " 85%|      | 82/96 [00:00<00:00, 151.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|               | 60/96 [00:00<00:00, 123.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 153.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 158.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 162.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|| 96/96 [00:00<00:00, 152.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|        | 77/96 [00:00<00:00, 136.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|    | 87/96 [00:00<00:00, 170.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 168.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 96/96 [00:00<00:00, 147.78it/s]\n",
      "100%|| 96/96 [00:00<00:00, 158.17it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_translate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_translate.tsv\n",
      "Processing Started...\n",
      "Data Size:  678\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|                                | 23/96 [00:00<00:00, 222.25it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 22%|                                | 21/96 [00:00<00:00, 209.01it/s]\u001b[A\n",
      "\n",
      "\n",
      " 25%|                               | 24/96 [00:00<00:00, 239.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|                                | 21/96 [00:00<00:00, 185.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|                                 | 19/96 [00:00<00:00, 185.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|                                 | 20/96 [00:00<00:00, 188.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|                     | 46/96 [00:00<00:00, 167.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|                       | 42/96 [00:00<00:00, 155.88it/s]\u001b[A\n",
      "\n",
      " 42%|                        | 40/96 [00:00<00:00, 152.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|                         | 38/96 [00:00<00:00, 154.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|                     | 48/96 [00:00<00:00, 179.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|                         | 39/96 [00:00<00:00, 163.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|          | 73/96 [00:00<00:00, 204.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|        | 77/96 [00:00<00:00, 229.93it/s]\u001b[A\n",
      "\n",
      " 73%|           | 70/96 [00:00<00:00, 207.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|             | 66/96 [00:00<00:00, 201.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|      | 81/96 [00:00<00:00, 235.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 216.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 204.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 96/96 [00:00<00:00, 220.64it/s]\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 199.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|    | 87/96 [00:00<00:00, 192.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 193.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 96/96 [00:00<00:00, 205.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 96/96 [00:00<00:00, 198.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_translate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_truncate.tsv\n",
      "Processing Started...\n",
      "Data Size:  161\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 39%|                          | 9/23 [00:00<00:00, 88.72it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 39%|                          | 9/23 [00:00<00:00, 84.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 43%|                        | 10/23 [00:00<00:00, 96.39it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|         | 18/23 [00:00<00:00, 72.02it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 26%|                                | 6/23 [00:00<00:00, 48.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|                                | 6/23 [00:00<00:00, 54.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 82.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 75.51it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 87%|     | 20/23 [00:00<00:00, 83.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 82.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 14/23 [00:00<00:00, 62.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|                              | 7/23 [00:00<00:00, 65.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|                | 14/23 [00:00<00:00, 61.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 75.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|               | 15/23 [00:00<00:00, 72.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 63.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 67.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 72.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_truncate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_truncate.tsv\n",
      "Processing Started...\n",
      "Data Size:  161\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 39%|                          | 9/23 [00:00<00:00, 80.79it/s]\n",
      " 39%|                          | 9/23 [00:00<00:00, 86.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 39%|                          | 9/23 [00:00<00:00, 82.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|                        | 10/23 [00:00<00:00, 87.39it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|       | 19/23 [00:00<00:00, 85.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 105.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|                                | 6/23 [00:00<00:00, 59.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 90.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 98.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 94.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|     | 20/23 [00:00<00:00, 91.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|| 23/23 [00:00<00:00, 87.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|| 23/23 [00:00<00:00, 118.15it/s]\n",
      "100%|| 23/23 [00:00<00:00, 85.19it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_truncate.json\n"
     ]
    }
   ],
   "source": [
    "!python ../data_preparation.py \\\n",
    "    --task_file 'tasks_file_SRL.yml' \\\n",
    "    --data_dir 'content/data/' \\\n",
    "    --max_seq_len 50\n",
    "    # --data_dir '../../data' \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -3 Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Downloading model.safetensors: 100%|| 440M/440M [00:20<00:00, 21.8MB/s]\n",
      "loading weights file model.safetensors from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/a265f773a47193eed794233aa2a0f0bb6d3eaa63/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch: 0:   0%|                              | 1/1305 [00:22<8:19:30, 22.98s/it]"
     ]
    }
   ],
   "source": [
    "!python ../train.py \\\n",
    "    --data_dir 'content/data/bert-base-uncased_prepared_data' \\\n",
    "    --task_file 'tasks_file_SRL.yml' \\\n",
    "    --out_dir 'conll_ner_pos_bert_base' \\\n",
    "    --epochs 10 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 32 \\\n",
    "    --grad_accumulation_steps 1 \\\n",
    "    --log_per_updates 50 \\\n",
    "    --max_seq_len 50 \\\n",
    "    --eval_while_train \\\n",
    "    --test_while_train \\\n",
    "    --silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 4 Infering\n",
    "\n",
    "You can import and use the ``inferPipeline`` to get predictions for the required tasks.\n",
    "The trained model and maximum sequence length to be used needs to be specified.\n",
    "\n",
    "For knowing more details about infering, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/infering.html\">infer pipeline</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from infer_pipeline import inferPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
