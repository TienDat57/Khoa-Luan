{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE - 5\n",
    "\n",
    "**Tasks :- NER tagging, POS tagging**\n",
    "\n",
    "**Tasks Description**\n",
    "\n",
    "``NER`` :-This is a Named Entity Recognition task where individual words of the sentence are tagged with an entity label it belongs to. The words which don't belong to any entity label are simply labeled as \"O\".\n",
    "\n",
    "``POS`` :- This is a Part of Speech tagging task. A part of speech is a category of words that have similar grammatical properties. Each word of the sentence is tagged with the part of speech label it belongs to. The words which don't belong to any part of speech label are simply labeled as \"O\".\n",
    "\n",
    "**Conversational Utility** :-  In conversational AI context, determining the syntactic parts of the sentence can help in extracting noun-phrases or important keyphrases from the sentence.\n",
    "\n",
    "**Data** :- In this example, we are using the <a href=\"https://www.clips.uantwerpen.be/conll2003/ner/\">coNLL 2003</a> data which is BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "The data is already present in ``coNLL_data`` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 1: Transforming data\n",
    "\n",
    "Raw data is in BIO tagged format with the POS and NER tags separated by space.\n",
    "\n",
    "We already provide a sample transformation function ``coNLL_ner_pos_to_tsv`` to convert this data to required tsv format. \n",
    "\n",
    "Running data transformations will save the required train, dev and test tsv data files under ``data`` directory in root of library. For more details on the data transformation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/data_transformations.html\">data transformations</a> in documentation.\n",
    "\n",
    "The transformation file should have the following details which is already created ``transform_file_conll.yml``.\n",
    "\n",
    "```\n",
    "transform1:\n",
    "  transform_func: coNLL_ner_pos_to_tsv\n",
    "  read_file_names:\n",
    "    - coNLL_train.txt\n",
    "    - coNLL_testa.txt\n",
    "    - coNLL_testb.txt\n",
    "  read_dir: coNLL_data\n",
    "  save_dir: ../../data\n",
    " ```\n",
    " Following command can be used to run the data transformation for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"../data_transformations.py\", line 37, in <module>\n",
      "    main()\n",
      "  File \"../data_transformations.py\", line 16, in main\n",
      "    transformParams = TransformParams(args.transform_file)\n",
      "  File \"d:\\FORME\\Khoa-Luan\\src\\model\\Model đơn tác vụ SRL\\utils\\transform_utils.py\", line 13, in __init__\n",
      "    self.transformDetails = yaml.safe_load(open(transformFilePath))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: \"'transform_file_conll.yml'\"\n"
     ]
    }
   ],
   "source": [
    "!python ../data_transformations.py \\\n",
    "    --transform_file 'transform_file_conll.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -2 Data Preparation\n",
    "\n",
    "For more details on the data preparation process, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/training.html#running-data-preparation\">data preparation</a> in documentation.\n",
    "\n",
    "Defining tasks file for training single model for entailment task. The file is already created at ``tasks_file_conll.yml``\n",
    "```\n",
    "conllner:\n",
    "  model_type: BERT\n",
    "  config_name: bert-base-uncased\n",
    "  dropout_prob: 0.2\n",
    "  label_map_or_file: ../../data/ner_coNLL_train_label_map.joblib\n",
    "  metrics:\n",
    "  - seqeval_f1_score\n",
    "  - seqeval_precision\n",
    "  - seqeval_recall\n",
    "  loss_type: NERLoss\n",
    "  task_type: NER\n",
    "  file_names:\n",
    "  - ner_coNLL_train.tsv\n",
    "  - ner_coNLL_testa.tsv\n",
    "  - ner_coNLL_testb.tsv\n",
    "\n",
    "conllpos:\n",
    "    model_type: BERT\n",
    "    config_name: bert-base-uncased\n",
    "    dropout_prob: 0.2\n",
    "    label_map_or_file: ../../data/pos_coNLL_train_label_map.joblib\n",
    "    metrics:\n",
    "    - seqeval_f1_score\n",
    "    - seqeval_precision\n",
    "    - seqeval_recall\n",
    "    loss_type: NERLoss\n",
    "    task_type: NER\n",
    "    file_names:\n",
    "    - pos_coNLL_train.tsv\n",
    "    - pos_coNLL_testa.tsv\n",
    "    - pos_coNLL_testb.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "task object created from task file...\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "bert model tokenizer loaded for config bert-base-uncased\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_train.tsv\n",
      "Processing Started...\n",
      "Data Size:  41740\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  1%|▎                                       | 47/5962 [00:00<00:12, 467.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  1%|▎                                       | 43/5962 [00:00<00:13, 426.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|▏                                       | 29/5962 [00:00<00:21, 279.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏                                       | 34/5962 [00:00<00:17, 334.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 94/5962 [00:00<00:14, 414.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏                                       | 33/5962 [00:00<00:18, 321.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  1%|▌                                       | 86/5962 [00:00<00:15, 368.52it/s]\u001b[A\n",
      "\n",
      "  1%|▍                                       | 66/5962 [00:00<00:18, 327.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 44/5962 [00:00<00:13, 434.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▌                                       | 80/5962 [00:00<00:14, 403.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 45/5962 [00:00<00:26, 222.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 70/5962 [00:00<00:16, 348.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|▋                                       | 99/5962 [00:00<00:19, 305.81it/s]\u001b[A\u001b[A\n",
      "  2%|▊                                      | 124/5962 [00:00<00:18, 310.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▉                                      | 136/5962 [00:00<00:19, 295.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▊                                      | 121/5962 [00:00<00:17, 333.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 68/5962 [00:00<00:29, 202.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                      | 105/5962 [00:00<00:17, 340.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|▊                                      | 130/5962 [00:00<00:19, 297.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▌                                       | 91/5962 [00:00<00:27, 210.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█                                      | 156/5962 [00:00<00:20, 281.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█                                      | 169/5962 [00:00<00:25, 227.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█                                      | 160/5962 [00:00<00:23, 244.66it/s]\u001b[A\u001b[A\n",
      "  3%|█                                      | 156/5962 [00:00<00:28, 205.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                      | 124/5962 [00:00<00:28, 202.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                      | 113/5962 [00:00<00:35, 165.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 186/5962 [00:00<00:24, 238.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█                                      | 171/5962 [00:00<00:23, 243.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▏                                     | 186/5962 [00:00<00:27, 210.24it/s]\u001b[A\u001b[A\n",
      "  3%|█▎                                     | 195/5962 [00:00<00:30, 186.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                      | 131/5962 [00:00<00:36, 159.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▉                                      | 150/5962 [00:00<00:30, 189.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 212/5962 [00:00<00:23, 243.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 197/5962 [00:00<00:23, 241.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▎                                     | 209/5962 [00:00<00:27, 207.54it/s]\u001b[A\u001b[A\n",
      "  3%|█▎                                     | 208/5962 [00:00<00:28, 199.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 216/5962 [00:00<00:31, 182.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 238/5962 [00:00<00:23, 240.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 172/5962 [00:00<00:31, 182.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▌                                     | 235/5962 [00:00<00:25, 221.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 223/5962 [00:00<00:27, 208.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|█▌                                     | 241/5962 [00:01<00:25, 225.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 236/5962 [00:01<00:30, 184.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 270/5962 [00:00<00:21, 260.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 192/5962 [00:00<00:31, 180.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▋                                     | 266/5962 [00:01<00:23, 244.55it/s]\u001b[A\u001b[A\n",
      "  4%|█▋                                     | 259/5962 [00:01<00:29, 195.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 246/5962 [00:01<00:28, 203.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 185/5962 [00:01<00:34, 165.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 299/5962 [00:01<00:21, 268.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 216/5962 [00:01<00:29, 194.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|█▉                                     | 303/5962 [00:01<00:20, 278.61it/s]\u001b[A\u001b[A\n",
      "  5%|█▊                                     | 283/5962 [00:01<00:27, 206.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 274/5962 [00:01<00:25, 222.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 206/5962 [00:01<00:32, 175.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 330/5962 [00:01<00:20, 278.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 237/5962 [00:01<00:28, 198.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▏                                    | 337/5962 [00:01<00:19, 294.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 303/5962 [00:01<00:23, 236.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  6%|██▎                                    | 347/5962 [00:01<00:21, 262.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 305/5962 [00:01<00:30, 186.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 224/5962 [00:01<00:36, 156.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▍                                    | 368/5962 [00:01<00:19, 287.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 258/5962 [00:01<00:32, 176.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 328/5962 [00:01<00:23, 237.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  6%|██▍                                    | 376/5962 [00:01<00:20, 267.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 341/5962 [00:01<00:24, 229.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 246/5962 [00:01<00:33, 172.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▋                                    | 418/5962 [00:01<00:15, 346.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 283/5962 [00:01<00:29, 194.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 362/5962 [00:01<00:21, 263.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▌                                    | 383/5962 [00:01<00:19, 279.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 279/5962 [00:01<00:26, 215.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▋                                    | 405/5962 [00:01<00:23, 240.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 329/5962 [00:01<00:21, 264.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 403/5962 [00:01<00:18, 301.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|██▉                                    | 454/5962 [00:01<00:17, 310.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 415/5962 [00:01<00:19, 285.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|██                                     | 306/5962 [00:01<00:24, 228.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▊                                    | 437/5962 [00:01<00:21, 259.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 358/5962 [00:01<00:22, 253.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▊                                    | 434/5962 [00:01<00:19, 286.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 451/5962 [00:01<00:18, 303.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 334/5962 [00:01<00:23, 241.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|███▏                                   | 487/5962 [00:01<00:19, 274.97it/s]\u001b[A\u001b[A\n",
      "  8%|███                                    | 475/5962 [00:01<00:19, 288.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▌                                    | 387/5962 [00:01<00:21, 262.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███                                    | 475/5962 [00:01<00:17, 316.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 551/5962 [00:01<00:14, 381.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 483/5962 [00:01<00:17, 306.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▍                                   | 516/5962 [00:01<00:20, 259.86it/s]\u001b[A\u001b[A\n",
      "  8%|███▎                                   | 506/5962 [00:01<00:19, 281.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 415/5962 [00:01<00:21, 256.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▎                                   | 508/5962 [00:01<00:17, 315.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 518/5962 [00:02<00:17, 317.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 590/5962 [00:01<00:14, 372.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▌                                   | 554/5962 [00:02<00:18, 289.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 452/5962 [00:01<00:19, 286.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▌                                   | 536/5962 [00:02<00:20, 266.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 552/5962 [00:01<00:15, 349.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▋                                   | 571/5962 [00:02<00:14, 377.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 641/5962 [00:02<00:12, 410.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|███▉                                   | 601/5962 [00:02<00:15, 336.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 485/5962 [00:02<00:18, 297.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 588/5962 [00:02<00:16, 333.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 610/5962 [00:02<00:15, 344.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▏                                  | 641/5962 [00:02<00:15, 351.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▉                                    | 443/5962 [00:02<00:23, 230.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▋                                   | 564/5962 [00:02<00:24, 223.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 516/5962 [00:02<00:18, 300.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 723/5962 [00:02<00:13, 387.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████                                   | 622/5962 [00:02<00:17, 308.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███                                    | 467/5962 [00:02<00:23, 229.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|███▊                                   | 588/5962 [00:02<00:23, 226.74it/s]\u001b[A\n",
      "\n",
      " 11%|████▍                                  | 678/5962 [00:02<00:16, 330.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 646/5962 [00:02<00:18, 285.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|████▉                                  | 763/5962 [00:02<00:13, 376.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▎                                  | 654/5962 [00:02<00:17, 301.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|████                                   | 623/5962 [00:02<00:20, 257.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 496/5962 [00:02<00:22, 243.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 581/5962 [00:02<00:18, 294.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|████▌                                  | 698/5962 [00:02<00:15, 339.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█████▎                                 | 804/5962 [00:02<00:13, 383.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 690/5962 [00:02<00:16, 316.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▎                                  | 666/5962 [00:02<00:17, 301.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 526/5962 [00:02<00:21, 256.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 742/5962 [00:02<00:14, 364.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|████▊                                  | 745/5962 [00:02<00:17, 299.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 845/5962 [00:02<00:13, 390.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 729/5962 [00:02<00:15, 336.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 12%|████▋                                  | 712/5962 [00:02<00:15, 343.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▋                                   | 568/5962 [00:02<00:17, 302.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 781/5962 [00:02<00:14, 361.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 885/5962 [00:02<00:12, 392.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 776/5962 [00:02<00:13, 373.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 609/5962 [00:02<00:16, 332.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|████▉                                  | 749/5962 [00:02<00:15, 347.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 820/5962 [00:02<00:13, 368.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████                                 | 925/5962 [00:02<00:13, 380.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▎                                 | 816/5962 [00:02<00:17, 287.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 814/5962 [00:02<00:15, 335.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|█████▏                                 | 785/5962 [00:02<00:16, 312.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 643/5962 [00:02<00:18, 288.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 859/5962 [00:02<00:13, 374.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 964/5962 [00:02<00:14, 350.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 851/5962 [00:02<00:14, 342.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▍                                 | 824/5962 [00:03<00:15, 331.26it/s]\u001b[A\n",
      "\n",
      " 14%|█████▌                                 | 846/5962 [00:02<00:19, 264.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▍                                  | 674/5962 [00:02<00:18, 288.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▉                                 | 900/5962 [00:03<00:13, 382.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|██████▍                               | 1002/5962 [00:03<00:13, 355.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 887/5962 [00:02<00:14, 339.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▋                                 | 860/5962 [00:03<00:15, 337.90it/s]\u001b[A\n",
      "\n",
      " 15%|█████▋                                 | 876/5962 [00:03<00:18, 271.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 720/5962 [00:03<00:15, 333.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 939/5962 [00:03<00:13, 383.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▋                               | 1044/5962 [00:03<00:13, 371.86it/s]\u001b[A\u001b[A\u001b[A\n",
      " 15%|█████▊                                 | 897/5962 [00:03<00:14, 343.29it/s]\u001b[A\n",
      "\n",
      " 15%|█████▉                                 | 906/5962 [00:03<00:18, 277.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████                                 | 922/5962 [00:03<00:15, 326.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▍                                | 978/5962 [00:03<00:13, 378.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 854/5962 [00:03<00:13, 377.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████▏                                | 941/5962 [00:03<00:13, 370.60it/s]\u001b[A\n",
      "\n",
      " 16%|██████                                 | 935/5962 [00:03<00:18, 273.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1082/5962 [00:03<00:14, 336.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                               | 1017/5962 [00:03<00:14, 349.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 790/5962 [00:03<00:17, 291.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▎                                | 963/5962 [00:03<00:18, 273.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 956/5962 [00:03<00:20, 240.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████                               | 1117/5962 [00:03<00:15, 314.17it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|██████▋                               | 1053/5962 [00:03<00:14, 347.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 826/5962 [00:03<00:16, 308.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████                                 | 931/5962 [00:03<00:15, 328.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▍                                | 993/5962 [00:03<00:17, 279.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                                | 984/5962 [00:03<00:20, 245.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1089/5962 [00:03<00:13, 350.68it/s]\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▍                               | 1013/5962 [00:03<00:15, 318.17it/s]\u001b[A\n",
      "\n",
      " 17%|██████▌                               | 1032/5962 [00:03<00:16, 306.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 859/5962 [00:03<00:19, 266.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 965/5962 [00:03<00:16, 298.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                               | 1017/5962 [00:03<00:18, 265.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1147/5962 [00:03<00:11, 414.95it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|██████▋                               | 1047/5962 [00:03<00:15, 322.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                                | 996/5962 [00:03<00:16, 301.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 888/5962 [00:03<00:18, 268.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████▊                               | 1063/5962 [00:03<00:17, 284.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▋                               | 1054/5962 [00:03<00:16, 290.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|███████▊                              | 1229/5962 [00:03<00:13, 347.45it/s]\u001b[A\u001b[A\u001b[A\n",
      " 20%|███████▌                              | 1190/5962 [00:03<00:11, 406.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1093/5962 [00:03<00:15, 315.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▉                                 | 917/5962 [00:03<00:19, 264.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                               | 1027/5962 [00:03<00:17, 275.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|███████▊                              | 1232/5962 [00:03<00:12, 389.71it/s]\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▏                              | 1121/5962 [00:03<00:15, 302.93it/s]\u001b[A\n",
      "\n",
      " 18%|██████▉                               | 1092/5962 [00:03<00:20, 238.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1144/5962 [00:03<00:13, 366.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1308/5962 [00:03<00:13, 356.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                              | 1273/5962 [00:04<00:11, 393.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 945/5962 [00:03<00:20, 240.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▎                              | 1153/5962 [00:04<00:15, 306.07it/s]\u001b[A\n",
      "\n",
      " 19%|███████▏                              | 1118/5962 [00:04<00:21, 221.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1313/5962 [00:04<00:11, 388.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 970/5962 [00:04<00:21, 230.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 20%|███████▌                              | 1185/5962 [00:04<00:16, 285.33it/s]\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▌                             | 1345/5962 [00:04<00:15, 300.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▎                              | 1144/5962 [00:04<00:20, 230.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████                               | 1113/5962 [00:04<00:17, 272.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                              | 1219/5962 [00:04<00:13, 342.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                                | 994/5962 [00:04<00:22, 222.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▍                              | 1174/5962 [00:04<00:19, 247.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▊                             | 1377/5962 [00:04<00:19, 232.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▌                             | 1353/5962 [00:04<00:18, 252.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|███████▉                              | 1243/5962 [00:04<00:21, 221.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                               | 1017/5962 [00:04<00:29, 168.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▉                              | 1255/5962 [00:04<00:20, 230.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▍                              | 1169/5962 [00:04<00:20, 234.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▊                             | 1385/5962 [00:04<00:18, 241.60it/s]\u001b[A\u001b[A\n",
      " 21%|████████                              | 1270/5962 [00:04<00:20, 232.59it/s]\u001b[A\n",
      "\n",
      "\n",
      " 24%|████████▉                             | 1404/5962 [00:04<00:22, 200.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▏                             | 1289/5962 [00:04<00:18, 252.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                               | 1036/5962 [00:04<00:29, 166.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|███████▊                              | 1222/5962 [00:04<00:25, 184.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▌                              | 1194/5962 [00:04<00:21, 220.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████                             | 1414/5962 [00:04<00:19, 231.35it/s]\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████                             | 1427/5962 [00:04<00:23, 196.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|███████▉                              | 1243/5962 [00:04<00:25, 183.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▋                               | 1055/5962 [00:04<00:31, 157.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                             | 1320/5962 [00:04<00:20, 229.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                              | 1218/5962 [00:04<00:24, 194.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▏                            | 1449/5962 [00:04<00:22, 199.27it/s]\u001b[A\u001b[A\u001b[A\n",
      " 22%|████████▍                             | 1325/5962 [00:04<00:20, 221.26it/s]\u001b[A\n",
      "\n",
      " 21%|████████                              | 1263/5962 [00:04<00:25, 185.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▊                               | 1072/5962 [00:04<00:31, 155.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▏                            | 1441/5962 [00:04<00:22, 196.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▉                              | 1239/5962 [00:04<00:24, 189.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|████████▋                             | 1355/5962 [00:04<00:19, 239.56it/s]\u001b[A\n",
      "\n",
      " 22%|████████▏                             | 1287/5962 [00:04<00:23, 197.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1093/5962 [00:04<00:28, 168.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1471/5962 [00:04<00:24, 184.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▋                             | 1372/5962 [00:04<00:20, 224.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▎                            | 1464/5962 [00:05<00:23, 189.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|████████▊                             | 1381/5962 [00:05<00:19, 233.59it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1308/5962 [00:05<00:23, 197.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████                               | 1115/5962 [00:04<00:26, 180.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▏                             | 1294/5962 [00:04<00:20, 229.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1486/5962 [00:05<00:22, 195.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|████████▉                             | 1406/5962 [00:05<00:19, 232.07it/s]\u001b[A\n",
      "\n",
      " 22%|████████▌                             | 1335/5962 [00:05<00:21, 216.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|█████████▋                            | 1522/5962 [00:05<00:20, 215.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1141/5962 [00:05<00:23, 200.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                             | 1328/5962 [00:05<00:17, 258.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▋                            | 1511/5962 [00:05<00:21, 207.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▋                             | 1361/5962 [00:05<00:20, 228.69it/s]\u001b[A\u001b[A\n",
      " 24%|█████████▏                            | 1433/5962 [00:05<00:18, 239.30it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|█████████▉                            | 1550/5962 [00:05<00:19, 229.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1541/5962 [00:05<00:19, 230.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▋                             | 1355/5962 [00:05<00:19, 238.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▉                             | 1396/5962 [00:05<00:17, 263.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████                            | 1581/5962 [00:05<00:17, 250.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▌                              | 1194/5962 [00:05<00:21, 227.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████                            | 1580/5962 [00:05<00:16, 271.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▉                             | 1396/5962 [00:05<00:16, 283.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████                             | 1427/5962 [00:05<00:16, 274.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                            | 1493/5962 [00:05<00:16, 268.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▊                              | 1225/5962 [00:05<00:19, 248.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                           | 1609/5962 [00:05<00:16, 257.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|█████████▋                            | 1522/5962 [00:05<00:16, 271.93it/s]\u001b[A\n",
      "\n",
      "\n",
      " 28%|██████████▌                           | 1648/5962 [00:05<00:15, 284.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▉                              | 1251/5962 [00:05<00:19, 241.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▎                            | 1455/5962 [00:05<00:18, 250.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████                             | 1426/5962 [00:05<00:17, 254.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▋                            | 1520/5962 [00:05<00:18, 246.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██████████▊                           | 1689/5962 [00:05<00:13, 320.03it/s]\u001b[A\u001b[A\u001b[A\n",
      " 26%|█████████▉                            | 1552/5962 [00:05<00:15, 278.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                             | 1277/5962 [00:05<00:19, 244.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                           | 1636/5962 [00:05<00:18, 237.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1546/5962 [00:05<00:18, 242.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▍                            | 1481/5962 [00:05<00:19, 227.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1302/5962 [00:05<00:19, 235.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████                            | 1581/5962 [00:05<00:17, 254.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▌                           | 1661/5962 [00:05<00:18, 235.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                            | 1571/5962 [00:05<00:18, 236.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▌                            | 1505/5962 [00:05<00:19, 223.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                             | 1327/5962 [00:05<00:19, 232.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▋                            | 1514/5962 [00:05<00:17, 261.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████▏                          | 1753/5962 [00:05<00:15, 272.64it/s]\u001b[A\u001b[A\u001b[A\n",
      " 28%|██████████▋                           | 1686/5962 [00:05<00:19, 217.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▏                           | 1596/5962 [00:05<00:22, 194.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|█████████▋                            | 1528/5962 [00:06<00:23, 191.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▉                            | 1551/5962 [00:05<00:15, 289.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▌                             | 1351/5962 [00:05<00:20, 226.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███████████▎                          | 1782/5962 [00:06<00:15, 264.74it/s]\u001b[A\u001b[A\u001b[A\n",
      " 29%|██████████▉                           | 1709/5962 [00:06<00:19, 214.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▊                             | 1379/5962 [00:06<00:19, 240.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|█████████▊                            | 1549/5962 [00:06<00:23, 186.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████                            | 1581/5962 [00:06<00:15, 276.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                           | 1617/5962 [00:06<00:24, 179.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████                           | 1733/5962 [00:06<00:19, 220.61it/s]\u001b[A\u001b[A\u001b[A\n",
      " 28%|██████████▌                           | 1665/5962 [00:06<00:17, 245.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|████████▉                             | 1405/5962 [00:06<00:18, 245.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████                            | 1585/5962 [00:06<00:19, 226.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                           | 1611/5962 [00:06<00:15, 281.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▍                           | 1641/5962 [00:06<00:22, 190.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▏                          | 1765/5962 [00:06<00:17, 244.53it/s]\u001b[A\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1837/5962 [00:06<00:15, 260.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████                             | 1430/5962 [00:06<00:19, 227.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1793/5962 [00:06<00:16, 254.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|███████████                           | 1728/5962 [00:06<00:15, 273.92it/s]\u001b[A\n",
      "\n",
      "\n",
      " 31%|███████████▉                          | 1866/5962 [00:06<00:15, 266.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▍                           | 1640/5962 [00:06<00:16, 265.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████▎                           | 1609/5962 [00:06<00:20, 210.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▎                            | 1454/5962 [00:06<00:20, 224.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▏                          | 1759/5962 [00:06<00:14, 283.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▋                           | 1671/5962 [00:06<00:15, 277.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████                          | 1900/5962 [00:06<00:14, 284.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████▍                           | 1632/5962 [00:06<00:20, 213.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████▎                         | 1929/5962 [00:06<00:14, 276.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████▉                           | 1708/5962 [00:06<00:22, 191.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1477/5962 [00:06<00:22, 202.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▍                          | 1788/5962 [00:06<00:17, 241.22it/s]\u001b[A\n",
      "\n",
      " 28%|██████████▌                           | 1655/5962 [00:06<00:22, 190.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████▊                           | 1700/5962 [00:06<00:17, 238.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1968/5962 [00:06<00:12, 307.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████                          | 1885/5962 [00:06<00:15, 269.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                            | 1498/5962 [00:06<00:22, 200.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|███████████▋                          | 1826/5962 [00:06<00:14, 276.70it/s]\u001b[A\n",
      "\n",
      " 28%|██████████▋                           | 1682/5962 [00:06<00:20, 209.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████                           | 1726/5962 [00:06<00:17, 242.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▏                         | 1916/5962 [00:06<00:14, 278.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|██████████▉                           | 1709/5962 [00:06<00:18, 225.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▏                          | 1757/5962 [00:06<00:16, 258.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1856/5962 [00:06<00:14, 275.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|████████████▋                         | 2000/5962 [00:06<00:15, 249.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▍                         | 1945/5962 [00:06<00:14, 281.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████                           | 1737/5962 [00:06<00:17, 237.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1802/5962 [00:06<00:13, 309.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1539/5962 [00:06<00:23, 188.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|████████████▌                         | 1974/5962 [00:07<00:14, 273.20it/s]\u001b[A\n",
      "\n",
      " 30%|███████████▏                          | 1762/5962 [00:07<00:17, 240.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1834/5962 [00:06<00:13, 309.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1831/5962 [00:06<00:17, 241.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|████████████▉                         | 2027/5962 [00:07<00:18, 213.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▏                           | 1592/5962 [00:06<00:15, 279.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|████████████▊                         | 2012/5962 [00:07<00:13, 300.88it/s]\u001b[A\n",
      "\n",
      " 30%|███████████▍                          | 1797/5962 [00:07<00:15, 270.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▉                          | 1865/5962 [00:07<00:15, 265.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▉                          | 1866/5962 [00:07<00:13, 304.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|█████████████                         | 2051/5962 [00:07<00:18, 209.93it/s]\u001b[A\u001b[A\u001b[A\n",
      " 33%|████████████▎                         | 1939/5962 [00:07<00:16, 241.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████                         | 2046/5962 [00:07<00:12, 310.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|███████████▋                          | 1829/5962 [00:07<00:14, 283.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████                          | 1897/5962 [00:07<00:13, 293.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2074/5962 [00:07<00:18, 212.84it/s]\u001b[A\u001b[A\u001b[A\n",
      " 33%|████████████▌                         | 1979/5962 [00:07<00:14, 283.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2078/5962 [00:07<00:12, 311.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|███████████▊                          | 1862/5962 [00:07<00:13, 295.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▌                           | 1648/5962 [00:07<00:19, 223.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▎                         | 1931/5962 [00:07<00:13, 304.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▎                        | 2097/5962 [00:07<00:17, 217.11it/s]\u001b[A\u001b[A\u001b[A\n",
      " 34%|████████████▉                         | 2022/5962 [00:07<00:12, 322.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                        | 2114/5962 [00:07<00:11, 323.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████                          | 1892/5962 [00:07<00:14, 286.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1971/5962 [00:07<00:12, 328.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|█████████████▌                        | 2136/5962 [00:07<00:14, 261.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▋                           | 1672/5962 [00:07<00:20, 210.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 35%|█████████████▏                        | 2061/5962 [00:07<00:11, 339.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▋                        | 2148/5962 [00:07<00:11, 327.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▎                         | 1928/5962 [00:07<00:13, 303.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▊                         | 2012/5962 [00:07<00:11, 350.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|█████████████▊                        | 2171/5962 [00:07<00:13, 283.37it/s]\u001b[A\u001b[A\u001b[A\n",
      " 35%|█████████████▍                        | 2100/5962 [00:07<00:10, 351.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▋                         | 1989/5962 [00:07<00:14, 283.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|█████████████▉                        | 2181/5962 [00:07<00:11, 320.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████                        | 2202/5962 [00:07<00:13, 284.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████▍                         | 1959/5962 [00:07<00:16, 244.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▊                         | 2019/5962 [00:07<00:15, 262.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████                         | 2048/5962 [00:07<00:14, 279.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 36%|█████████████▌                        | 2136/5962 [00:07<00:13, 277.66it/s]\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████▏                       | 2232/5962 [00:07<00:13, 268.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████                        | 2214/5962 [00:07<00:15, 248.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████▋                         | 1986/5962 [00:07<00:16, 246.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████                         | 2047/5962 [00:07<00:15, 257.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 36%|█████████████▊                        | 2176/5962 [00:07<00:12, 304.84it/s]\u001b[A\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 2261/5962 [00:07<00:13, 271.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████                           | 1734/5962 [00:07<00:25, 164.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2243/5962 [00:07<00:14, 253.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|████████████▊                         | 2013/5962 [00:07<00:15, 252.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▎                        | 2087/5962 [00:07<00:13, 291.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▏                          | 1755/5962 [00:07<00:24, 174.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|██████████████▌                       | 2289/5962 [00:08<00:14, 255.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|█████████████                         | 2049/5962 [00:08<00:13, 280.58it/s]\u001b[A\u001b[A\n",
      " 37%|██████████████                        | 2209/5962 [00:08<00:13, 269.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                        | 2106/5962 [00:07<00:17, 224.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▍                        | 2117/5962 [00:07<00:13, 285.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▎                          | 1783/5962 [00:08<00:20, 199.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 2305/5962 [00:08<00:13, 278.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|█████████████▎                        | 2085/5962 [00:08<00:12, 302.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▌                        | 2135/5962 [00:08<00:16, 239.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|██████████████▎                       | 2239/5962 [00:08<00:14, 261.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▋                        | 2147/5962 [00:08<00:13, 279.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▌                          | 1809/5962 [00:08<00:19, 211.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2335/5962 [00:08<00:13, 263.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▍                        | 2117/5962 [00:08<00:13, 283.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▊                        | 2166/5962 [00:08<00:14, 255.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▊                        | 2176/5962 [00:08<00:13, 282.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|██████████████▍                       | 2267/5962 [00:08<00:15, 244.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1835/5962 [00:08<00:18, 222.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2373/5962 [00:08<00:13, 268.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▋                        | 2147/5962 [00:08<00:13, 287.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|█████████████▉                        | 2194/5962 [00:08<00:15, 250.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|███████████████                       | 2363/5962 [00:08<00:16, 215.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1858/5962 [00:08<00:21, 193.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▏                       | 2221/5962 [00:08<00:14, 254.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2401/5962 [00:08<00:15, 235.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2244/5962 [00:08<00:13, 277.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|█████████████▉                        | 2177/5962 [00:08<00:15, 251.29it/s]\u001b[A\u001b[A\n",
      " 39%|██████████████▊                       | 2320/5962 [00:08<00:15, 228.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████                          | 1888/5962 [00:08<00:18, 219.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2248/5962 [00:08<00:14, 250.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 2387/5962 [00:08<00:19, 186.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|██████████████                        | 2204/5962 [00:08<00:15, 236.27it/s]\u001b[A\u001b[A\n",
      " 39%|██████████████▉                       | 2352/5962 [00:08<00:14, 250.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 2273/5962 [00:08<00:15, 244.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▏                         | 1915/5962 [00:08<00:17, 231.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2459/5962 [00:08<00:13, 251.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2408/5962 [00:08<00:18, 187.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|██████████████▏                       | 2232/5962 [00:08<00:15, 246.65it/s]\u001b[A\u001b[A\n",
      " 40%|███████████████▏                      | 2379/5962 [00:08<00:14, 252.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▍                         | 1942/5962 [00:08<00:16, 240.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 2485/5962 [00:08<00:14, 235.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 2299/5962 [00:08<00:15, 229.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 2429/5962 [00:08<00:19, 177.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|███████████████▎                      | 2405/5962 [00:08<00:15, 233.70it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1967/5962 [00:08<00:17, 230.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|██████████████▍                       | 2258/5962 [00:08<00:16, 219.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▊                       | 2323/5962 [00:08<00:16, 222.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 2510/5962 [00:08<00:16, 209.87it/s]\u001b[A\u001b[A\u001b[A\n",
      " 41%|███████████████▌                      | 2437/5962 [00:09<00:13, 255.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 2448/5962 [00:09<00:20, 173.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▋                         | 1997/5962 [00:08<00:15, 249.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|██████████████▌                       | 2281/5962 [00:09<00:17, 206.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2357/5962 [00:08<00:14, 252.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|███████████████▋                      | 2468/5962 [00:09<00:12, 270.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2349/5962 [00:09<00:17, 210.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████████████████▏                     | 2532/5962 [00:09<00:16, 202.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2466/5962 [00:09<00:20, 169.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 2389/5962 [00:09<00:13, 270.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▋                       | 2303/5962 [00:09<00:18, 198.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 2500/5962 [00:09<00:16, 212.53it/s]\u001b[A\u001b[A\u001b[A\n",
      " 42%|███████████████▉                      | 2496/5962 [00:09<00:13, 259.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████                         | 2057/5962 [00:09<00:14, 265.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2372/5962 [00:09<00:18, 198.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 2419/5962 [00:09<00:12, 275.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▊                       | 2324/5962 [00:09<00:19, 189.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 2584/5962 [00:09<00:15, 224.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████▏                     | 2532/5962 [00:09<00:14, 238.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2393/5962 [00:09<00:17, 200.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|████████████████                      | 2523/5962 [00:09<00:14, 237.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 2450/5962 [00:09<00:12, 284.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▉                       | 2352/5962 [00:09<00:17, 211.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2608/5962 [00:09<00:14, 227.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▎                     | 2563/5962 [00:09<00:13, 254.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 2479/5962 [00:09<00:12, 285.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▍                      | 2414/5962 [00:09<00:18, 187.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▏                      | 2377/5962 [00:09<00:16, 217.53it/s]\u001b[A\u001b[A\n",
      " 43%|████████████████▌                     | 2593/5962 [00:09<00:12, 267.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▋                        | 2145/5962 [00:09<00:14, 270.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 2513/5962 [00:09<00:11, 300.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▊                     | 2632/5962 [00:09<00:16, 198.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▎                      | 2405/5962 [00:09<00:15, 231.70it/s]\u001b[A\u001b[A\n",
      " 43%|████████████████▍                     | 2571/5962 [00:09<00:16, 211.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 2624/5962 [00:09<00:12, 274.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|█████████████▉                        | 2181/5962 [00:09<00:12, 291.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▏                     | 2544/5962 [00:09<00:11, 294.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▉                     | 2653/5962 [00:09<00:17, 189.31it/s]\u001b[A\u001b[A\u001b[A\n",
      " 44%|████████████████▌                     | 2594/5962 [00:09<00:15, 215.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2452/5962 [00:09<00:21, 162.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|████████████████▉                     | 2652/5962 [00:09<00:13, 248.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████                        | 2211/5962 [00:09<00:14, 261.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2674/5962 [00:09<00:16, 193.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 2574/5962 [00:09<00:12, 267.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2469/5962 [00:09<00:24, 140.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|████████████████▋                     | 2617/5962 [00:10<00:21, 155.41it/s]\u001b[A\n",
      "\n",
      " 45%|█████████████████                     | 2678/5962 [00:10<00:16, 193.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2238/5962 [00:09<00:19, 193.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 2484/5962 [00:10<00:29, 118.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2694/5962 [00:10<00:26, 125.21it/s]\u001b[A\u001b[A\u001b[A\n",
      " 44%|████████████████▊                     | 2636/5962 [00:10<00:22, 147.22it/s]\u001b[A\n",
      "\n",
      " 45%|█████████████████▏                    | 2700/5962 [00:10<00:18, 175.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2602/5962 [00:10<00:21, 159.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 2261/5962 [00:10<00:21, 172.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 2504/5962 [00:10<00:25, 135.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|████████████████▉                     | 2658/5962 [00:10<00:20, 161.90it/s]\u001b[A\n",
      "\n",
      " 42%|███████████████▊                      | 2489/5962 [00:10<00:22, 157.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████▎                    | 2710/5962 [00:10<00:26, 124.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2720/5962 [00:10<00:20, 160.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▌                       | 2281/5962 [00:10<00:21, 175.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 2528/5962 [00:10<00:21, 156.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████                      | 2513/5962 [00:10<00:19, 172.74it/s]\u001b[A\u001b[A\n",
      " 46%|█████████████████▍                    | 2738/5962 [00:10<00:20, 160.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 2301/5962 [00:10<00:21, 171.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2725/5962 [00:10<00:29, 108.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▏                     | 2545/5962 [00:10<00:22, 148.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████▏                     | 2533/5962 [00:10<00:19, 178.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▊                     | 2645/5962 [00:10<00:22, 150.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▌                    | 2757/5962 [00:10<00:19, 164.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▊                       | 2321/5962 [00:10<00:20, 173.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 2570/5962 [00:10<00:19, 173.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 2738/5962 [00:10<00:31, 103.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████████▉                     | 2663/5962 [00:10<00:22, 145.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████▎                     | 2552/5962 [00:10<00:22, 154.99it/s]\u001b[A\u001b[A\n",
      " 47%|█████████████████▋                    | 2775/5962 [00:10<00:20, 156.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▌                     | 2589/5962 [00:10<00:19, 174.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2340/5962 [00:10<00:23, 156.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2680/5962 [00:10<00:23, 141.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▊                    | 2793/5962 [00:10<00:19, 161.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▉                     | 2750/5962 [00:10<00:36, 88.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2608/5962 [00:10<00:21, 157.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2700/5962 [00:10<00:21, 150.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|██████████████████                     | 2763/5962 [00:10<00:33, 95.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████▍                     | 2585/5962 [00:10<00:26, 129.11it/s]\u001b[A\u001b[A\n",
      " 47%|█████████████████▉                    | 2810/5962 [00:10<00:22, 137.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 2625/5962 [00:10<00:22, 149.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2357/5962 [00:10<00:30, 119.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2717/5962 [00:10<00:21, 151.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2778/5962 [00:10<00:29, 106.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|██████████████████                    | 2828/5962 [00:11<00:21, 147.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 2389/5962 [00:10<00:22, 159.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▌                    | 2746/5962 [00:11<00:30, 106.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 2742/5962 [00:10<00:18, 175.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 2798/5962 [00:11<00:25, 123.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▊                     | 2641/5962 [00:11<00:26, 126.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|████████████████▋                     | 2616/5962 [00:11<00:25, 130.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 2844/5962 [00:11<00:21, 144.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2761/5962 [00:11<00:18, 174.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▌                    | 2759/5962 [00:11<00:30, 104.14it/s]\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2824/5962 [00:11<00:20, 155.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|████████████████▊                     | 2642/5962 [00:11<00:20, 162.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2864/5962 [00:11<00:19, 155.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 2438/5962 [00:11<00:19, 181.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2780/5962 [00:11<00:18, 172.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|█████████████████▋                    | 2778/5962 [00:11<00:25, 122.67it/s]\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████                    | 2841/5962 [00:11<00:19, 156.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2881/5962 [00:11<00:19, 157.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|████████████████▉                     | 2660/5962 [00:11<00:20, 157.76it/s]\u001b[A\u001b[A\n",
      " 47%|█████████████████▊                    | 2793/5962 [00:11<00:24, 128.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2459/5962 [00:11<00:19, 176.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 2798/5962 [00:11<00:18, 168.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▍                   | 2902/5962 [00:11<00:17, 171.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2686/5962 [00:11<00:24, 133.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|█████████████████▉                    | 2819/5962 [00:11<00:19, 157.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2816/5962 [00:11<00:19, 165.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 2479/5962 [00:11<00:21, 160.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2920/5962 [00:11<00:17, 170.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|█████████████████▎                    | 2712/5962 [00:11<00:16, 198.07it/s]\u001b[A\u001b[A\n",
      " 48%|██████████████████                    | 2838/5962 [00:11<00:19, 160.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████                    | 2833/5962 [00:11<00:18, 165.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2700/5962 [00:11<00:27, 118.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▋                   | 2938/5962 [00:11<00:20, 147.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████▍                    | 2736/5962 [00:11<00:18, 170.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████▍                   | 2890/5962 [00:11<00:26, 114.16it/s]\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▏                   | 2855/5962 [00:11<00:22, 136.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2713/5962 [00:11<00:31, 102.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 2850/5962 [00:11<00:24, 128.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 2954/5962 [00:11<00:21, 136.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████▌                    | 2757/5962 [00:11<00:19, 167.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2903/5962 [00:11<00:26, 114.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2726/5962 [00:11<00:30, 106.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2874/5962 [00:11<00:20, 151.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▏                     | 2541/5962 [00:11<00:21, 160.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|██████████████████▉                   | 2969/5962 [00:12<00:22, 133.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2917/5962 [00:11<00:26, 115.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▋                    | 2775/5962 [00:12<00:20, 158.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▍                   | 2897/5962 [00:11<00:18, 168.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 2989/5962 [00:12<00:19, 150.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▍                   | 2883/5962 [00:12<00:27, 110.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2759/5962 [00:12<00:24, 130.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▊                    | 2792/5962 [00:12<00:20, 157.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▊                   | 2944/5962 [00:12<00:20, 144.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▋                   | 2923/5962 [00:12<00:15, 190.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▏                  | 3005/5962 [00:12<00:20, 146.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2783/5962 [00:12<00:19, 158.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▍                   | 2896/5962 [00:12<00:27, 110.83it/s]\u001b[A\n",
      "\n",
      " 47%|█████████████████▉                    | 2814/5962 [00:12<00:18, 171.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 2966/5962 [00:12<00:18, 162.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 2956/5962 [00:12<00:13, 227.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 2622/5962 [00:12<00:14, 224.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████                    | 2840/5962 [00:12<00:16, 194.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2908/5962 [00:12<00:28, 108.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 2994/5962 [00:12<00:15, 189.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 2981/5962 [00:12<00:12, 230.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3021/5962 [00:12<00:22, 129.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2817/5962 [00:12<00:19, 158.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▋                   | 2926/5962 [00:12<00:24, 124.51it/s]\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▏                  | 3016/5962 [00:12<00:15, 193.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████▏                   | 2861/5962 [00:12<00:17, 177.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3035/5962 [00:12<00:23, 122.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2674/5962 [00:12<00:15, 211.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████                    | 2836/5962 [00:12<00:18, 165.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▊                   | 2943/5962 [00:12<00:22, 135.47it/s]\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3043/5962 [00:12<00:13, 212.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|███████████████████▍                  | 3048/5962 [00:12<00:24, 117.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2696/5962 [00:12<00:16, 201.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3027/5962 [00:12<00:15, 189.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 2854/5962 [00:12<00:18, 168.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▌                  | 3068/5962 [00:12<00:13, 218.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▌                   | 2906/5962 [00:12<00:15, 199.90it/s]\u001b[A\u001b[A\n",
      " 51%|███████████████████▌                  | 3065/5962 [00:12<00:22, 129.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2717/5962 [00:12<00:16, 195.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2878/5962 [00:12<00:16, 186.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3047/5962 [00:12<00:15, 184.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3091/5962 [00:12<00:14, 203.70it/s]\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████                   | 2986/5962 [00:12<00:18, 161.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 2737/5962 [00:12<00:16, 195.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▋                   | 2927/5962 [00:12<00:17, 170.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▌                  | 3069/5962 [00:12<00:15, 190.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▍                   | 2897/5962 [00:12<00:17, 176.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2757/5962 [00:12<00:17, 183.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3092/5962 [00:13<00:25, 113.88it/s]\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████▏                  | 3004/5962 [00:13<00:22, 131.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3089/5962 [00:12<00:17, 167.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▊                   | 2946/5962 [00:13<00:20, 143.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 3104/5962 [00:13<00:25, 113.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|███████████████████▏                  | 3020/5962 [00:13<00:21, 135.88it/s]\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 3130/5962 [00:13<00:17, 159.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 3107/5962 [00:13<00:17, 164.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2776/5962 [00:13<00:20, 156.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|██████████████████▉                   | 2962/5962 [00:13<00:21, 139.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 2963/5962 [00:13<00:15, 187.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▊                  | 3116/5962 [00:13<00:27, 103.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 2793/5962 [00:13<00:20, 155.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 3124/5962 [00:13<00:18, 156.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 2977/5962 [00:13<00:22, 132.83it/s]\u001b[A\u001b[A\u001b[A\n",
      " 51%|███████████████████▍                  | 3053/5962 [00:13<00:20, 140.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 3140/5962 [00:13<00:19, 145.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 2983/5962 [00:13<00:19, 151.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████                   | 2993/5962 [00:13<00:21, 135.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2810/5962 [00:13<00:22, 137.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▌                  | 3073/5962 [00:13<00:18, 155.44it/s]\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 3162/5962 [00:13<00:23, 119.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 3156/5962 [00:13<00:19, 145.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████▏                  | 3008/5962 [00:13<00:21, 138.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▌                  | 3137/5962 [00:13<00:31, 88.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▋                  | 3096/5962 [00:13<00:16, 172.85it/s]\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 3176/5962 [00:13<00:22, 123.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 3000/5962 [00:13<00:23, 127.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|███████████████████▎                  | 3024/5962 [00:13<00:20, 143.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 3171/5962 [00:13<00:20, 138.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▌                  | 3147/5962 [00:13<00:31, 90.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▊                  | 3118/5962 [00:13<00:15, 183.98it/s]\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▎                 | 3191/5962 [00:13<00:21, 127.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|███████████████████▍                  | 3041/5962 [00:13<00:19, 148.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▎                 | 3190/5962 [00:13<00:18, 150.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 3157/5962 [00:13<00:30, 90.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|████████████████████                  | 3139/5962 [00:13<00:14, 189.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▏                  | 3015/5962 [00:13<00:25, 113.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▍                 | 3208/5962 [00:13<00:20, 136.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▍                 | 3211/5962 [00:13<00:16, 164.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▋                  | 3167/5962 [00:13<00:30, 92.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2878/5962 [00:13<00:20, 151.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3035/5962 [00:13<00:22, 131.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 3227/5962 [00:13<00:18, 150.63it/s]\u001b[A\u001b[A\u001b[A\n",
      " 53%|████████████████████▏                 | 3159/5962 [00:13<00:15, 175.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 3239/5962 [00:13<00:13, 195.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████▋                  | 3082/5962 [00:13<00:16, 171.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▍                   | 2898/5962 [00:13<00:18, 163.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3050/5962 [00:13<00:21, 134.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 3246/5962 [00:13<00:16, 159.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▊                 | 3259/5962 [00:13<00:14, 189.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2922/5962 [00:13<00:16, 183.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|████████████████████▎                 | 3192/5962 [00:14<00:27, 100.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▌                  | 3070/5962 [00:13<00:19, 149.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████▊                  | 3100/5962 [00:14<00:19, 150.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████▊                 | 3263/5962 [00:14<00:17, 153.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3282/5962 [00:14<00:13, 199.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▋                   | 2941/5962 [00:14<00:16, 184.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|████████████████████▍                 | 3204/5962 [00:14<00:26, 105.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3098/5962 [00:14<00:15, 181.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████▊                  | 3116/5962 [00:14<00:19, 144.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 3217/5962 [00:14<00:25, 108.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 2960/5962 [00:14<00:17, 167.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|████████████████████▍                 | 3215/5962 [00:14<00:20, 134.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 3118/5962 [00:14<00:19, 142.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████████████████████                  | 3228/5962 [00:14<00:27, 99.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████                 | 3303/5962 [00:14<00:23, 114.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▍                  | 3131/5962 [00:14<00:35, 80.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▏                 | 3239/5962 [00:14<00:36, 74.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████████████████████▋                 | 3307/5962 [00:14<00:29, 90.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▎                 | 3251/5962 [00:14<00:32, 83.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▏                | 3323/5962 [00:14<00:25, 104.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▋                 | 3319/5962 [00:14<00:27, 96.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▌                  | 3143/5962 [00:14<00:35, 78.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▎                 | 3261/5962 [00:14<00:31, 85.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|█████████████████████▏                 | 3230/5962 [00:14<00:40, 67.55it/s]\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 3336/5962 [00:14<00:25, 101.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▊                 | 3333/5962 [00:14<00:27, 95.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3278/5962 [00:15<00:25, 105.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 3161/5962 [00:14<00:29, 94.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▋                  | 3154/5962 [00:14<00:38, 72.89it/s]\u001b[A\u001b[A\n",
      " 54%|█████████████████████▏                 | 3246/5962 [00:15<00:33, 80.15it/s]\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 3351/5962 [00:14<00:23, 111.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 3350/5962 [00:14<00:24, 108.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3290/5962 [00:15<00:25, 106.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▋                  | 3166/5962 [00:15<00:34, 81.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 3172/5962 [00:14<00:29, 93.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|█████████████████████▎                 | 3263/5962 [00:15<00:28, 94.70it/s]\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3367/5962 [00:15<00:21, 123.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3365/5962 [00:15<00:23, 109.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3040/5962 [00:15<00:24, 118.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▊                  | 3178/5962 [00:15<00:31, 88.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▎                 | 3187/5962 [00:15<00:26, 104.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|████████████████████▉                 | 3278/5962 [00:15<00:25, 104.65it/s]\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3389/5962 [00:15<00:17, 147.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3386/5962 [00:15<00:19, 130.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3054/5962 [00:15<00:23, 122.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▍                 | 3204/5962 [00:15<00:23, 119.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 3405/5962 [00:15<00:17, 147.86it/s]\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▏                | 3323/5962 [00:15<00:22, 119.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 3405/5962 [00:15<00:18, 140.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████▉                  | 3203/5962 [00:15<00:32, 86.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▌                  | 3068/5962 [00:15<00:28, 102.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████                  | 3217/5962 [00:15<00:28, 96.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|█████████████████████▋                 | 3306/5962 [00:15<00:28, 94.26it/s]\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▊                 | 3336/5962 [00:15<00:31, 83.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 3421/5962 [00:15<00:23, 106.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▏                  | 3080/5962 [00:15<00:35, 82.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████                  | 3213/5962 [00:15<00:43, 63.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████                  | 3228/5962 [00:15<00:36, 74.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▋                 | 3318/5962 [00:15<00:35, 74.15it/s]\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▍                | 3434/5962 [00:15<00:29, 86.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▍                | 3434/5962 [00:15<00:28, 88.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▉                 | 3347/5962 [00:15<00:38, 67.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████                  | 3221/5962 [00:15<00:45, 60.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 3447/5962 [00:15<00:26, 94.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▉                 | 3356/5962 [00:16<00:38, 68.29it/s]\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 3445/5962 [00:16<00:35, 71.10it/s]\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▊                 | 3337/5962 [00:16<00:38, 68.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████                  | 3229/5962 [00:16<00:52, 52.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|██████████████████████                 | 3364/5962 [00:16<00:43, 59.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▉                 | 3345/5962 [00:16<00:37, 69.80it/s]\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 3454/5962 [00:16<00:38, 65.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 3472/5962 [00:16<00:26, 94.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▎                 | 3254/5962 [00:16<00:43, 62.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████                 | 3374/5962 [00:16<00:38, 66.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▏                 | 3236/5962 [00:16<00:54, 49.93it/s]\u001b[A\u001b[A\n",
      " 56%|█████████████████████▉                 | 3361/5962 [00:16<00:29, 88.59it/s]\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 3468/5962 [00:16<00:32, 77.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▏               | 3487/5962 [00:16<00:23, 104.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▍                 | 3269/5962 [00:16<00:33, 79.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▍                  | 3115/5962 [00:16<00:46, 61.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████                 | 3382/5962 [00:16<00:40, 64.50it/s]\u001b[A\u001b[A\n",
      " 57%|█████████████████████▌                | 3377/5962 [00:16<00:24, 104.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▍                 | 3280/5962 [00:16<00:31, 85.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 3499/5962 [00:16<00:23, 103.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▊                | 3478/5962 [00:16<00:33, 75.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▏                 | 3248/5962 [00:16<00:51, 52.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▏                | 3391/5962 [00:16<00:37, 68.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3389/5962 [00:16<00:24, 102.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3513/5962 [00:16<00:21, 112.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 3499/5962 [00:16<00:24, 102.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████▎                 | 3265/5962 [00:16<00:37, 72.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▏                | 3399/5962 [00:16<00:37, 67.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 57%|█████████████████████▋                | 3401/5962 [00:16<00:25, 100.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3525/5962 [00:16<00:21, 111.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3512/5962 [00:16<00:22, 108.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████                 | 3308/5962 [00:16<00:26, 100.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████▍                 | 3281/5962 [00:16<00:29, 91.92it/s]\u001b[A\u001b[A\n",
      " 57%|█████████████████████▋                | 3412/5962 [00:16<00:25, 101.39it/s]\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3527/5962 [00:16<00:20, 117.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▋                 | 3319/5962 [00:16<00:29, 90.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▌                  | 3136/5962 [00:16<00:56, 49.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████▏               | 3537/5962 [00:16<00:26, 89.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|██████████████████████▎                | 3407/5962 [00:16<00:48, 52.27it/s]\u001b[A\u001b[A\u001b[A\n",
      " 57%|██████████████████████▍                | 3423/5962 [00:16<00:25, 98.61it/s]\u001b[A\n",
      "\n",
      " 55%|█████████████████████▌                 | 3291/5962 [00:16<00:32, 81.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▊                 | 3330/5962 [00:16<00:27, 95.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▌                  | 3142/5962 [00:16<00:55, 50.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▎                | 3413/5962 [00:17<00:48, 52.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 3557/5962 [00:16<00:18, 127.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████▌                 | 3304/5962 [00:16<00:28, 92.80it/s]\u001b[A\u001b[A\n",
      " 58%|█████████████████████▉                | 3438/5962 [00:17<00:25, 100.70it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 3344/5962 [00:16<00:24, 106.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▌                  | 3148/5962 [00:17<00:55, 50.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 3564/5962 [00:16<00:23, 102.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▊               | 3574/5962 [00:17<00:17, 135.47it/s]\n",
      "\n",
      " 57%|██████████████████████▎                | 3419/5962 [00:17<00:48, 52.48it/s]\u001b[A\u001b[A\n",
      " 58%|██████████████████████                | 3459/5962 [00:17<00:20, 124.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3360/5962 [00:17<00:23, 112.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▍                | 3427/5962 [00:17<00:43, 58.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 3593/5962 [00:17<00:16, 146.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 3576/5962 [00:17<00:23, 99.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3375/5962 [00:17<00:22, 116.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 3164/5962 [00:17<00:47, 58.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████▎                | 3343/5962 [00:17<00:24, 106.73it/s]\u001b[A\u001b[A\n",
      " 58%|██████████████████████▌                | 3440/5962 [00:17<00:33, 75.23it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 3587/5962 [00:17<00:24, 97.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 3609/5962 [00:17<00:18, 128.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3387/5962 [00:17<00:22, 112.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 3171/5962 [00:17<00:45, 60.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|██████████████████████▏               | 3484/5962 [00:17<00:23, 104.61it/s]\u001b[A\n",
      "\n",
      " 56%|█████████████████████▍                | 3355/5962 [00:17<00:26, 100.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 3449/5962 [00:17<00:35, 70.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 3400/5962 [00:17<00:21, 116.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▊                  | 3187/5962 [00:17<00:32, 86.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 3623/5962 [00:17<00:19, 120.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████▍                | 3366/5962 [00:17<00:25, 100.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 3467/5962 [00:17<00:27, 91.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▉                  | 3201/5962 [00:17<00:27, 99.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 3636/5962 [00:17<00:19, 117.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 3417/5962 [00:17<00:21, 117.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 3624/5962 [00:17<00:21, 111.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▎               | 3506/5962 [00:17<00:24, 100.05it/s]\u001b[A\n",
      "\n",
      " 57%|██████████████████████                 | 3377/5962 [00:17<00:27, 93.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 3477/5962 [00:17<00:29, 85.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 3652/5962 [00:17<00:18, 127.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▏                | 3387/5962 [00:17<00:28, 90.21it/s]\u001b[A\u001b[A\n",
      " 59%|███████████████████████                | 3518/5962 [00:17<00:26, 90.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▊                | 3486/5962 [00:17<00:33, 74.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 3666/5962 [00:17<00:22, 102.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████                  | 3224/5962 [00:17<00:38, 71.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 3646/5962 [00:18<00:33, 68.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▍                | 3429/5962 [00:18<00:43, 58.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▊                | 3494/5962 [00:18<00:46, 53.07it/s]\u001b[A\n",
      "\n",
      " 57%|██████████████████████▏                | 3397/5962 [00:18<00:46, 55.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▏                 | 3233/5962 [00:18<00:53, 51.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3678/5962 [00:18<00:34, 65.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|██████████████████████▎                | 3405/5962 [00:18<00:54, 47.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3687/5962 [00:18<00:38, 59.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▉                | 3501/5962 [00:18<01:06, 37.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▍                | 3439/5962 [00:18<01:00, 41.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▏                 | 3240/5962 [00:18<01:07, 40.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 3695/5962 [00:18<00:39, 58.04it/s]\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▉                | 3506/5962 [00:18<01:06, 36.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▏                 | 3246/5962 [00:18<01:02, 43.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▎                | 3412/5962 [00:18<01:03, 39.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 3662/5962 [00:18<00:53, 43.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 3446/5962 [00:18<00:57, 43.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 3715/5962 [00:18<00:27, 80.66it/s]\u001b[A\u001b[A\u001b[A\n",
      " 59%|███████████████████████                | 3524/5962 [00:18<00:40, 60.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3671/5962 [00:18<00:45, 50.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 3457/5962 [00:18<00:47, 53.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▎                 | 3260/5962 [00:18<00:46, 58.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▍                | 3423/5962 [00:18<00:52, 48.07it/s]\u001b[A\u001b[A\n",
      " 60%|███████████████████████▎               | 3562/5962 [00:18<00:42, 56.65it/s]\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3727/5962 [00:18<00:31, 71.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████                | 3533/5962 [00:19<00:44, 54.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3678/5962 [00:18<00:49, 45.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|██████████████████████▍                | 3433/5962 [00:19<00:50, 49.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 3465/5962 [00:18<00:51, 48.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3687/5962 [00:19<00:42, 53.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▍                 | 3278/5962 [00:19<00:45, 59.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|██████████████████████▌                | 3443/5962 [00:19<00:43, 58.40it/s]\u001b[A\u001b[A\n",
      " 60%|███████████████████████▎               | 3569/5962 [00:19<00:51, 46.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 3558/5962 [00:19<00:30, 79.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 3700/5962 [00:19<00:33, 68.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▌                 | 3289/5962 [00:19<00:38, 69.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|██████████████████████▋                | 3459/5962 [00:19<00:32, 76.78it/s]\u001b[A\u001b[A\n",
      " 60%|███████████████████████▍               | 3577/5962 [00:19<00:46, 51.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▊                | 3481/5962 [00:19<00:43, 56.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3744/5962 [00:19<00:33, 66.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 3709/5962 [00:19<00:30, 72.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|███████████████████████▍               | 3591/5962 [00:19<00:34, 69.65it/s]\u001b[A\n",
      "\n",
      " 60%|███████████████████████▎               | 3568/5962 [00:19<00:31, 76.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▌                 | 3297/5962 [00:19<00:43, 60.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▊                | 3488/5962 [00:19<00:46, 52.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 3718/5962 [00:19<00:29, 75.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|███████████████████████▌               | 3602/5962 [00:19<00:30, 78.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▌              | 3752/5962 [00:19<00:39, 56.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▌                 | 3304/5962 [00:19<00:43, 60.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▉                | 3497/5962 [00:19<00:40, 60.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3728/5962 [00:19<00:28, 79.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|███████████████████████▍               | 3577/5962 [00:19<00:36, 65.75it/s]\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████▋              | 3765/5962 [00:19<00:31, 70.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▊                | 3494/5962 [00:19<00:27, 91.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▋                 | 3315/5962 [00:19<00:37, 71.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 3589/5962 [00:19<00:30, 76.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▋               | 3622/5962 [00:19<00:27, 84.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3744/5962 [00:19<00:23, 95.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▉                | 3505/5962 [00:19<00:25, 95.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████▋              | 3778/5962 [00:19<00:26, 81.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▋                 | 3323/5962 [00:19<00:36, 72.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████                | 3530/5962 [00:19<00:24, 97.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▉              | 3760/5962 [00:19<00:19, 111.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▊               | 3633/5962 [00:19<00:27, 84.42it/s]\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 3791/5962 [00:19<00:23, 91.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|███████████████████████▌               | 3598/5962 [00:19<00:34, 68.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▊                 | 3333/5962 [00:19<00:33, 79.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████▏               | 3541/5962 [00:19<00:24, 99.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3776/5962 [00:19<00:17, 124.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▊               | 3642/5962 [00:19<00:27, 83.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▉                 | 3345/5962 [00:19<00:29, 88.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 3802/5962 [00:19<00:23, 90.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 3553/5962 [00:19<00:23, 104.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████                | 3526/5962 [00:19<00:28, 85.19it/s]\u001b[A\u001b[A\n",
      " 60%|███████████████████████▌               | 3606/5962 [00:20<00:41, 56.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 3789/5962 [00:19<00:21, 99.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▉                 | 3355/5962 [00:20<00:30, 84.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████▏               | 3537/5962 [00:20<00:26, 90.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3812/5962 [00:20<00:25, 83.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 3564/5962 [00:20<00:26, 92.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▋               | 3615/5962 [00:20<00:37, 63.00it/s]\u001b[A\n",
      "\n",
      " 59%|███████████████████████▏               | 3547/5962 [00:20<00:26, 91.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3821/5962 [00:20<00:25, 84.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 3801/5962 [00:20<00:21, 98.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|██████████████████████                 | 3364/5962 [00:20<00:33, 76.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▊               | 3578/5962 [00:20<00:23, 100.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▋               | 3623/5962 [00:20<00:37, 62.59it/s]\u001b[A\n",
      "\n",
      " 60%|██████████████████████▋               | 3563/5962 [00:20<00:21, 109.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 3818/5962 [00:20<00:18, 115.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|█████████████████████████              | 3834/5962 [00:20<00:23, 90.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 3598/5962 [00:20<00:18, 124.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▋               | 3630/5962 [00:20<00:40, 57.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 3831/5962 [00:20<00:20, 105.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▊               | 3639/5962 [00:20<00:35, 65.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 3612/5962 [00:20<00:22, 102.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▌             | 3845/5962 [00:20<00:19, 111.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▏             | 3853/5962 [00:20<00:24, 84.44it/s]\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████▏              | 3698/5962 [00:20<00:26, 84.27it/s]\u001b[A\n",
      "\n",
      " 60%|██████████████████████▊               | 3588/5962 [00:20<00:23, 101.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 3651/5962 [00:20<00:29, 77.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▌             | 3858/5962 [00:20<00:18, 116.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▋               | 3624/5962 [00:20<00:23, 98.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████▎              | 3708/5962 [00:20<00:25, 88.28it/s]\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▎             | 3866/5962 [00:20<00:21, 95.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 3663/5962 [00:20<00:26, 88.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|███████████████████████▊              | 3729/5962 [00:20<00:18, 122.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 3635/5962 [00:20<00:23, 98.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 3871/5962 [00:20<00:18, 112.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████               | 3610/5962 [00:20<00:23, 101.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 3884/5962 [00:20<00:18, 110.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 3681/5962 [00:20<00:20, 110.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|███████████████████████▊              | 3745/5962 [00:20<00:16, 132.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 3657/5962 [00:20<00:18, 127.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 3885/5962 [00:20<00:17, 119.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 3898/5962 [00:20<00:17, 115.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▎                | 3419/5962 [00:20<00:27, 91.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▌              | 3697/5962 [00:21<00:19, 114.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 3899/5962 [00:20<00:16, 121.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|███████████████████████▉              | 3759/5962 [00:21<00:18, 121.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 3434/5962 [00:20<00:24, 105.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|████████████████████████▉             | 3910/5962 [00:21<00:19, 105.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 3671/5962 [00:20<00:22, 100.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████▉             | 3913/5962 [00:20<00:16, 122.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 3446/5962 [00:21<00:23, 106.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▊               | 3631/5962 [00:21<00:31, 73.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3923/5962 [00:21<00:18, 110.65it/s]\u001b[A\u001b[A\u001b[A\n",
      " 63%|████████████████████████              | 3772/5962 [00:21<00:20, 107.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3926/5962 [00:21<00:16, 122.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 3709/5962 [00:21<00:27, 80.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3941/5962 [00:21<00:15, 127.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▊               | 3640/5962 [00:21<00:30, 75.61it/s]\u001b[A\u001b[A\n",
      " 63%|████████████████████████              | 3784/5962 [00:21<00:20, 108.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3939/5962 [00:21<00:16, 122.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3683/5962 [00:21<00:28, 81.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▏               | 3477/5962 [00:21<00:20, 121.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▉               | 3650/5962 [00:21<00:28, 80.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 3955/5962 [00:21<00:16, 123.47it/s]\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████▎              | 3719/5962 [00:21<00:29, 76.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 3699/5962 [00:21<00:23, 96.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 3954/5962 [00:21<00:16, 122.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▏               | 3490/5962 [00:21<00:20, 121.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▎              | 3666/5962 [00:21<00:22, 100.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 3968/5962 [00:21<00:16, 123.86it/s]\u001b[A\u001b[A\u001b[A\n",
      " 63%|████████████████████████▍              | 3730/5962 [00:21<00:27, 82.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 3723/5962 [00:21<00:17, 127.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 3984/5962 [00:21<00:11, 169.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 3506/5962 [00:21<00:18, 131.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▍              | 3687/5962 [00:21<00:18, 125.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3745/5962 [00:21<00:22, 98.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 3745/5962 [00:21<00:14, 147.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▌            | 4008/5962 [00:21<00:10, 188.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▋              | 3707/5962 [00:21<00:15, 143.53it/s]\u001b[A\u001b[A\n",
      " 65%|████████████████████████▋             | 3869/5962 [00:21<00:11, 188.82it/s]\u001b[A\n",
      "\n",
      "\n",
      " 63%|███████████████████████▉              | 3764/5962 [00:21<00:18, 120.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3520/5962 [00:21<00:21, 115.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 4033/5962 [00:21<00:09, 205.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3778/5962 [00:21<00:18, 120.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 4026/5962 [00:21<00:15, 124.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|████████████████████████▎              | 3722/5962 [00:21<00:23, 96.11it/s]\u001b[A\u001b[A\n",
      " 65%|████████████████████████▊             | 3889/5962 [00:22<00:17, 116.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3778/5962 [00:21<00:20, 106.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████                | 3533/5962 [00:21<00:33, 72.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▊              | 3791/5962 [00:22<00:24, 86.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 4040/5962 [00:22<00:18, 103.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▊            | 4054/5962 [00:21<00:15, 120.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▏             | 3791/5962 [00:22<00:20, 105.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▊              | 3802/5962 [00:22<00:25, 83.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████▏               | 3543/5962 [00:22<00:35, 67.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|█████████████████████████▌             | 3905/5962 [00:22<00:21, 94.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▉            | 4071/5962 [00:22<00:17, 110.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4052/5962 [00:22<00:22, 83.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3804/5962 [00:22<00:22, 94.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████              | 3775/5962 [00:22<00:20, 106.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3812/5962 [00:22<00:30, 69.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████            | 4086/5962 [00:22<00:18, 101.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3815/5962 [00:22<00:24, 86.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4062/5962 [00:22<00:26, 71.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▏             | 3788/5962 [00:22<00:21, 101.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 3560/5962 [00:22<00:39, 60.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 3567/5962 [00:22<00:38, 62.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▉              | 3821/5962 [00:22<00:36, 59.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████              | 3825/5962 [00:22<00:31, 68.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 3575/5962 [00:22<00:36, 66.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 4099/5962 [00:22<00:24, 76.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 4071/5962 [00:22<00:32, 58.18it/s]\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▋             | 3918/5962 [00:22<00:36, 56.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 3587/5962 [00:22<00:30, 78.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████              | 3840/5962 [00:22<00:29, 71.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████              | 3833/5962 [00:22<00:33, 63.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 4082/5962 [00:22<00:28, 65.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▉              | 3811/5962 [00:22<00:29, 73.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▏             | 3851/5962 [00:23<00:27, 77.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 4094/5962 [00:22<00:24, 75.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████▏             | 3841/5962 [00:22<00:33, 62.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4119/5962 [00:22<00:24, 75.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 3862/5962 [00:23<00:24, 85.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▉              | 3820/5962 [00:23<00:30, 69.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4109/5962 [00:23<00:20, 90.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▏             | 3849/5962 [00:23<00:32, 65.16it/s]\n",
      " 66%|█████████████████████████▋             | 3928/5962 [00:23<00:42, 48.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4128/5962 [00:23<00:23, 77.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|█████████████████████████              | 3833/5962 [00:23<00:26, 80.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4120/5962 [00:23<00:20, 92.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▋               | 3627/5962 [00:23<00:25, 92.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 3872/5962 [00:23<00:29, 71.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▋             | 3936/5962 [00:23<00:41, 48.57it/s]\u001b[A\n",
      "\n",
      " 64%|█████████████████████████▏             | 3843/5962 [00:23<00:28, 75.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 3637/5962 [00:23<00:27, 85.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 4147/5962 [00:23<00:24, 75.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 3863/5962 [00:23<00:37, 56.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4131/5962 [00:23<00:26, 68.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 3646/5962 [00:23<00:28, 81.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 3881/5962 [00:23<00:34, 59.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▏             | 3852/5962 [00:23<00:30, 69.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 3869/5962 [00:23<00:38, 53.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 3888/5962 [00:23<00:34, 60.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▊             | 3943/5962 [00:23<00:51, 39.26it/s]\u001b[A\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4140/5962 [00:23<00:27, 66.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 3876/5962 [00:23<00:36, 57.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 4165/5962 [00:23<00:28, 64.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▊             | 3949/5962 [00:23<00:48, 41.92it/s]\u001b[A\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 3895/5962 [00:23<00:34, 59.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 3885/5962 [00:23<00:32, 64.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 3664/5962 [00:23<00:32, 70.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▊             | 3955/5962 [00:23<00:46, 42.93it/s]\u001b[A\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▌             | 3904/5962 [00:23<00:31, 64.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 3892/5962 [00:23<00:33, 62.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4172/5962 [00:23<00:31, 56.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▎             | 3868/5962 [00:23<00:40, 51.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3672/5962 [00:23<00:36, 62.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4168/5962 [00:23<00:22, 78.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▌             | 3900/5962 [00:23<00:31, 65.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▉             | 3961/5962 [00:24<00:47, 42.04it/s]\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▎             | 3875/5962 [00:24<00:37, 54.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████               | 3683/5962 [00:24<00:31, 72.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 3908/5962 [00:23<00:30, 68.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4186/5962 [00:23<00:29, 60.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4177/5962 [00:24<00:23, 74.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▍             | 3883/5962 [00:24<00:34, 59.84it/s]\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▌             | 3911/5962 [00:24<00:47, 43.29it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4194/5962 [00:24<00:27, 64.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4185/5962 [00:24<00:23, 74.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 3915/5962 [00:24<00:31, 64.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▍             | 3890/5962 [00:24<00:34, 60.63it/s]\u001b[A\u001b[A\n",
      " 67%|██████████████████████████             | 3977/5962 [00:24<00:36, 53.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4201/5962 [00:24<00:27, 63.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▍             | 3897/5962 [00:24<00:35, 58.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 3699/5962 [00:24<00:35, 62.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|██████████████████████████             | 3984/5962 [00:24<00:36, 54.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 3922/5962 [00:24<00:41, 49.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 3917/5962 [00:24<01:00, 33.76it/s]\u001b[A\u001b[A\u001b[A\n",
      " 67%|██████████████████████████             | 3990/5962 [00:24<00:38, 51.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 3706/5962 [00:24<00:41, 54.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4208/5962 [00:24<00:37, 46.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▌             | 3904/5962 [00:24<00:44, 46.53it/s]\u001b[A\u001b[A\n",
      " 67%|██████████████████████████▏            | 4000/5962 [00:24<00:32, 61.00it/s]\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 3922/5962 [00:24<01:01, 33.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 3928/5962 [00:24<00:50, 40.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 3712/5962 [00:24<00:49, 45.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4217/5962 [00:24<00:30, 57.05it/s]\u001b[A\u001b[A\u001b[A\n",
      " 67%|██████████████████████████▏            | 4007/5962 [00:24<00:38, 51.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4214/5962 [00:24<00:48, 36.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▌             | 3910/5962 [00:24<00:59, 34.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 3933/5962 [00:24<01:02, 32.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|██████████████████████████▎            | 4013/5962 [00:24<00:37, 52.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 3927/5962 [00:24<01:12, 28.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4223/5962 [00:24<00:36, 48.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 3937/5962 [00:24<01:01, 32.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 3722/5962 [00:25<00:57, 38.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▌             | 3915/5962 [00:25<01:02, 32.92it/s]\n",
      " 67%|██████████████████████████▎            | 4019/5962 [00:25<00:41, 47.28it/s]\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4229/5962 [00:25<00:35, 48.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 3937/5962 [00:25<01:01, 32.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4235/5962 [00:25<00:33, 51.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████▍              | 3727/5962 [00:25<00:56, 39.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 3952/5962 [00:25<00:40, 49.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▊             | 3945/5962 [00:25<00:48, 41.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4231/5962 [00:25<00:44, 38.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3736/5962 [00:25<00:43, 50.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4246/5962 [00:25<00:26, 64.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▋             | 3925/5962 [00:25<00:53, 37.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▉             | 3959/5962 [00:25<00:37, 53.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▊             | 3952/5962 [00:25<00:42, 46.78it/s]\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4255/5962 [00:25<00:24, 70.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4236/5962 [00:25<00:46, 36.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 3742/5962 [00:25<00:50, 44.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▋             | 3930/5962 [00:25<00:59, 34.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▉             | 3966/5962 [00:25<00:41, 48.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|██████████████████████████▍            | 4049/5962 [00:25<00:32, 58.50it/s]\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▉             | 3958/5962 [00:25<00:48, 41.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4240/5962 [00:25<00:48, 35.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▋             | 3934/5962 [00:25<01:00, 33.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▌              | 3747/5962 [00:25<00:53, 41.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▉             | 3972/5962 [00:25<00:39, 50.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|██████████████████████████▌            | 4056/5962 [00:25<00:32, 59.28it/s]\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4271/5962 [00:25<00:25, 66.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4248/5962 [00:25<00:38, 44.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▉             | 3963/5962 [00:25<00:54, 36.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▌              | 3752/5962 [00:25<00:56, 39.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▊             | 3940/5962 [00:25<00:59, 33.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4279/5962 [00:25<00:25, 66.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4255/5962 [00:25<00:34, 49.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|██████████████████████████▌            | 4063/5962 [00:25<00:33, 56.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4261/5962 [00:25<00:37, 45.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 4286/5962 [00:25<00:33, 49.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▌              | 3757/5962 [00:25<01:13, 29.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 3984/5962 [00:25<00:56, 34.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▉             | 3968/5962 [00:26<01:15, 26.33it/s]\u001b[A\u001b[A\n",
      " 68%|██████████████████████████▌            | 4070/5962 [00:26<00:46, 40.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4266/5962 [00:26<00:41, 40.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 4295/5962 [00:26<00:28, 57.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 3993/5962 [00:26<00:45, 43.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████████████████████████             | 3975/5962 [00:26<00:59, 33.33it/s]\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▋            | 4086/5962 [00:26<00:29, 62.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▌              | 3761/5962 [00:26<01:19, 27.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 4002/5962 [00:26<00:36, 53.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████████████████████████             | 3981/5962 [00:26<00:52, 37.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4302/5962 [00:26<00:40, 40.87it/s]\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▊            | 4096/5962 [00:26<00:34, 53.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4277/5962 [00:26<00:48, 34.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▋              | 3765/5962 [00:26<01:35, 23.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▉             | 3961/5962 [00:26<01:00, 32.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 4009/5962 [00:26<00:43, 45.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▉            | 4114/5962 [00:26<00:24, 76.83it/s]\u001b[A\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4308/5962 [00:26<00:39, 41.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 4285/5962 [00:26<00:39, 42.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▋              | 3779/5962 [00:26<00:51, 42.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▎            | 4020/5962 [00:26<00:33, 58.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 4001/5962 [00:26<00:38, 50.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 4295/5962 [00:26<00:30, 53.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4125/5962 [00:26<00:23, 77.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 4011/5962 [00:26<00:31, 62.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 4327/5962 [00:26<00:26, 60.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4307/5962 [00:26<00:23, 69.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████████████████████████             | 3993/5962 [00:26<00:28, 68.86it/s]\u001b[A\u001b[A\n",
      " 69%|███████████████████████████            | 4135/5962 [00:26<00:22, 80.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3806/5962 [00:26<00:29, 73.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▎            | 4021/5962 [00:26<00:27, 70.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▍          | 4338/5962 [00:26<00:22, 70.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 3816/5962 [00:26<00:27, 78.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|███████████████████████████            | 4146/5962 [00:26<00:21, 83.25it/s]\u001b[A\n",
      "\n",
      " 67%|██████████████████████████▏            | 4001/5962 [00:26<00:30, 63.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4315/5962 [00:26<00:27, 58.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▎            | 4030/5962 [00:27<00:25, 74.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 4048/5962 [00:26<00:31, 61.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████              | 3830/5962 [00:26<00:22, 92.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|███████████████████████████▏           | 4158/5962 [00:27<00:21, 84.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 4324/5962 [00:26<00:24, 65.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 4040/5962 [00:27<00:23, 81.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████▏             | 3841/5962 [00:27<00:21, 96.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|███████████████████████████▎           | 4168/5962 [00:27<00:21, 82.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 4332/5962 [00:27<00:26, 60.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4055/5962 [00:27<00:38, 49.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 4049/5962 [00:27<00:28, 66.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████████████████████████▎            | 4017/5962 [00:27<00:37, 52.53it/s]\u001b[A\u001b[A\n",
      " 70%|███████████████████████████▎           | 4177/5962 [00:27<00:22, 78.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4061/5962 [00:27<00:38, 49.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▏             | 3852/5962 [00:27<00:29, 72.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▋          | 4380/5962 [00:27<00:19, 79.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▍          | 4339/5962 [00:27<00:29, 55.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|███████████████████████████▍           | 4192/5962 [00:27<00:18, 95.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4069/5962 [00:27<00:34, 54.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▋          | 4390/5962 [00:27<00:18, 82.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4067/5962 [00:27<00:26, 72.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▍          | 4346/5962 [00:27<00:28, 55.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|███████████████████████████▍           | 4203/5962 [00:27<00:19, 90.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 4076/5962 [00:27<00:32, 57.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4402/5962 [00:27<00:17, 91.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████████████████████████▍            | 4041/5962 [00:27<00:30, 63.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 4358/5962 [00:27<00:23, 67.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 3878/5962 [00:27<00:24, 83.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▌           | 4214/5962 [00:27<00:19, 90.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▋            | 4085/5962 [00:27<00:29, 63.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████████████████████████▍            | 4048/5962 [00:27<00:30, 63.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▋            | 4084/5962 [00:27<00:27, 68.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 4366/5962 [00:27<00:24, 65.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 3891/5962 [00:27<00:23, 89.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 4092/5962 [00:27<00:30, 61.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▊            | 4094/5962 [00:27<00:24, 75.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4421/5962 [00:27<00:19, 79.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 4373/5962 [00:27<00:25, 62.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▋           | 4224/5962 [00:27<00:24, 71.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 4101/5962 [00:27<00:27, 67.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▊            | 4102/5962 [00:28<00:25, 72.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▋          | 4380/5962 [00:27<00:25, 61.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4430/5962 [00:28<00:21, 72.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 3911/5962 [00:27<00:25, 79.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▉            | 4111/5962 [00:28<00:24, 75.32it/s]\u001b[A\n",
      "\n",
      " 68%|██████████████████████████▋            | 4077/5962 [00:28<00:27, 68.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▋          | 4387/5962 [00:28<00:25, 62.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 3922/5962 [00:28<00:23, 85.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▋           | 4241/5962 [00:28<00:24, 69.84it/s]\u001b[A\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████          | 4438/5962 [00:28<00:22, 68.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4121/5962 [00:28<00:23, 78.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▋            | 4086/5962 [00:28<00:26, 72.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▋          | 4394/5962 [00:28<00:24, 63.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▊           | 4250/5962 [00:28<00:23, 71.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4130/5962 [00:28<00:26, 69.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4402/5962 [00:28<00:23, 67.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4130/5962 [00:28<00:24, 74.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▊            | 4094/5962 [00:28<00:28, 66.24it/s]\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▊           | 4259/5962 [00:28<00:22, 75.78it/s]\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▊            | 4102/5962 [00:28<00:26, 69.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4139/5962 [00:28<00:29, 60.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 3940/5962 [00:28<00:31, 64.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4411/5962 [00:28<00:27, 55.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4453/5962 [00:28<00:31, 47.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▉            | 4110/5962 [00:28<00:28, 65.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 4147/5962 [00:28<00:28, 63.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|███████████████████████████            | 4138/5962 [00:28<00:36, 49.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4421/5962 [00:28<00:24, 63.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4461/5962 [00:28<00:28, 52.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 3948/5962 [00:28<00:36, 55.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▉            | 4122/5962 [00:28<00:24, 74.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████            | 4146/5962 [00:28<00:33, 54.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▉           | 4280/5962 [00:28<00:26, 64.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4431/5962 [00:28<00:22, 66.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▉             | 3956/5962 [00:28<00:33, 60.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 4161/5962 [00:28<00:28, 63.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▏           | 4157/5962 [00:28<00:27, 64.58it/s]\u001b[A\u001b[A\n",
      " 72%|████████████████████████████           | 4292/5962 [00:28<00:22, 74.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 4442/5962 [00:28<00:20, 74.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4169/5962 [00:28<00:26, 68.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▎           | 4167/5962 [00:29<00:25, 70.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▉             | 3963/5962 [00:28<00:38, 52.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|████████████████████████████▏          | 4301/5962 [00:29<00:22, 73.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 4451/5962 [00:28<00:20, 74.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4488/5962 [00:29<00:21, 67.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4178/5962 [00:28<00:24, 71.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▎           | 4176/5962 [00:29<00:24, 73.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▉             | 3973/5962 [00:29<00:32, 61.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|████████████████████████████▏          | 4311/5962 [00:29<00:21, 78.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4459/5962 [00:29<00:20, 74.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4187/5962 [00:29<00:23, 74.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▎           | 4166/5962 [00:29<00:19, 90.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4185/5962 [00:29<00:22, 77.27it/s]\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▎          | 4323/5962 [00:29<00:18, 88.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 3983/5962 [00:29<00:29, 66.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4467/5962 [00:29<00:20, 73.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4195/5962 [00:29<00:21, 82.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▎          | 4335/5962 [00:29<00:16, 96.27it/s]\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▍         | 4503/5962 [00:29<00:25, 56.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 3991/5962 [00:29<00:29, 66.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4204/5962 [00:29<00:22, 76.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4475/5962 [00:29<00:21, 70.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▎           | 4176/5962 [00:29<00:24, 71.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 4510/5962 [00:29<00:24, 59.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 3999/5962 [00:29<00:30, 63.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▌           | 4204/5962 [00:29<00:26, 66.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4212/5962 [00:29<00:25, 68.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4483/5962 [00:29<00:22, 64.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▍          | 4346/5962 [00:29<00:20, 78.02it/s]\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 4519/5962 [00:29<00:22, 62.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 4006/5962 [00:29<00:30, 63.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▍           | 4193/5962 [00:29<00:24, 70.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4220/5962 [00:29<00:24, 70.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4212/5962 [00:29<00:28, 61.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▋         | 4531/5962 [00:29<00:18, 75.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▎            | 4014/5962 [00:29<00:30, 63.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▍           | 4201/5962 [00:29<00:25, 68.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4219/5962 [00:29<00:31, 55.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▍         | 4497/5962 [00:29<00:28, 50.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▍          | 4355/5962 [00:29<00:30, 52.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▎            | 4021/5962 [00:29<00:33, 58.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4228/5962 [00:29<00:34, 50.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▊         | 4552/5962 [00:29<00:15, 88.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▋           | 4225/5962 [00:29<00:31, 55.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▍         | 4503/5962 [00:29<00:28, 51.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▎            | 4029/5962 [00:29<00:30, 63.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4235/5962 [00:29<00:32, 53.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4234/5962 [00:30<00:29, 58.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 4036/5962 [00:30<00:30, 63.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4218/5962 [00:30<00:29, 59.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4249/5962 [00:30<00:24, 68.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▋           | 4241/5962 [00:30<00:28, 59.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 4049/5962 [00:30<00:23, 79.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 4521/5962 [00:30<00:23, 62.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▋           | 4225/5962 [00:30<00:29, 58.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4257/5962 [00:30<00:24, 69.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▉         | 4583/5962 [00:30<00:16, 85.85it/s]\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▌          | 4369/5962 [00:30<00:36, 44.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▋         | 4530/5962 [00:30<00:21, 67.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4248/5962 [00:30<00:32, 52.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4232/5962 [00:30<00:32, 52.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4265/5962 [00:30<00:29, 57.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 4067/5962 [00:30<00:26, 72.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4254/5962 [00:30<00:35, 47.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▋          | 4381/5962 [00:30<00:36, 43.37it/s]\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▋           | 4238/5962 [00:30<00:37, 46.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4272/5962 [00:30<00:33, 50.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4592/5962 [00:30<00:28, 47.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▋         | 4545/5962 [00:30<00:28, 48.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▊           | 4243/5962 [00:30<00:40, 42.70it/s]\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▊           | 4259/5962 [00:30<00:48, 34.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 4075/5962 [00:30<00:40, 46.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▊         | 4552/5962 [00:30<00:26, 53.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▋          | 4394/5962 [00:30<00:33, 46.35it/s]\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▊           | 4251/5962 [00:30<00:35, 48.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4599/5962 [00:30<00:32, 42.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 4286/5962 [00:30<00:33, 49.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4264/5962 [00:31<00:50, 33.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▊           | 4259/5962 [00:30<00:31, 53.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 4081/5962 [00:30<00:42, 43.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▊          | 4400/5962 [00:31<00:33, 46.02it/s]\u001b[A\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4605/5962 [00:31<00:32, 42.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4276/5962 [00:31<00:33, 50.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▉         | 4571/5962 [00:30<00:21, 66.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▋            | 4087/5962 [00:31<00:40, 46.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▉           | 4267/5962 [00:31<00:29, 58.40it/s]\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▊          | 4411/5962 [00:31<00:25, 60.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 4288/5962 [00:31<00:26, 63.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 4093/5962 [00:31<00:38, 48.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▉           | 4275/5962 [00:31<00:27, 62.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▉         | 4579/5962 [00:31<00:21, 65.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████▏        | 4611/5962 [00:31<00:32, 41.59it/s]\u001b[A\u001b[A\u001b[A\n",
      " 72%|████████████████████████████           | 4298/5962 [00:31<00:25, 66.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▉         | 4586/5962 [00:31<00:22, 61.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████▏        | 4616/5962 [00:31<00:33, 40.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4309/5962 [00:31<00:34, 47.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▏        | 4622/5962 [00:31<00:30, 43.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████           | 4282/5962 [00:31<00:37, 44.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 4100/5962 [00:31<00:49, 37.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4593/5962 [00:31<00:25, 53.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▉          | 4425/5962 [00:31<00:38, 39.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4318/5962 [00:31<00:31, 52.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4627/5962 [00:31<00:30, 43.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████           | 4292/5962 [00:31<00:31, 52.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 4306/5962 [00:31<00:36, 45.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4632/5962 [00:31<00:33, 40.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 4324/5962 [00:31<00:36, 44.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4114/5962 [00:31<00:45, 40.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████           | 4299/5962 [00:31<00:35, 46.70it/s]\u001b[A\u001b[A\n",
      " 72%|████████████████████████████▏          | 4312/5962 [00:31<00:39, 41.47it/s]\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4637/5962 [00:31<00:33, 40.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 4330/5962 [00:31<00:34, 47.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████▏        | 4611/5962 [00:31<00:26, 51.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 4119/5962 [00:31<00:45, 40.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▎          | 4322/5962 [00:31<00:31, 51.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4646/5962 [00:31<00:25, 51.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▍          | 4342/5962 [00:31<00:26, 61.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|█████████████████████████████          | 4435/5962 [00:32<00:49, 30.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 4332/5962 [00:32<00:26, 60.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▏          | 4311/5962 [00:32<00:34, 47.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4652/5962 [00:32<00:25, 50.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▍          | 4349/5962 [00:31<00:25, 63.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4625/5962 [00:31<00:23, 57.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4132/5962 [00:32<00:37, 49.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▍          | 4343/5962 [00:32<00:23, 70.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 4358/5962 [00:32<00:23, 68.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▏          | 4317/5962 [00:32<00:35, 45.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 4139/5962 [00:32<00:35, 51.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4658/5962 [00:32<00:29, 43.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████          | 4448/5962 [00:32<00:41, 36.35it/s]\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▎          | 4323/5962 [00:32<00:33, 48.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▍          | 4352/5962 [00:32<00:26, 60.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4644/5962 [00:32<00:19, 67.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████            | 4145/5962 [00:32<00:36, 50.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4666/5962 [00:32<00:26, 49.51it/s]\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████▏         | 4453/5962 [00:32<00:39, 38.63it/s]\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▎          | 4329/5962 [00:32<00:36, 44.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▋          | 4378/5962 [00:32<00:21, 73.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4674/5962 [00:32<00:22, 56.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 4362/5962 [00:32<00:24, 66.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4655/5962 [00:32<00:17, 74.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████▏         | 4460/5962 [00:32<00:34, 42.98it/s]\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▎          | 4334/5962 [00:32<00:35, 46.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▋          | 4389/5962 [00:32<00:19, 79.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▌        | 4681/5962 [00:32<00:22, 57.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 4157/5962 [00:32<00:35, 51.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▌          | 4370/5962 [00:32<00:26, 59.55it/s]\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▍          | 4340/5962 [00:32<00:33, 47.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4663/5962 [00:32<00:21, 59.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 4163/5962 [00:32<00:35, 50.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████▎         | 4476/5962 [00:32<00:26, 57.13it/s]\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4687/5962 [00:32<00:25, 50.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▍          | 4345/5962 [00:32<00:33, 48.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▋          | 4377/5962 [00:32<00:27, 58.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4169/5962 [00:32<00:34, 52.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4673/5962 [00:32<00:20, 63.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████▎         | 4483/5962 [00:32<00:25, 58.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4408/5962 [00:32<00:21, 73.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▋          | 4384/5962 [00:32<00:26, 59.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4693/5962 [00:32<00:27, 46.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 4178/5962 [00:32<00:28, 61.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4683/5962 [00:32<00:19, 67.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4416/5962 [00:32<00:20, 74.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▌          | 4363/5962 [00:32<00:24, 63.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4190/5962 [00:32<00:22, 77.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▋          | 4391/5962 [00:33<00:29, 53.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4693/5962 [00:32<00:18, 70.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4428/5962 [00:32<00:18, 80.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 4200/5962 [00:33<00:21, 82.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4698/5962 [00:33<00:34, 36.38it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▊          | 4403/5962 [00:33<00:23, 66.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▊        | 4703/5962 [00:33<00:16, 75.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████          | 4437/5962 [00:33<00:20, 75.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|█████████████████████████████▍         | 4503/5962 [00:33<00:26, 55.25it/s]\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4411/5962 [00:33<00:22, 68.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▊        | 4711/5962 [00:33<00:16, 74.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4209/5962 [00:33<00:27, 64.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▊        | 4708/5962 [00:33<00:31, 39.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 4720/5962 [00:33<00:16, 73.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 4445/5962 [00:33<00:24, 61.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▉          | 4419/5962 [00:33<00:27, 55.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▌           | 4217/5962 [00:33<00:29, 60.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▋          | 4392/5962 [00:33<00:29, 53.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▊        | 4713/5962 [00:33<00:33, 37.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 4728/5962 [00:33<00:17, 71.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|█████████████████████████████▋         | 4530/5962 [00:33<00:18, 77.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4426/5962 [00:33<00:27, 55.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 4722/5962 [00:33<00:25, 49.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 4736/5962 [00:33<00:17, 70.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▊          | 4399/5962 [00:33<00:31, 49.41it/s]\u001b[A\u001b[A\n",
      " 76%|█████████████████████████████▋         | 4539/5962 [00:33<00:17, 79.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4432/5962 [00:33<00:27, 56.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 4728/5962 [00:33<00:25, 48.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4231/5962 [00:33<00:30, 57.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4755/5962 [00:33<00:11, 101.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▊          | 4405/5962 [00:33<00:30, 50.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4466/5962 [00:33<00:24, 60.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|█████████████████████████████          | 4438/5962 [00:33<00:28, 53.70it/s]\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 4734/5962 [00:33<00:24, 50.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 4242/5962 [00:33<00:24, 69.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4774/5962 [00:33<00:09, 122.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▉          | 4421/5962 [00:33<00:20, 74.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4478/5962 [00:33<00:20, 73.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████▏         | 4453/5962 [00:33<00:20, 74.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 4254/5962 [00:33<00:21, 81.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▉          | 4431/5962 [00:33<00:19, 80.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4487/5962 [00:33<00:19, 75.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████        | 4746/5962 [00:34<00:22, 53.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████▎       | 4787/5962 [00:33<00:11, 99.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▊         | 4565/5962 [00:34<00:20, 67.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4264/5962 [00:34<00:20, 81.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|█████████████████████████████          | 4441/5962 [00:34<00:20, 74.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▍         | 4497/5962 [00:34<00:19, 76.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4461/5962 [00:34<00:25, 59.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 4276/5962 [00:34<00:18, 88.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▉         | 4573/5962 [00:34<00:22, 63.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████▍       | 4798/5962 [00:34<00:13, 86.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4455/5962 [00:34<00:16, 89.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▍         | 4506/5962 [00:34<00:18, 79.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4468/5962 [00:34<00:24, 60.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▎          | 4294/5962 [00:34<00:15, 110.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▉         | 4582/5962 [00:34<00:19, 69.50it/s]\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████▏         | 4467/5962 [00:34<00:15, 95.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 4516/5962 [00:34<00:17, 84.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 4812/5962 [00:34<00:12, 91.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4478/5962 [00:34<00:23, 64.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4590/5962 [00:34<00:20, 67.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 4525/5962 [00:34<00:17, 80.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4478/5962 [00:34<00:16, 89.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4485/5962 [00:34<00:22, 64.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████▍       | 4797/5962 [00:34<00:13, 84.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▌          | 4326/5962 [00:34<00:13, 121.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▉         | 4543/5962 [00:34<00:13, 105.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████▍         | 4494/5962 [00:34<00:20, 70.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4834/5962 [00:34<00:12, 92.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 4806/5962 [00:34<00:13, 83.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▋          | 4343/5962 [00:34<00:12, 132.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|██████████████████████████████▏        | 4608/5962 [00:34<00:18, 73.98it/s]\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▊         | 4513/5962 [00:34<00:11, 123.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████         | 4556/5962 [00:34<00:13, 104.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▍         | 4508/5962 [00:34<00:16, 87.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 4815/5962 [00:34<00:14, 80.46it/s]\u001b[A\u001b[A\u001b[A\n",
      " 77%|██████████████████████████████▏        | 4616/5962 [00:34<00:18, 71.09it/s]\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▉         | 4531/5962 [00:34<00:11, 125.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 4357/5962 [00:34<00:13, 117.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████         | 4568/5962 [00:34<00:13, 103.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▊       | 4857/5962 [00:34<00:11, 95.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4824/5962 [00:34<00:13, 82.33it/s]\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████         | 4566/5962 [00:34<00:07, 186.88it/s]\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▉         | 4544/5962 [00:34<00:10, 132.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 4870/5962 [00:34<00:10, 102.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4833/5962 [00:34<00:13, 81.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▉         | 4579/5962 [00:34<00:14, 92.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▌        | 4645/5962 [00:35<00:13, 100.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 4370/5962 [00:34<00:16, 97.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▋       | 4843/5962 [00:35<00:12, 86.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|█████████████████████████████         | 4558/5962 [00:35<00:12, 116.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 4592/5962 [00:34<00:13, 100.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4881/5962 [00:34<00:11, 94.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▋        | 4657/5962 [00:35<00:12, 103.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 4586/5962 [00:35<00:09, 138.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4603/5962 [00:35<00:13, 99.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▏        | 4571/5962 [00:35<00:12, 110.02it/s]\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▊        | 4668/5962 [00:35<00:12, 101.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4891/5962 [00:35<00:14, 74.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▋       | 4852/5962 [00:35<00:17, 61.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▋          | 4392/5962 [00:35<00:19, 79.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▉         | 4583/5962 [00:35<00:14, 96.55it/s]\u001b[A\u001b[A\n",
      " 78%|██████████████████████████████▌        | 4679/5962 [00:35<00:14, 89.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████████████████████████████       | 4900/5962 [00:35<00:13, 76.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 4603/5962 [00:35<00:13, 99.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4401/5962 [00:35<00:20, 74.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▎        | 4598/5962 [00:35<00:12, 105.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▊       | 4860/5962 [00:35<00:19, 55.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4928/5962 [00:35<00:08, 119.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████▋        | 4689/5962 [00:35<00:15, 84.23it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▍        | 4617/5962 [00:35<00:13, 100.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▍        | 4617/5962 [00:35<00:10, 123.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▊          | 4410/5962 [00:35<00:21, 73.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4876/5962 [00:35<00:14, 75.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 4950/5962 [00:35<00:07, 143.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████▋        | 4698/5962 [00:35<00:14, 84.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4639/5962 [00:35<00:15, 86.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4421/5962 [00:35<00:18, 81.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████▊        | 4710/5962 [00:35<00:13, 93.99it/s]\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4630/5962 [00:35<00:12, 106.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████████████████████████████       | 4892/5962 [00:35<00:13, 81.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 4966/5962 [00:35<00:07, 127.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4649/5962 [00:35<00:18, 72.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 4430/5962 [00:35<00:22, 69.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4642/5962 [00:35<00:15, 85.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|██████████████████████████████▎        | 4642/5962 [00:35<00:13, 95.96it/s]\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████▉        | 4720/5962 [00:35<00:17, 72.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4663/5962 [00:35<00:15, 86.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████          | 4439/5962 [00:35<00:20, 72.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▋      | 4980/5962 [00:35<00:09, 101.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4653/5962 [00:36<00:14, 88.37it/s]\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████▉        | 4735/5962 [00:36<00:13, 87.75it/s]\u001b[A\n",
      "\n",
      " 78%|██████████████████████████████▍        | 4653/5962 [00:36<00:14, 90.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 4450/5962 [00:36<00:18, 81.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4663/5962 [00:36<00:15, 86.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████▏      | 4923/5962 [00:36<00:12, 81.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████▋      | 4992/5962 [00:36<00:10, 92.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4663/5962 [00:36<00:14, 92.42it/s]\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▎       | 4752/5962 [00:36<00:11, 105.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4683/5962 [00:36<00:14, 86.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 4675/5962 [00:36<00:13, 93.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████▋      | 5004/5962 [00:36<00:09, 98.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████▎      | 4935/5962 [00:36<00:11, 87.99it/s]\u001b[A\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▍       | 4768/5962 [00:36<00:10, 117.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4694/5962 [00:36<00:13, 91.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4475/5962 [00:36<00:14, 99.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▋        | 4686/5962 [00:36<00:13, 96.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4695/5962 [00:36<00:10, 122.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████▎      | 4945/5962 [00:36<00:11, 87.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▊        | 4704/5962 [00:36<00:13, 90.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 4486/5962 [00:36<00:16, 90.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4698/5962 [00:36<00:12, 100.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|███████████████████████████████▎       | 4781/5962 [00:36<00:12, 96.51it/s]\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 4717/5962 [00:36<00:08, 147.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▊        | 4714/5962 [00:36<00:14, 84.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5050/5962 [00:36<00:07, 129.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 4709/5962 [00:36<00:12, 100.40it/s]\u001b[A\u001b[A\n",
      " 80%|███████████████████████████████▎       | 4792/5962 [00:36<00:12, 95.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▍         | 4496/5962 [00:36<00:16, 86.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 4976/5962 [00:36<00:09, 106.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5071/5962 [00:36<00:05, 151.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 4722/5962 [00:36<00:11, 105.64it/s]\u001b[A\u001b[A\n",
      " 81%|███████████████████████████████▍       | 4803/5962 [00:36<00:11, 98.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 4515/5962 [00:36<00:12, 112.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4753/5962 [00:36<00:09, 133.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4990/5962 [00:36<00:08, 113.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5087/5962 [00:36<00:05, 145.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4778/5962 [00:36<00:06, 172.88it/s]\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▋       | 4814/5962 [00:36<00:11, 100.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▏       | 4735/5962 [00:36<00:11, 110.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4768/5962 [00:36<00:09, 131.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4754/5962 [00:36<00:09, 132.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▊       | 4839/5962 [00:36<00:08, 137.85it/s]\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5003/5962 [00:36<00:09, 104.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████▌       | 4796/5962 [00:36<00:07, 153.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5103/5962 [00:36<00:06, 124.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4782/5962 [00:36<00:09, 130.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4773/5962 [00:37<00:08, 148.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 5027/5962 [00:37<00:06, 136.36it/s]\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▉       | 4854/5962 [00:37<00:08, 124.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5120/5962 [00:36<00:06, 134.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▌       | 4802/5962 [00:37<00:07, 147.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4812/5962 [00:37<00:08, 135.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▌       | 4789/5962 [00:37<00:09, 118.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5137/5962 [00:37<00:06, 120.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4818/5962 [00:37<00:09, 116.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 4604/5962 [00:37<00:12, 110.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 4802/5962 [00:37<00:11, 99.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|█████████████████████████████████      | 5057/5962 [00:37<00:09, 96.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▍        | 4619/5962 [00:37<00:11, 113.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4827/5962 [00:37<00:13, 84.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4831/5962 [00:37<00:11, 99.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████▊       | 4868/5962 [00:37<00:16, 67.60it/s]\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 4814/5962 [00:37<00:12, 92.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5166/5962 [00:37<00:07, 101.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|███████████████████████████████▋       | 4841/5962 [00:37<00:12, 91.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4634/5962 [00:37<00:12, 110.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5091/5962 [00:37<00:07, 122.24it/s]\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████▉       | 4879/5962 [00:37<00:14, 72.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4647/5962 [00:37<00:11, 113.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▉       | 4858/5962 [00:37<00:11, 100.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▊     | 5178/5962 [00:37<00:08, 92.12it/s]\n",
      " 82%|████████████████████████████████       | 4893/5962 [00:37<00:12, 84.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▋       | 4843/5962 [00:37<00:13, 80.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4825/5962 [00:37<00:14, 80.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 4662/5962 [00:37<00:11, 117.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5198/5962 [00:37<00:06, 114.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5134/5962 [00:37<00:05, 159.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 4676/5962 [00:37<00:10, 120.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▎      | 4920/5962 [00:37<00:10, 103.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▌       | 4834/5962 [00:38<00:16, 66.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5217/5962 [00:37<00:05, 127.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▊       | 4870/5962 [00:37<00:13, 79.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4693/5962 [00:37<00:09, 132.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|███████████████████████████████▋       | 4852/5962 [00:38<00:12, 87.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▊       | 4861/5962 [00:37<00:16, 67.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5170/5962 [00:38<00:05, 156.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4880/5962 [00:38<00:13, 77.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▊       | 4863/5962 [00:38<00:12, 89.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▌      | 4946/5962 [00:38<00:09, 101.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████▏    | 5231/5962 [00:38<00:07, 95.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 5187/5962 [00:38<00:05, 150.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▊       | 4869/5962 [00:38<00:16, 65.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4889/5962 [00:38<00:14, 76.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 4881/5962 [00:38<00:09, 109.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4876/5962 [00:38<00:17, 62.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████████████████████████████       | 4898/5962 [00:38<00:13, 77.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 4898/5962 [00:38<00:08, 124.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 83%|████████████████████████████████▍      | 4958/5962 [00:38<00:12, 82.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████▎    | 5243/5962 [00:38<00:08, 80.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████████████████████████████       | 4907/5962 [00:38<00:13, 80.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▉       | 4888/5962 [00:38<00:14, 73.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5203/5962 [00:38<00:07, 108.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4771/5962 [00:38<00:07, 157.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████▎      | 4915/5962 [00:38<00:08, 121.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████▎    | 5253/5962 [00:38<00:08, 82.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████████████████████████████▏      | 4919/5962 [00:38<00:11, 89.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████████████████████████████       | 4900/5962 [00:38<00:12, 82.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▌       | 4795/5962 [00:38<00:06, 173.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5222/5962 [00:38<00:06, 118.48it/s]\u001b[A\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████▊      | 5001/5962 [00:38<00:07, 126.08it/s]\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4939/5962 [00:38<00:09, 113.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████▍    | 5263/5962 [00:38<00:08, 80.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████████████████████████████       | 4909/5962 [00:38<00:13, 80.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████▏      | 4929/5962 [00:38<00:10, 99.03it/s]\u001b[A\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████▉      | 5016/5962 [00:38<00:07, 129.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4813/5962 [00:38<00:07, 155.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████▏      | 4922/5962 [00:38<00:11, 92.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████▍    | 5272/5962 [00:38<00:09, 74.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4942/5962 [00:38<00:09, 105.50it/s]\u001b[A\u001b[A\u001b[A\n",
      " 84%|████████████████████████████████      | 5032/5962 [00:38<00:06, 136.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 4830/5962 [00:38<00:07, 155.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████████████████████████████▍      | 4951/5962 [00:38<00:10, 92.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4935/5962 [00:38<00:10, 101.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████▌    | 5280/5962 [00:38<00:09, 74.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▏     | 5059/5962 [00:38<00:05, 168.49it/s]\u001b[A\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 4958/5962 [00:39<00:08, 113.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▉       | 4853/5962 [00:38<00:06, 170.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████████████████████████████▍      | 4962/5962 [00:38<00:10, 94.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 4971/5962 [00:39<00:08, 113.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████▌    | 5288/5962 [00:38<00:10, 66.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████████████████████████████▌      | 4973/5962 [00:39<00:10, 93.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 4871/5962 [00:39<00:06, 158.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5277/5962 [00:39<00:06, 105.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4996/5962 [00:39<00:06, 146.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████▋    | 5297/5962 [00:39<00:09, 71.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4991/5962 [00:39<00:08, 114.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 4889/5962 [00:39<00:06, 158.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5290/5962 [00:39<00:06, 109.24it/s]\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▍     | 5093/5962 [00:39<00:06, 142.59it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5012/5962 [00:39<00:06, 142.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5013/5962 [00:39<00:07, 134.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4906/5962 [00:39<00:06, 157.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5308/5962 [00:39<00:05, 124.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4994/5962 [00:39<00:07, 128.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 84%|████████████████████████████████      | 5034/5962 [00:39<00:05, 162.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5326/5962 [00:39<00:06, 105.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████      | 5027/5962 [00:39<00:06, 134.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4926/5962 [00:39<00:06, 167.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5323/5962 [00:39<00:04, 130.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5019/5962 [00:39<00:05, 161.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▏     | 5056/5962 [00:39<00:05, 175.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 5354/5962 [00:39<00:04, 149.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 4955/5962 [00:39<00:05, 200.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 5337/5962 [00:39<00:04, 131.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5047/5962 [00:39<00:04, 193.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▍     | 5080/5962 [00:39<00:04, 191.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5373/5962 [00:39<00:03, 158.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4987/5962 [00:39<00:04, 234.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5056/5962 [00:39<00:06, 135.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 5353/5962 [00:39<00:04, 138.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5069/5962 [00:39<00:04, 200.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████▌     | 5106/5962 [00:39<00:04, 209.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5392/5962 [00:39<00:03, 167.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5016/5962 [00:39<00:03, 247.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5370/5962 [00:39<00:04, 147.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5095/5962 [00:39<00:04, 216.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▎    | 5217/5962 [00:39<00:03, 235.80it/s]\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5132/5962 [00:39<00:03, 222.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5416/5962 [00:39<00:02, 183.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5386/5962 [00:39<00:04, 142.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5084/5962 [00:39<00:07, 122.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5043/5962 [00:39<00:04, 209.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5117/5962 [00:39<00:04, 181.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████▊     | 5155/5962 [00:39<00:03, 212.00it/s]\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5097/5962 [00:39<00:07, 120.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5067/5962 [00:39<00:04, 217.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5137/5962 [00:39<00:04, 183.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5177/5962 [00:40<00:03, 197.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5116/5962 [00:40<00:06, 138.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5090/5962 [00:40<00:04, 206.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████▎   | 5401/5962 [00:40<00:05, 93.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5131/5962 [00:40<00:06, 135.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5198/5962 [00:40<00:04, 173.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 5157/5962 [00:40<00:05, 143.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5112/5962 [00:40<00:04, 202.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▌    | 5266/5962 [00:40<00:05, 130.07it/s]\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5217/5962 [00:40<00:04, 174.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5465/5962 [00:40<00:04, 119.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5136/5962 [00:40<00:03, 211.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5235/5962 [00:40<00:04, 166.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▋    | 5284/5962 [00:40<00:05, 118.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5158/5962 [00:40<00:03, 207.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5162/5962 [00:40<00:06, 127.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5478/5962 [00:40<00:04, 109.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████▍   | 5424/5962 [00:40<00:06, 80.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5253/5962 [00:40<00:04, 168.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▊    | 5301/5962 [00:40<00:05, 127.31it/s]\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 5275/5962 [00:40<00:03, 180.56it/s]\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▉    | 5321/5962 [00:40<00:04, 141.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████▌   | 5434/5962 [00:40<00:07, 72.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████     | 5189/5962 [00:40<00:06, 124.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 5180/5962 [00:40<00:05, 153.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5294/5962 [00:40<00:03, 179.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████    | 5338/5962 [00:40<00:04, 143.14it/s]\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5209/5962 [00:40<00:05, 143.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████▌   | 5443/5962 [00:40<00:07, 73.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████▉   | 5501/5962 [00:40<00:05, 82.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 5263/5962 [00:40<00:04, 150.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5313/5962 [00:40<00:03, 170.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████▏   | 5355/5962 [00:40<00:04, 137.76it/s]\u001b[A\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████▋   | 5451/5962 [00:40<00:07, 71.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5333/5962 [00:41<00:03, 177.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5224/5962 [00:40<00:06, 111.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5280/5962 [00:40<00:04, 142.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████▏   | 5370/5962 [00:41<00:04, 139.07it/s]\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████▋   | 5465/5962 [00:40<00:05, 86.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████   | 5520/5962 [00:40<00:05, 86.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5373/5962 [00:41<00:02, 233.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5302/5962 [00:41<00:04, 154.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▍    | 5237/5962 [00:41<00:06, 111.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████▏  | 5534/5962 [00:41<00:04, 98.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5228/5962 [00:41<00:06, 113.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5326/5962 [00:41<00:03, 175.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5403/5962 [00:41<00:03, 147.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5397/5962 [00:41<00:02, 195.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 5348/5962 [00:41<00:03, 182.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 5505/5962 [00:41<00:04, 108.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5243/5962 [00:41<00:06, 106.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▌   | 5419/5962 [00:41<00:04, 130.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 5575/5962 [00:41<00:02, 134.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 5268/5962 [00:41<00:06, 107.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████▍    | 5255/5962 [00:41<00:07, 99.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|██████████████████████████████████▌    | 5280/5962 [00:41<00:06, 98.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5590/5962 [00:41<00:03, 113.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▌   | 5418/5962 [00:41<00:04, 126.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5368/5962 [00:41<00:04, 122.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|██████████████████████████████████▌    | 5291/5962 [00:41<00:07, 87.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████▏  | 5528/5962 [00:41<00:05, 73.65it/s]\u001b[A\u001b[A\u001b[A\n",
      " 91%|███████████████████████████████████▌   | 5445/5962 [00:41<00:05, 94.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████▌    | 5277/5962 [00:41<00:07, 94.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5435/5962 [00:41<00:04, 117.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|████████████████████████████████████▋  | 5603/5962 [00:41<00:04, 89.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|██████████████████████████████████▋    | 5306/5962 [00:41<00:06, 96.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████▏  | 5539/5962 [00:41<00:05, 79.30it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|██████████████████████████████████▊   | 5458/5962 [00:41<00:04, 101.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5450/5962 [00:41<00:04, 117.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|████████████████████████████████████▋  | 5614/5962 [00:41<00:03, 88.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5328/5962 [00:41<00:05, 124.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5399/5962 [00:41<00:05, 108.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████▎  | 5548/5962 [00:41<00:05, 81.18it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|██████████████████████████████████▉   | 5472/5962 [00:41<00:04, 109.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5478/5962 [00:42<00:03, 149.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████    | 5347/5962 [00:42<00:04, 140.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|████████████████████████████████████▊  | 5624/5962 [00:41<00:03, 87.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|██████████████████████████████████▉   | 5484/5962 [00:42<00:04, 112.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 5509/5962 [00:42<00:02, 185.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5371/5962 [00:42<00:03, 162.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▉  | 5638/5962 [00:42<00:03, 98.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5597/5962 [00:42<00:02, 149.08it/s]\u001b[A\u001b[A\u001b[A\n",
      " 92%|███████████████████████████████████   | 5504/5962 [00:42<00:03, 134.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5424/5962 [00:42<00:04, 109.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 5531/5962 [00:42<00:02, 187.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5397/5962 [00:42<00:03, 185.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5622/5962 [00:42<00:01, 173.52it/s]\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▏  | 5522/5962 [00:42<00:03, 143.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5440/5962 [00:42<00:04, 118.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5561/5962 [00:42<00:01, 214.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5427/5962 [00:42<00:02, 216.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5648/5962 [00:42<00:01, 195.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5675/5962 [00:42<00:02, 131.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▍  | 5560/5962 [00:42<00:01, 208.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5467/5962 [00:42<00:03, 155.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5591/5962 [00:42<00:01, 236.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5676/5962 [00:42<00:01, 216.39it/s]\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▌  | 5582/5962 [00:42<00:01, 208.53it/s]\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5450/5962 [00:42<00:02, 205.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5460/5962 [00:42<00:02, 249.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5484/5962 [00:42<00:03, 148.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5617/5962 [00:42<00:01, 240.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5703/5962 [00:42<00:01, 230.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5472/5962 [00:42<00:02, 208.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5486/5962 [00:42<00:01, 246.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▋  | 5604/5962 [00:42<00:01, 196.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5643/5962 [00:42<00:01, 245.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5702/5962 [00:42<00:02, 111.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5730/5962 [00:42<00:00, 240.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|███████████████████████████████████   | 5503/5962 [00:42<00:01, 235.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████▏  | 5512/5962 [00:42<00:01, 238.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5671/5962 [00:42<00:01, 250.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5757/5962 [00:42<00:00, 247.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5714/5962 [00:42<00:02, 107.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 5528/5962 [00:42<00:01, 224.71it/s]\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▊  | 5625/5962 [00:42<00:02, 154.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 5537/5962 [00:42<00:01, 235.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5697/5962 [00:42<00:01, 225.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5783/5962 [00:42<00:00, 243.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5726/5962 [00:42<00:02, 104.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5561/5962 [00:42<00:01, 215.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5551/5962 [00:42<00:02, 195.13it/s]\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▌ | 5729/5962 [00:43<00:00, 246.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5567/5962 [00:42<00:02, 155.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5739/5962 [00:42<00:02, 108.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████████████████████████████████ | 5808/5962 [00:43<00:00, 218.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 5583/5962 [00:43<00:01, 210.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 5584/5962 [00:43<00:02, 149.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████▌ | 5750/5962 [00:43<00:02, 98.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5605/5962 [00:43<00:01, 209.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5755/5962 [00:43<00:01, 185.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5604/5962 [00:43<00:02, 160.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▌  | 5572/5962 [00:43<00:02, 138.20it/s]\u001b[A\u001b[A\n",
      " 95%|████████████████████████████████████  | 5659/5962 [00:43<00:02, 104.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5762/5962 [00:43<00:01, 103.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5622/5962 [00:43<00:02, 163.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5627/5962 [00:43<00:01, 181.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5851/5962 [00:43<00:00, 166.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5777/5962 [00:43<00:01, 163.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████████████████████████████████  | 5672/5962 [00:43<00:03, 93.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5871/5962 [00:43<00:00, 169.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5790/5962 [00:43<00:01, 116.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5639/5962 [00:43<00:02, 137.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5646/5962 [00:43<00:02, 157.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5604/5962 [00:43<00:03, 118.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5796/5962 [00:43<00:01, 143.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5803/5962 [00:43<00:01, 116.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 5657/5962 [00:43<00:02, 144.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|█████████████████████████████████████▏ | 5683/5962 [00:43<00:03, 84.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 5665/5962 [00:43<00:01, 159.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▋| 5911/5962 [00:43<00:00, 178.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5618/5962 [00:43<00:03, 114.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5677/5962 [00:43<00:01, 158.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5815/5962 [00:43<00:01, 107.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5813/5962 [00:43<00:01, 127.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▊| 5933/5962 [00:43<00:00, 188.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5696/5962 [00:43<00:01, 166.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▉  | 5631/5962 [00:43<00:02, 111.04it/s]\u001b[A\u001b[A\n",
      " 96%|█████████████████████████████████████▎ | 5706/5962 [00:43<00:02, 92.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5826/5962 [00:43<00:01, 103.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5703/5962 [00:43<00:01, 152.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5828/5962 [00:43<00:01, 125.46it/s]\u001b[A\u001b[A\u001b[A\n",
      " 96%|█████████████████████████████████████▍ | 5716/5962 [00:43<00:02, 91.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:44<00:00, 135.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▉  | 5643/5962 [00:44<00:03, 90.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5842/5962 [00:44<00:01, 108.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5740/5962 [00:44<00:01, 154.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5719/5962 [00:44<00:02, 108.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████▉  | 5653/5962 [00:44<00:03, 90.56it/s]\u001b[A\u001b[A\n",
      " 98%|█████████████████████████████████████▎| 5855/5962 [00:44<00:00, 111.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████▏| 5846/5962 [00:44<00:01, 74.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████████████████████████████████  | 5666/5962 [00:44<00:03, 96.81it/s]\u001b[A\u001b[A\n",
      " 96%|█████████████████████████████████████▌ | 5735/5962 [00:44<00:03, 75.44it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5867/5962 [00:44<00:00, 102.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████████████████████████████████▏ | 5677/5962 [00:44<00:02, 97.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████▍ | 5732/5962 [00:44<00:02, 86.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 5881/5962 [00:44<00:00, 108.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████████████████████████████████▏ | 5688/5962 [00:44<00:02, 98.00it/s]\u001b[A\u001b[A\n",
      " 97%|█████████████████████████████████████▋ | 5758/5962 [00:44<00:02, 87.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5772/5962 [00:44<00:01, 110.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████▎| 5862/5962 [00:44<00:01, 63.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 5893/5962 [00:44<00:00, 106.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%|█████████████████████████████████████▊ | 5772/5962 [00:44<00:01, 99.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████▍| 5870/5962 [00:44<00:01, 66.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████████████████████████████████▎ | 5699/5962 [00:44<00:02, 88.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▋| 5910/5962 [00:44<00:00, 119.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████████████████████████████████▋ | 5754/5962 [00:44<00:02, 81.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 5787/5962 [00:44<00:01, 112.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|██████████████████████████████████████▍| 5878/5962 [00:44<00:01, 68.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5814/5962 [00:44<00:01, 143.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 5929/5962 [00:44<00:00, 137.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████████████████████████████████ | 5809/5962 [00:44<00:01, 140.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|██████████████████████████████████████▌| 5886/5962 [00:44<00:01, 68.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5834/5962 [00:44<00:00, 156.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████▉| 5946/5962 [00:44<00:00, 146.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5785/5962 [00:44<00:01, 106.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:44<00:00, 132.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5855/5962 [00:44<00:00, 169.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|██████████████████████████████████████▌| 5894/5962 [00:44<00:01, 67.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████████████████████████████████▌ | 5733/5962 [00:44<00:02, 98.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5798/5962 [00:44<00:01, 107.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 5881/5962 [00:44<00:00, 192.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████▋ | 5749/5962 [00:45<00:01, 114.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|██████████████████████████████████████▋| 5908/5962 [00:44<00:00, 83.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▍| 5874/5962 [00:45<00:00, 183.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5813/5962 [00:45<00:01, 115.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▋| 5907/5962 [00:45<00:00, 207.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5775/5962 [00:45<00:01, 154.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▊| 5933/5962 [00:45<00:00, 127.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▋| 5907/5962 [00:45<00:00, 225.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5831/5962 [00:45<00:01, 127.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5793/5962 [00:45<00:01, 159.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:45<00:00, 131.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 5929/5962 [00:45<00:00, 181.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▊| 5930/5962 [00:45<00:00, 213.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5845/5962 [00:45<00:00, 125.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████████████████████████████████ | 5810/5962 [00:45<00:01, 140.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▉| 5949/5962 [00:45<00:00, 173.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:45<00:00, 130.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:45<00:00, 131.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████▎| 5858/5962 [00:45<00:01, 99.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5848/5962 [00:45<00:00, 157.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5870/5962 [00:45<00:00, 100.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5870/5962 [00:45<00:00, 173.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 5888/5962 [00:45<00:00, 119.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 5903/5962 [00:45<00:00, 215.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 5925/5962 [00:45<00:00, 183.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 5926/5962 [00:46<00:00, 191.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:46<00:00, 129.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:46<00:00, 129.04it/s]\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_train.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa.tsv\n",
      "Processing Started...\n",
      "Data Size:  9202\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  1%|▎                                         | 9/1314 [00:00<00:21, 61.64it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                          | 1/1314 [00:00<04:59,  4.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "  1%|▍                                        | 16/1314 [00:00<00:19, 65.00it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                        | 15/1314 [00:00<00:23, 54.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  1%|▎                                       | 11/1314 [00:00<00:12, 102.80it/s]\u001b[A\u001b[A\n",
      "  3%|█▏                                      | 38/1314 [00:00<00:10, 124.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  3%|█                                        | 33/1314 [00:00<00:13, 98.05it/s]\n",
      "\n",
      "\n",
      "  3%|█▎                                      | 42/1314 [00:00<00:09, 132.67it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  2%|▊                                       | 25/1314 [00:00<00:10, 119.34it/s]\u001b[A\u001b[A\n",
      "  4%|█▌                                      | 51/1314 [00:00<00:11, 107.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 14/1314 [00:00<00:09, 135.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                      | 48/1314 [00:00<00:05, 217.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  4%|█▌                                      | 52/1314 [00:00<00:09, 128.10it/s]\n",
      "\n",
      "\n",
      "  5%|█▉                                      | 63/1314 [00:00<00:07, 156.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▌                                      | 51/1314 [00:00<00:07, 179.05it/s]\u001b[A\u001b[A\n",
      "  5%|██                                      | 69/1314 [00:00<00:09, 128.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▌                                       | 20/1314 [00:00<00:06, 194.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                      | 43/1314 [00:00<00:05, 223.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|██▏                                     | 71/1314 [00:00<00:08, 147.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▌                                     | 85/1314 [00:00<00:07, 175.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▍                                     | 80/1314 [00:00<00:05, 220.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▍                                     | 80/1314 [00:00<00:04, 288.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▉                                     | 97/1314 [00:00<00:07, 170.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                     | 90/1314 [00:00<00:08, 151.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 120/1314 [00:00<00:05, 220.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|███                                    | 103/1314 [00:00<00:05, 211.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 101/1314 [00:00<00:07, 156.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▊                                      | 59/1314 [00:00<00:09, 136.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 107/1314 [00:00<00:09, 133.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▍                                   | 115/1314 [00:00<00:09, 130.25it/s]\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 143/1314 [00:00<00:06, 183.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|███▋                                   | 125/1314 [00:00<00:07, 151.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▉                                     | 95/1314 [00:00<00:05, 205.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 133/1314 [00:01<00:07, 166.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|███▉                                   | 131/1314 [00:01<00:08, 135.13it/s]\u001b[A\n",
      "\n",
      "\n",
      " 13%|████▉                                  | 165/1314 [00:00<00:06, 190.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 133/1314 [00:00<00:06, 179.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|████▉                                  | 166/1314 [00:01<00:05, 207.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|████                                   | 137/1314 [00:00<00:07, 148.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 119/1314 [00:00<00:05, 205.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▎                                  | 146/1314 [00:01<00:08, 135.45it/s]\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 185/1314 [00:01<00:06, 185.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 155/1314 [00:00<00:06, 189.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▏                                 | 176/1314 [00:00<00:05, 190.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 143/1314 [00:00<00:05, 213.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 160/1314 [00:00<00:07, 164.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▌                                 | 189/1314 [00:01<00:05, 190.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 176/1314 [00:00<00:05, 190.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████                                 | 205/1314 [00:01<00:06, 170.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 169/1314 [00:00<00:05, 225.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 181/1314 [00:01<00:06, 175.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▊                                 | 197/1314 [00:01<00:06, 178.51it/s]\u001b[A\u001b[A\n",
      " 17%|██████▌                                | 220/1314 [00:01<00:04, 220.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████                                 | 204/1314 [00:00<00:05, 212.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|██████▋                                | 226/1314 [00:01<00:06, 174.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▉                                 | 200/1314 [00:01<00:06, 178.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▋                                 | 193/1314 [00:00<00:05, 216.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▎                               | 245/1314 [00:01<00:04, 227.70it/s]\u001b[A\u001b[A\n",
      " 15%|██████                                 | 203/1314 [00:01<00:07, 143.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                                | 227/1314 [00:01<00:05, 188.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▍                                | 216/1314 [00:01<00:05, 217.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                                | 219/1314 [00:01<00:06, 170.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|███████▉                               | 269/1314 [00:01<00:04, 227.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▏                               | 244/1314 [00:01<00:07, 144.01it/s]\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▌                                | 223/1314 [00:01<00:06, 157.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                               | 271/1314 [00:01<00:04, 250.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▏                               | 242/1314 [00:01<00:04, 228.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▎                              | 279/1314 [00:01<00:04, 238.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████                                | 237/1314 [00:01<00:06, 168.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 275/1314 [00:01<00:05, 181.59it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|███████                                | 240/1314 [00:01<00:07, 153.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▊                              | 299/1314 [00:01<00:03, 258.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▉                               | 266/1314 [00:01<00:04, 227.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▊                             | 330/1314 [00:01<00:03, 260.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▌                               | 255/1314 [00:01<00:06, 164.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▉                              | 301/1314 [00:01<00:05, 198.14it/s]\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▌                               | 256/1314 [00:01<00:07, 144.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▊                              | 295/1314 [00:01<00:04, 242.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████▋                            | 360/1314 [00:01<00:03, 265.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 277/1314 [00:01<00:05, 178.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▋                             | 327/1314 [00:01<00:04, 233.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|████████                               | 271/1314 [00:01<00:07, 141.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▉                             | 334/1314 [00:01<00:03, 282.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████                            | 374/1314 [00:01<00:03, 283.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▍                           | 387/1314 [00:02<00:03, 247.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██████████▎                            | 349/1314 [00:01<00:05, 190.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▊                            | 364/1314 [00:01<00:03, 283.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                            | 352/1314 [00:01<00:04, 197.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|████████▍                              | 286/1314 [00:02<00:07, 135.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▎                          | 413/1314 [00:02<00:03, 245.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|███████████▉                           | 403/1314 [00:01<00:03, 243.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 374/1314 [00:01<00:04, 195.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|████████▉                              | 303/1314 [00:02<00:07, 141.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 393/1314 [00:01<00:03, 262.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                             | 337/1314 [00:01<00:05, 187.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██████████▉                            | 370/1314 [00:02<00:05, 162.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|█████████████                          | 438/1314 [00:02<00:03, 226.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 395/1314 [00:01<00:04, 194.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▌                            | 356/1314 [00:02<00:05, 185.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▍                          | 420/1314 [00:01<00:03, 256.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████▍                             | 318/1314 [00:02<00:07, 125.44it/s]\u001b[A\n",
      "\n",
      " 35%|█████████████▌                         | 455/1314 [00:02<00:04, 200.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▋                         | 462/1314 [00:02<00:04, 181.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▏                           | 375/1314 [00:02<00:05, 159.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 25%|█████████▊                             | 331/1314 [00:02<00:08, 111.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▏                         | 446/1314 [00:02<00:04, 200.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████▎                        | 482/1314 [00:02<00:04, 184.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▉                          | 437/1314 [00:02<00:04, 178.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████▏                        | 477/1314 [00:02<00:04, 175.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 392/1314 [00:02<00:06, 151.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|██████████▏                            | 343/1314 [00:02<00:08, 112.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▉                         | 468/1314 [00:02<00:04, 195.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████▍                          | 421/1314 [00:02<00:06, 137.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▋                         | 460/1314 [00:02<00:04, 188.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▉                        | 502/1314 [00:02<00:04, 163.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 28%|██████████▊                            | 363/1314 [00:02<00:07, 131.74it/s]\u001b[A\n",
      "\n",
      " 38%|██████████████▊                        | 497/1314 [00:02<00:04, 172.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|████████████▉                          | 436/1314 [00:02<00:06, 137.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 485/1314 [00:02<00:04, 203.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▌                        | 489/1314 [00:02<00:04, 179.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▍                       | 521/1314 [00:02<00:04, 168.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▌                       | 525/1314 [00:02<00:04, 197.10it/s]\u001b[A\u001b[A\n",
      " 29%|███████████▏                           | 379/1314 [00:02<00:06, 136.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████▏                       | 510/1314 [00:02<00:03, 215.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                         | 454/1314 [00:02<00:04, 188.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████▎                      | 551/1314 [00:02<00:03, 200.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▊                           | 397/1314 [00:02<00:06, 146.67it/s]\u001b[A\n",
      "\n",
      " 42%|████████████████▏                      | 547/1314 [00:02<00:03, 194.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 478/1314 [00:02<00:04, 168.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▉                       | 538/1314 [00:02<00:03, 231.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████▎                     | 582/1314 [00:03<00:03, 229.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▉                       | 535/1314 [00:02<00:04, 191.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████▌                          | 423/1314 [00:03<00:05, 176.90it/s]\u001b[A\n",
      "\n",
      " 44%|█████████████████                      | 576/1314 [00:02<00:03, 216.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▊                        | 500/1314 [00:02<00:04, 202.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▌                      | 559/1314 [00:02<00:03, 203.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████▊                     | 601/1314 [00:02<00:03, 225.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|██████████████████                     | 607/1314 [00:03<00:03, 197.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|█████████████                          | 442/1314 [00:03<00:05, 154.61it/s]\u001b[A\n",
      "\n",
      "\n",
      " 39%|███████████████▎                       | 518/1314 [00:03<00:04, 175.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▍                       | 521/1314 [00:02<00:03, 199.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▍                     | 589/1314 [00:02<00:03, 225.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████▌                    | 625/1314 [00:03<00:03, 220.28it/s]\u001b[A\u001b[A\n",
      " 48%|██████████████████▊                    | 634/1314 [00:03<00:03, 210.62it/s]\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▉                       | 537/1314 [00:03<00:04, 172.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████████████████                       | 543/1314 [00:03<00:03, 199.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████▎                     | 583/1314 [00:02<00:04, 162.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|██████████████████▏                    | 613/1314 [00:02<00:03, 214.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|███████████████████▏                   | 648/1314 [00:03<00:03, 214.44it/s]\u001b[A\u001b[A\n",
      " 50%|███████████████████▌                   | 657/1314 [00:03<00:03, 211.85it/s]\u001b[A\n",
      "\n",
      "\n",
      " 42%|████████████████▍                      | 555/1314 [00:03<00:04, 167.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|██████████████████▏                    | 612/1314 [00:03<00:03, 190.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▋                      | 564/1314 [00:03<00:03, 188.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▉                    | 636/1314 [00:02<00:03, 215.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|████████████████████▏                  | 680/1314 [00:03<00:02, 214.82it/s]\u001b[A\u001b[A\n",
      " 38%|██████████████▊                        | 497/1314 [00:03<00:04, 165.23it/s]\u001b[A\n",
      "\n",
      "\n",
      " 44%|█████████████████▏                     | 580/1314 [00:03<00:03, 188.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▊                    | 634/1314 [00:03<00:03, 196.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▌                   | 660/1314 [00:03<00:02, 221.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████▊                  | 703/1314 [00:03<00:02, 237.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▊                  | 703/1314 [00:03<00:02, 210.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 39%|███████████████▎                       | 514/1314 [00:03<00:05, 154.67it/s]\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▊                     | 600/1314 [00:03<00:03, 185.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▎                  | 683/1314 [00:03<00:02, 223.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▍                   | 656/1314 [00:03<00:03, 199.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▉                     | 606/1314 [00:03<00:03, 188.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████▌                 | 725/1314 [00:03<00:02, 209.09it/s]\u001b[A\u001b[A\n",
      " 40%|███████████████▋                       | 530/1314 [00:03<00:05, 154.05it/s]\u001b[A\n",
      "\n",
      "\n",
      " 47%|██████████████████▍                    | 622/1314 [00:03<00:03, 195.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 695/1314 [00:03<00:02, 248.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▉                  | 706/1314 [00:03<00:02, 218.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▊                    | 635/1314 [00:03<00:03, 214.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▏                | 748/1314 [00:03<00:02, 212.51it/s]\u001b[A\u001b[A\n",
      " 42%|████████████████▎                      | 550/1314 [00:03<00:04, 165.99it/s]\u001b[A\n",
      "\n",
      "\n",
      " 49%|███████████████████                    | 642/1314 [00:03<00:03, 193.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▍                 | 722/1314 [00:03<00:02, 250.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▋                 | 732/1314 [00:03<00:02, 227.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▋                   | 663/1314 [00:03<00:02, 232.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▊                | 770/1314 [00:03<00:02, 211.71it/s]\u001b[A\u001b[A\n",
      " 44%|█████████████████▎                     | 584/1314 [00:03<00:03, 213.02it/s]\u001b[A\n",
      "\n",
      "\n",
      " 50%|███████████████████▋                   | 662/1314 [00:03<00:03, 193.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▍                | 755/1314 [00:03<00:02, 228.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▏                | 748/1314 [00:03<00:02, 239.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▍                  | 688/1314 [00:03<00:02, 230.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|███████████████████████▌               | 793/1314 [00:04<00:02, 216.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|████████████████████▏                  | 682/1314 [00:04<00:03, 194.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████                | 778/1314 [00:03<00:02, 215.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▊              | 836/1314 [00:03<00:01, 247.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▉                | 773/1314 [00:03<00:02, 218.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▉                     | 606/1314 [00:04<00:04, 165.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▏                 | 712/1314 [00:03<00:03, 173.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▋               | 800/1314 [00:03<00:03, 169.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████▊                  | 702/1314 [00:04<00:04, 141.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▌             | 861/1314 [00:04<00:02, 204.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 835/1314 [00:04<00:02, 179.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▋               | 796/1314 [00:03<00:02, 176.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▌                    | 625/1314 [00:04<00:04, 139.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▋              | 830/1314 [00:03<00:02, 200.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████████████████████████▎            | 887/1314 [00:04<00:01, 218.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████████████████████▍                 | 721/1314 [00:04<00:03, 151.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 862/1314 [00:04<00:02, 200.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 823/1314 [00:04<00:02, 196.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|███████████████████▏                   | 645/1314 [00:04<00:04, 152.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 852/1314 [00:04<00:02, 201.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▉                 | 739/1314 [00:04<00:03, 158.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|███████████████████████████            | 910/1314 [00:04<00:01, 219.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████              | 845/1314 [00:04<00:02, 200.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 791/1314 [00:04<00:02, 208.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|██████████████████████████▏            | 883/1314 [00:04<00:02, 170.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 877/1314 [00:04<00:02, 214.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▉                | 771/1314 [00:04<00:02, 198.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▋           | 933/1314 [00:04<00:01, 212.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 870/1314 [00:04<00:02, 211.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 815/1314 [00:04<00:02, 215.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|████████████████████▎                  | 684/1314 [00:04<00:03, 165.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 905/1314 [00:04<00:01, 232.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 805/1314 [00:04<00:02, 235.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▏             | 850/1314 [00:04<00:01, 249.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 902/1314 [00:04<00:02, 145.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 937/1314 [00:04<00:01, 256.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 835/1314 [00:04<00:01, 253.02it/s]\u001b[A\u001b[A\u001b[A\n",
      " 53%|████████████████████▊                  | 702/1314 [00:04<00:04, 140.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 915/1314 [00:04<00:01, 200.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 964/1314 [00:04<00:01, 257.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 876/1314 [00:04<00:02, 218.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▏           | 918/1314 [00:04<00:03, 126.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 862/1314 [00:04<00:02, 215.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▉           | 943/1314 [00:04<00:01, 221.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████         | 1005/1314 [00:04<00:01, 301.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|█████████████████████▎                 | 718/1314 [00:05<00:04, 126.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 900/1314 [00:04<00:01, 211.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████           | 946/1314 [00:05<00:02, 159.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 976/1314 [00:04<00:01, 251.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 889/1314 [00:05<00:01, 227.88it/s]\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▉                 | 740/1314 [00:05<00:03, 147.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 925/1314 [00:04<00:01, 220.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 1036/1314 [00:04<00:01, 272.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████          | 980/1314 [00:05<00:01, 200.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▍        | 1020/1314 [00:04<00:00, 304.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 914/1314 [00:05<00:01, 224.72it/s]\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▊                | 770/1314 [00:05<00:02, 183.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 949/1314 [00:04<00:01, 222.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 1065/1314 [00:04<00:00, 270.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▏        | 1011/1314 [00:05<00:01, 227.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 1066/1314 [00:04<00:00, 347.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 938/1314 [00:05<00:01, 228.01it/s]\u001b[A\u001b[A\u001b[A\n",
      " 60%|███████████████████████▌               | 792/1314 [00:05<00:02, 188.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 980/1314 [00:05<00:01, 245.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████▉        | 1037/1314 [00:05<00:01, 228.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 964/1314 [00:05<00:01, 236.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 1093/1314 [00:04<00:00, 232.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████▏              | 813/1314 [00:05<00:02, 190.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 1102/1314 [00:05<00:00, 302.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████         | 1006/1314 [00:05<00:01, 230.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████      | 1110/1314 [00:05<00:00, 234.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 1118/1314 [00:05<00:00, 233.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▎         | 989/1314 [00:05<00:01, 221.66it/s]\u001b[A\u001b[A\u001b[A\n",
      " 64%|████████████████████████▉              | 841/1314 [00:05<00:02, 212.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 1134/1314 [00:05<00:00, 282.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▊        | 1032/1314 [00:05<00:01, 233.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████▌      | 1093/1314 [00:05<00:00, 245.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 1013/1314 [00:05<00:01, 225.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 1143/1314 [00:05<00:00, 233.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▋             | 867/1314 [00:05<00:02, 220.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 1164/1314 [00:05<00:00, 282.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████    | 1179/1314 [00:05<00:00, 290.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 1119/1314 [00:05<00:00, 232.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|██████████████████████████▍            | 890/1314 [00:05<00:02, 189.10it/s]\u001b[A\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 1037/1314 [00:05<00:01, 188.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 1082/1314 [00:05<00:01, 209.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 1167/1314 [00:05<00:00, 183.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 1194/1314 [00:05<00:00, 235.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 1209/1314 [00:05<00:00, 230.19it/s]\u001b[A\u001b[A\n",
      " 69%|███████████████████████████            | 913/1314 [00:05<00:02, 199.11it/s]\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 1060/1314 [00:05<00:01, 198.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 1124/1314 [00:05<00:00, 261.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 1166/1314 [00:05<00:00, 204.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 1188/1314 [00:05<00:00, 171.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 1081/1314 [00:05<00:01, 201.00it/s]\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▋           | 934/1314 [00:06<00:01, 195.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 1235/1314 [00:05<00:00, 204.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 1190/1314 [00:06<00:00, 209.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 1207/1314 [00:05<00:00, 165.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 1106/1314 [00:06<00:00, 212.07it/s]\u001b[A\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▍          | 957/1314 [00:06<00:01, 196.20it/s]\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 1258/1314 [00:05<00:00, 190.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 1288/1314 [00:05<00:00, 243.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|███████████████████████████████████   | 1212/1314 [00:06<00:00, 189.68it/s]\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 1128/1314 [00:06<00:00, 195.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 1181/1314 [00:05<00:00, 208.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 1225/1314 [00:05<00:00, 148.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:05<00:00, 222.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 1232/1314 [00:06<00:00, 188.16it/s]\u001b[A\u001b[A\n",
      " 76%|█████████████████████████████         | 1004/1314 [00:06<00:01, 199.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 1244/1314 [00:05<00:00, 157.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 1149/1314 [00:06<00:00, 184.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 1205/1314 [00:06<00:00, 185.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:06<00:00, 211.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████▊        | 1033/1314 [00:06<00:01, 221.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▋ | 1268/1314 [00:05<00:00, 174.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 1168/1314 [00:06<00:00, 182.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 1285/1314 [00:06<00:00, 225.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▋       | 1063/1314 [00:06<00:01, 239.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 1290/1314 [00:06<00:00, 185.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 1201/1314 [00:06<00:00, 220.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:06<00:00, 197.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████▋      | 1095/1314 [00:06<00:00, 262.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:06<00:00, 212.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 1236/1314 [00:06<00:00, 255.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 1282/1314 [00:06<00:00, 222.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████▋     | 1129/1314 [00:06<00:00, 281.41it/s]\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 1263/1314 [00:06<00:00, 238.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:06<00:00, 200.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 1158/1314 [00:06<00:00, 272.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:06<00:00, 191.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 1209/1314 [00:06<00:00, 338.99it/s]\u001b[A\n",
      " 95%|████████████████████████████████████  | 1249/1314 [00:07<00:00, 355.48it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:07<00:00, 180.89it/s]\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb.tsv\n",
      "Processing Started...\n",
      "Data Size:  18406\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 27/2629 [00:00<00:09, 265.84it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  1%|▎                                       | 23/2629 [00:00<00:11, 226.73it/s]\u001b[A\n",
      "\n",
      "  1%|▎                                       | 23/2629 [00:00<00:11, 226.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▎                                       | 20/2629 [00:00<00:13, 199.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|▋                                       | 49/2629 [00:00<00:10, 244.46it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏                                       | 16/2629 [00:00<00:21, 122.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|▊                                       | 54/2629 [00:00<00:09, 274.74it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  2%|▊                                       | 54/2629 [00:00<00:14, 174.52it/s]\n",
      "\n",
      "\n",
      "  2%|▌                                       | 40/2629 [00:00<00:15, 163.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                       | 12/2629 [00:00<00:23, 111.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                         | 8/2629 [00:00<00:37, 69.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|█▏                                      | 74/2629 [00:00<00:11, 215.58it/s]\u001b[A\n",
      "\n",
      "  3%|█▏                                      | 82/2629 [00:00<00:10, 238.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 24/2629 [00:00<00:23, 109.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                        | 19/2629 [00:00<00:28, 90.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                       | 53/2629 [00:00<00:15, 166.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|█▍                                     | 101/2629 [00:00<00:10, 231.54it/s]\u001b[A\n",
      "\n",
      "  4%|█▋                                     | 110/2629 [00:00<00:09, 252.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 74/2629 [00:00<00:18, 140.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 45/2629 [00:00<00:17, 150.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 79/2629 [00:00<00:12, 201.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 43/2629 [00:00<00:17, 152.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  5%|██                                     | 135/2629 [00:00<00:09, 267.80it/s]\u001b[A\n",
      "\n",
      "  6%|██▏                                    | 146/2629 [00:00<00:08, 287.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▍                                      | 97/2629 [00:00<00:15, 165.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▉                                       | 64/2629 [00:00<00:15, 163.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 111/2629 [00:00<00:10, 242.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▉                                       | 64/2629 [00:00<00:14, 172.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 120/2629 [00:00<00:10, 229.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "  6%|██▍                                    | 163/2629 [00:00<00:09, 252.66it/s]\u001b[A\n",
      "\n",
      "  4%|█▋                                     | 116/2629 [00:00<00:16, 152.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                      | 88/2629 [00:00<00:13, 184.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|██                                     | 136/2629 [00:00<00:10, 231.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 82/2629 [00:00<00:16, 155.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▊                                    | 193/2629 [00:00<00:09, 266.51it/s]\u001b[A\n",
      "\n",
      "  8%|███                                    | 205/2629 [00:00<00:09, 262.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|██                                     | 136/2629 [00:00<00:15, 164.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 111/2629 [00:00<00:12, 194.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▍                                    | 165/2629 [00:00<00:09, 247.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▎                                   | 225/2629 [00:00<00:08, 281.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                      | 98/2629 [00:00<00:16, 153.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▍                                    | 163/2629 [00:00<00:12, 190.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 114/2629 [00:00<00:17, 147.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 131/2629 [00:00<00:16, 153.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▍                                   | 232/2629 [00:01<00:13, 182.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▊                                    | 191/2629 [00:00<00:13, 186.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▋                                    | 184/2629 [00:01<00:15, 162.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 131/2629 [00:00<00:16, 151.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 149/2629 [00:00<00:15, 160.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|██▉                                    | 196/2629 [00:01<00:14, 171.53it/s]\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▏                                  | 279/2629 [00:01<00:10, 222.94it/s]\u001b[A\n",
      "\n",
      " 10%|███▊                                   | 254/2629 [00:01<00:13, 179.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███                                    | 206/2629 [00:01<00:13, 175.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 212/2629 [00:01<00:14, 172.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▌                                    | 169/2629 [00:01<00:14, 170.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▎                                   | 220/2629 [00:01<00:12, 186.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|████                                   | 275/2629 [00:01<00:12, 183.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 233/2629 [00:01<00:12, 199.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 231/2629 [00:01<00:14, 166.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▊                                    | 187/2629 [00:01<00:14, 166.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 12%|████▌                                  | 304/2629 [00:01<00:11, 198.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 241/2629 [00:01<00:13, 178.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▍                                  | 299/2629 [00:01<00:11, 196.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 198/2629 [00:01<00:11, 204.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 255/2629 [00:01<00:12, 186.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▎                                   | 223/2629 [00:01<00:11, 216.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|████▉                                  | 331/2629 [00:01<00:10, 212.93it/s]\u001b[A\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 266/2629 [00:01<00:12, 194.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|████▊                                  | 327/2629 [00:01<00:10, 217.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 219/2629 [00:01<00:12, 198.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████                                   | 278/2629 [00:01<00:11, 197.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 255/2629 [00:01<00:09, 244.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▌                                 | 372/2629 [00:01<00:08, 261.03it/s]\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▎                                  | 291/2629 [00:01<00:11, 203.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▏                                 | 351/2629 [00:01<00:11, 206.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 310/2629 [00:01<00:10, 229.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|█████▉                                 | 401/2629 [00:01<00:08, 266.70it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 305/2629 [00:01<00:11, 197.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 319/2629 [00:01<00:10, 222.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 281/2629 [00:01<00:10, 220.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▌                                 | 373/2629 [00:01<00:10, 208.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 349/2629 [00:01<00:08, 273.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▌                                | 443/2629 [00:01<00:07, 308.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 338/2629 [00:01<00:09, 232.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 363/2629 [00:01<00:08, 281.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 308/2629 [00:01<00:10, 231.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▊                                 | 395/2629 [00:01<00:11, 202.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 378/2629 [00:01<00:08, 266.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 18%|███████                                | 476/2629 [00:01<00:07, 294.52it/s]\u001b[A\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 395/2629 [00:01<00:07, 291.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 363/2629 [00:01<00:10, 222.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|████▉                                  | 332/2629 [00:01<00:10, 218.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▎                                | 423/2629 [00:01<00:09, 221.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 416/2629 [00:01<00:07, 295.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▌                               | 507/2629 [00:01<00:07, 294.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 390/2629 [00:01<00:09, 233.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▌                                | 446/2629 [00:02<00:09, 221.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 355/2629 [00:01<00:11, 202.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                                | 447/2629 [00:02<00:07, 284.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|████████                               | 542/2629 [00:02<00:06, 308.62it/s]\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▊                                | 462/2629 [00:02<00:07, 294.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 414/2629 [00:02<00:10, 209.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████▉                                | 471/2629 [00:02<00:09, 228.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 377/2629 [00:01<00:08, 257.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 379/2629 [00:01<00:10, 208.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 18%|███████                                | 477/2629 [00:02<00:07, 281.01it/s]\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▎                               | 493/2629 [00:02<00:07, 273.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                                | 436/2629 [00:02<00:10, 202.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▎                               | 495/2629 [00:02<00:09, 227.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████                                 | 410/2629 [00:02<00:09, 234.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▉                                 | 403/2629 [00:02<00:09, 240.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▋                               | 521/2629 [00:02<00:09, 219.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▊                                | 457/2629 [00:02<00:12, 167.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                                | 435/2629 [00:02<00:11, 198.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|███████▋                               | 518/2629 [00:02<00:11, 187.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 552/2629 [00:02<00:08, 248.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▊                                | 461/2629 [00:02<00:10, 212.76it/s]\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████                                | 475/2629 [00:02<00:12, 168.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▏                              | 548/2629 [00:02<00:09, 213.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|████████                               | 545/2629 [00:02<00:10, 206.65it/s]\u001b[A\u001b[A\u001b[A\n",
      " 26%|█████████▉                             | 673/2629 [00:02<00:07, 246.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▌                              | 580/2629 [00:02<00:08, 238.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                               | 495/2629 [00:02<00:12, 175.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▍                              | 572/2629 [00:02<00:09, 217.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▏                               | 484/2629 [00:02<00:10, 210.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|████████▌                              | 575/2629 [00:02<00:08, 228.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                            | 700/2629 [00:02<00:07, 246.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▌                               | 514/2629 [00:02<00:12, 175.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▌                               | 506/2629 [00:02<00:10, 209.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▉                              | 606/2629 [00:02<00:09, 207.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▉                              | 600/2629 [00:02<00:09, 204.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                               | 490/2629 [00:02<00:12, 177.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 28%|██████████▊                            | 726/2629 [00:02<00:08, 232.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                               | 541/2629 [00:02<00:10, 199.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                               | 528/2629 [00:02<00:10, 203.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▏                             | 623/2629 [00:02<00:09, 221.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▎                             | 629/2629 [00:02<00:10, 197.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▋                               | 515/2629 [00:02<00:10, 195.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▎                              | 563/2629 [00:02<00:10, 204.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|███████████▏                           | 752/2629 [00:02<00:07, 237.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 554/2629 [00:02<00:09, 215.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▋                             | 649/2629 [00:02<00:08, 230.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|█████████▊                             | 662/2629 [00:03<00:08, 224.87it/s]\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▋                           | 789/2629 [00:03<00:06, 271.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                               | 545/2629 [00:02<00:09, 219.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▋                              | 584/2629 [00:03<00:10, 186.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▌                              | 576/2629 [00:02<00:10, 204.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▏                          | 818/2629 [00:03<00:06, 274.58it/s]\u001b[A\n",
      "\n",
      " 26%|██████████▏                            | 686/2629 [00:03<00:09, 211.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                              | 568/2629 [00:03<00:10, 194.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▉                              | 604/2629 [00:03<00:11, 179.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████▌                          | 847/2629 [00:03<00:06, 275.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▌                            | 709/2629 [00:03<00:09, 206.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██████████▌                            | 713/2629 [00:03<00:09, 212.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▉                              | 599/2629 [00:03<00:09, 223.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▍                             | 635/2629 [00:03<00:09, 212.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|█████████████                          | 880/2629 [00:03<00:06, 290.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▉                            | 737/2629 [00:03<00:08, 224.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██████████▉                            | 739/2629 [00:03<00:08, 223.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▉                            | 735/2629 [00:03<00:08, 227.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▋                             | 657/2629 [00:03<00:09, 204.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                             | 648/2629 [00:03<00:09, 199.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|███████████▎                           | 761/2629 [00:03<00:08, 226.69it/s]\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████▎                           | 763/2629 [00:03<00:08, 227.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▎                           | 760/2629 [00:03<00:08, 217.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                             | 678/2629 [00:03<00:09, 198.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 787/2629 [00:03<00:07, 231.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                             | 647/2629 [00:03<00:10, 184.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 787/2629 [00:03<00:08, 223.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|███████████▋                           | 784/2629 [00:03<00:08, 223.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▋                            | 720/2629 [00:03<00:07, 255.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                            | 697/2629 [00:03<00:08, 218.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|████████████                           | 811/2629 [00:03<00:08, 226.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▉                             | 668/2629 [00:03<00:10, 185.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|████████████                           | 811/2629 [00:03<00:08, 226.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|███████████▉                           | 808/2629 [00:03<00:08, 217.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▋                            | 720/2629 [00:03<00:08, 217.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████▍                          | 837/2629 [00:03<00:07, 234.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▎                            | 696/2629 [00:03<00:09, 207.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████▌                          | 845/2629 [00:03<00:06, 257.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 747/2629 [00:03<00:09, 194.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▎                          | 831/2629 [00:03<00:08, 209.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 748/2629 [00:03<00:08, 232.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▊                          | 862/2629 [00:03<00:07, 236.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|█████████████                          | 880/2629 [00:03<00:06, 282.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▋                          | 853/2629 [00:04<00:09, 181.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▏                         | 886/2629 [00:04<00:08, 194.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▉                            | 740/2629 [00:03<00:11, 166.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▍                         | 909/2629 [00:04<00:07, 220.67it/s]\u001b[A\u001b[A\u001b[A\n",
      " 40%|███████████████▏                      | 1052/2629 [00:04<00:07, 198.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▍                           | 772/2629 [00:03<00:11, 165.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 790/2629 [00:04<00:11, 167.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████▉                          | 872/2629 [00:04<00:11, 158.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|█████████████▊                         | 934/2629 [00:04<00:07, 221.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▎                           | 759/2629 [00:04<00:11, 162.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▋                           | 792/2629 [00:04<00:10, 172.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 907/2629 [00:04<00:12, 135.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 958/2629 [00:04<00:08, 190.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████                           | 812/2629 [00:04<00:11, 151.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|███████████████▊                      | 1098/2629 [00:04<00:08, 171.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▎                          | 826/2629 [00:04<00:11, 150.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▌                           | 777/2629 [00:04<00:14, 131.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▌                        | 979/2629 [00:04<00:08, 193.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▎                          | 833/2629 [00:04<00:10, 164.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▏                     | 1119/2629 [00:04<00:08, 179.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▌                          | 847/2629 [00:04<00:10, 163.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▋                         | 924/2629 [00:04<00:14, 116.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 1000/2629 [00:04<00:08, 185.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▋                          | 852/2629 [00:04<00:11, 151.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▍                     | 1139/2629 [00:04<00:09, 161.42it/s]\u001b[A\n",
      "\n",
      " 34%|█████████████▋                          | 903/2629 [00:04<00:17, 99.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▊                          | 865/2629 [00:04<00:12, 146.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▉                         | 939/2629 [00:04<00:15, 111.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 1020/2629 [00:04<00:08, 182.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████                          | 877/2629 [00:04<00:10, 174.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|████████████████▋                     | 1157/2629 [00:04<00:08, 163.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▏                         | 889/2629 [00:04<00:10, 168.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|█████████████▊                         | 931/2629 [00:04<00:13, 130.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 959/2629 [00:04<00:12, 128.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 1044/2629 [00:04<00:08, 196.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                         | 910/2629 [00:04<00:08, 211.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|█████████████████                     | 1183/2629 [00:04<00:07, 187.21it/s]\u001b[A\n",
      "\n",
      " 36%|██████████████                         | 948/2629 [00:04<00:12, 137.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                         | 910/2629 [00:04<00:10, 171.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▌                          | 846/2629 [00:04<00:12, 142.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 974/2629 [00:04<00:13, 122.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▍                    | 1210/2629 [00:04<00:06, 207.76it/s]\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 1065/2629 [00:04<00:09, 171.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|██████████████▎                        | 966/2629 [00:04<00:11, 146.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▊                         | 928/2629 [00:04<00:10, 167.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▋                        | 988/2629 [00:05<00:12, 126.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▎                        | 966/2629 [00:04<00:06, 239.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|█████████████████▉                    | 1239/2629 [00:05<00:06, 227.69it/s]\u001b[A\n",
      "\n",
      " 38%|██████████████▋                        | 994/2629 [00:05<00:09, 178.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 1088/2629 [00:05<00:08, 184.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 959/2629 [00:05<00:08, 203.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▏                         | 885/2629 [00:04<00:10, 159.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|██████████████▍                       | 1002/2629 [00:05<00:13, 119.23it/s]\u001b[A\n",
      "\n",
      " 39%|██████████████▋                       | 1015/2629 [00:05<00:08, 182.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 1108/2629 [00:05<00:08, 172.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▌                        | 981/2629 [00:05<00:08, 196.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▋                        | 991/2629 [00:05<00:08, 185.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 902/2629 [00:05<00:11, 153.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 39%|██████████████▋                       | 1015/2629 [00:05<00:13, 121.04it/s]\u001b[A\n",
      "\n",
      " 39%|██████████████▉                       | 1035/2629 [00:05<00:08, 178.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████████████████▎                     | 1126/2629 [00:05<00:08, 167.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 1002/2629 [00:05<00:08, 193.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 1015/2629 [00:05<00:08, 196.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 1032/2629 [00:05<00:12, 132.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████                   | 1322/2629 [00:05<00:05, 232.23it/s]\u001b[A\n",
      "\n",
      " 40%|███████████████▎                      | 1061/2629 [00:05<00:07, 199.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 1151/2629 [00:05<00:07, 187.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 1037/2629 [00:05<00:06, 232.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 1041/2629 [00:05<00:07, 211.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 1070/2629 [00:05<00:07, 196.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|███████████████▊                      | 1090/2629 [00:05<00:06, 223.67it/s]\u001b[A\u001b[A\n",
      " 51%|███████████████████▍                  | 1346/2629 [00:05<00:06, 197.29it/s]\u001b[A\n",
      "\n",
      "\n",
      " 45%|████████████████▉                     | 1171/2629 [00:05<00:08, 168.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 1061/2629 [00:05<00:07, 203.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▊                      | 1091/2629 [00:05<00:08, 181.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▍                      | 1064/2629 [00:05<00:08, 181.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▊                  | 1367/2629 [00:05<00:06, 186.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 1083/2629 [00:05<00:07, 202.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████                      | 1114/2629 [00:05<00:08, 177.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 1189/2629 [00:05<00:09, 159.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 1111/2629 [00:05<00:08, 182.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 1088/2629 [00:05<00:07, 192.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|████████████████████▏                 | 1398/2629 [00:05<00:05, 216.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 1138/2629 [00:05<00:07, 190.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 1207/2629 [00:05<00:08, 163.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 1145/2629 [00:05<00:06, 223.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 1115/2629 [00:05<00:07, 211.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▊                     | 1161/2629 [00:05<00:07, 200.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|████████████████████▋                 | 1427/2629 [00:05<00:05, 227.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 1056/2629 [00:05<00:06, 225.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 1138/2629 [00:05<00:07, 205.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▉                     | 1169/2629 [00:06<00:07, 200.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 1183/2629 [00:06<00:07, 195.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|████████████████████▉                 | 1451/2629 [00:06<00:05, 214.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 1080/2629 [00:05<00:06, 223.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 1191/2629 [00:06<00:07, 189.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 1239/2629 [00:06<00:10, 128.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▎                    | 1196/2629 [00:06<00:05, 245.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▋                    | 1225/2629 [00:06<00:05, 253.64it/s]\u001b[A\u001b[A\n",
      " 56%|█████████████████████▍                | 1481/2629 [00:06<00:04, 236.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 1111/2629 [00:06<00:06, 242.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 1216/2629 [00:06<00:06, 203.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████▍                   | 1272/2629 [00:06<00:07, 175.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████▏                   | 1259/2629 [00:06<00:04, 276.73it/s]\u001b[A\u001b[A\n",
      " 57%|█████████████████████▊                | 1506/2629 [00:06<00:04, 238.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 1212/2629 [00:06<00:06, 227.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 1136/2629 [00:06<00:06, 231.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▋                    | 1221/2629 [00:06<00:06, 202.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 1238/2629 [00:06<00:07, 194.00it/s]\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▎               | 1541/2629 [00:06<00:04, 265.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 1236/2629 [00:06<00:06, 228.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▌                   | 1288/2629 [00:06<00:05, 227.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 1569/2629 [00:06<00:04, 230.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 1258/2629 [00:06<00:08, 163.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|██████████████████▉                   | 1313/2629 [00:06<00:06, 198.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 1313/2629 [00:06<00:09, 132.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 1243/2629 [00:06<00:10, 136.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▍                   | 1276/2629 [00:06<00:08, 156.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████               | 1594/2629 [00:06<00:05, 199.58it/s]\u001b[A\n",
      "\n",
      " 51%|███████████████████▎                  | 1337/2629 [00:06<00:06, 205.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 1337/2629 [00:06<00:08, 151.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 1265/2629 [00:06<00:09, 149.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 1201/2629 [00:06<00:07, 180.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 1304/2629 [00:06<00:06, 208.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▋                   | 1293/2629 [00:06<00:09, 141.91it/s]\u001b[A\n",
      "\n",
      " 52%|███████████████████▋                  | 1360/2629 [00:06<00:06, 201.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▌                  | 1355/2629 [00:06<00:08, 151.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▋                    | 1221/2629 [00:06<00:07, 184.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 1335/2629 [00:06<00:05, 235.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 1284/2629 [00:06<00:08, 154.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████                   | 1315/2629 [00:06<00:08, 159.53it/s]\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 1379/2629 [00:06<00:07, 171.50it/s]\n",
      "\n",
      " 53%|███████████████████▉                  | 1382/2629 [00:06<00:06, 202.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████                    | 1251/2629 [00:06<00:06, 213.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 1304/2629 [00:06<00:08, 161.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 1372/2629 [00:06<00:04, 262.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▎                 | 1403/2629 [00:07<00:06, 198.85it/s]\u001b[A\u001b[A\n",
      " 63%|████████████████████████              | 1662/2629 [00:07<00:05, 166.15it/s]\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 1398/2629 [00:07<00:08, 143.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 1332/2629 [00:07<00:10, 123.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 1323/2629 [00:07<00:09, 136.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████▌                 | 1424/2629 [00:07<00:06, 184.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 1399/2629 [00:07<00:06, 193.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|████████████████████████▎             | 1683/2629 [00:07<00:05, 173.93it/s]\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 1347/2629 [00:07<00:10, 127.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▋                   | 1294/2629 [00:07<00:07, 172.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████                 | 1454/2629 [00:07<00:05, 213.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 1426/2629 [00:07<00:05, 210.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|████████████████████████▌             | 1702/2629 [00:07<00:05, 172.52it/s]\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 1368/2629 [00:07<00:08, 144.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 1315/2629 [00:07<00:07, 178.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 1367/2629 [00:07<00:07, 168.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████▍                | 1483/2629 [00:07<00:04, 231.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████                 | 1460/2629 [00:07<00:04, 241.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████             | 1732/2629 [00:07<00:04, 202.70it/s]\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 1398/2629 [00:07<00:06, 182.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 1347/2629 [00:07<00:05, 214.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 1396/2629 [00:07<00:06, 197.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████████████████████▉                | 1522/2629 [00:07<00:04, 274.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 1493/2629 [00:07<00:04, 258.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|█████████████████████████▍            | 1764/2629 [00:07<00:03, 231.74it/s]\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 1428/2629 [00:07<00:05, 211.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 1370/2629 [00:07<00:05, 217.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 1423/2629 [00:07<00:05, 215.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▍               | 1551/2629 [00:07<00:04, 251.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 1521/2629 [00:07<00:04, 250.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 1451/2629 [00:07<00:05, 209.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 1446/2629 [00:07<00:05, 201.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 1393/2629 [00:07<00:06, 199.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|█████████████████████████▊            | 1789/2629 [00:07<00:04, 199.75it/s]\u001b[A\n",
      "\n",
      " 60%|██████████████████████▊               | 1578/2629 [00:07<00:04, 253.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 1554/2629 [00:07<00:04, 245.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 1484/2629 [00:07<00:04, 241.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 1429/2629 [00:07<00:05, 239.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▏           | 1811/2629 [00:07<00:04, 203.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▏                | 1468/2629 [00:07<00:05, 194.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▏              | 1605/2629 [00:07<00:04, 252.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 1573/2629 [00:07<00:04, 220.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▊               | 1580/2629 [00:07<00:04, 217.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 1497/2629 [00:07<00:05, 218.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 1510/2629 [00:07<00:05, 209.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|██████████████████████████▍           | 1833/2629 [00:07<00:04, 186.17it/s]\u001b[A\n",
      "\n",
      " 62%|███████████████████████▌              | 1631/2629 [00:07<00:04, 240.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 1604/2629 [00:07<00:04, 222.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 1596/2629 [00:07<00:04, 213.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████                | 1525/2629 [00:07<00:04, 232.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 1489/2629 [00:07<00:04, 258.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████              | 1666/2629 [00:08<00:03, 268.12it/s]\u001b[A\u001b[A\n",
      " 58%|██████████████████████▏               | 1533/2629 [00:08<00:05, 191.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 1636/2629 [00:08<00:04, 242.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 1549/2629 [00:08<00:05, 193.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▍             | 1694/2629 [00:08<00:03, 233.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 1516/2629 [00:08<00:05, 210.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▍               | 1554/2629 [00:08<00:06, 165.24it/s]\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 1661/2629 [00:08<00:04, 208.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 1645/2629 [00:08<00:05, 185.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 1570/2629 [00:08<00:05, 190.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 1540/2629 [00:08<00:05, 210.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|██████████████████████▊               | 1580/2629 [00:08<00:05, 186.24it/s]\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 1684/2629 [00:08<00:04, 211.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|████████████████████████▊             | 1719/2629 [00:08<00:04, 200.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 1666/2629 [00:08<00:05, 188.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 1601/2629 [00:08<00:04, 220.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▌               | 1565/2629 [00:08<00:04, 218.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▏              | 1602/2629 [00:08<00:05, 193.36it/s]\u001b[A\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 1706/2629 [00:08<00:04, 201.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 1686/2629 [00:08<00:05, 173.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▏            | 1741/2629 [00:08<00:04, 180.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 1588/2629 [00:08<00:05, 185.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 1623/2629 [00:08<00:05, 170.34it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████          | 1941/2629 [00:08<00:04, 151.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 1705/2629 [00:08<00:05, 170.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 1625/2629 [00:08<00:06, 162.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▍            | 1761/2629 [00:08<00:05, 162.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 1611/2629 [00:08<00:05, 193.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 1642/2629 [00:08<00:06, 163.52it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▎         | 1958/2629 [00:08<00:04, 144.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████▉             | 1723/2629 [00:08<00:05, 153.08it/s]\n",
      "\n",
      " 68%|█████████████████████████▋            | 1781/2629 [00:08<00:04, 169.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 1645/2629 [00:08<00:06, 155.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 1640/2629 [00:08<00:04, 216.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 1669/2629 [00:08<00:05, 188.78it/s]\u001b[A\u001b[A\u001b[A\n",
      " 75%|████████████████████████████▌         | 1980/2629 [00:08<00:04, 159.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 1744/2629 [00:08<00:05, 165.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 1665/2629 [00:08<00:05, 163.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▏             | 1675/2629 [00:08<00:03, 250.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|████████████████████████████▊         | 1997/2629 [00:09<00:04, 156.49it/s]\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████            | 1804/2629 [00:08<00:04, 193.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████████████████████████            | 1799/2629 [00:09<00:05, 141.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 1684/2629 [00:08<00:05, 167.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 1690/2629 [00:09<00:05, 158.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 1762/2629 [00:08<00:05, 153.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▏        | 2018/2629 [00:09<00:03, 168.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▍           | 1830/2629 [00:09<00:04, 176.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 1741/2629 [00:08<00:03, 287.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 1715/2629 [00:09<00:04, 193.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 1709/2629 [00:09<00:05, 161.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▊           | 1856/2629 [00:09<00:03, 195.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 1736/2629 [00:09<00:04, 196.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▌            | 1771/2629 [00:09<00:03, 276.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████▉             | 1727/2629 [00:09<00:07, 121.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▋        | 2052/2629 [00:09<00:04, 134.43it/s]\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▏          | 1881/2629 [00:09<00:04, 169.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 1757/2629 [00:09<00:05, 157.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▏          | 1878/2629 [00:09<00:05, 146.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 1742/2629 [00:09<00:07, 122.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|█████████████████████████████▉        | 2067/2629 [00:09<00:04, 134.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████            | 1800/2629 [00:09<00:04, 182.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▊            | 1787/2629 [00:09<00:04, 188.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▍          | 1898/2629 [00:09<00:04, 157.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▍          | 1901/2629 [00:09<00:04, 165.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 1757/2629 [00:09<00:06, 127.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████        | 2083/2629 [00:09<00:04, 132.72it/s]\u001b[A\n",
      "\n",
      " 73%|███████████████████████████▊          | 1920/2629 [00:09<00:04, 170.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▏           | 1808/2629 [00:09<00:04, 188.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 1921/2629 [00:09<00:04, 172.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▎           | 1823/2629 [00:09<00:04, 181.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 1779/2629 [00:09<00:05, 148.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▌       | 2111/2629 [00:09<00:03, 167.77it/s]\u001b[A\n",
      "\n",
      " 74%|████████████████████████████          | 1941/2629 [00:09<00:03, 178.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 1844/2629 [00:09<00:03, 230.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████          | 1945/2629 [00:09<00:03, 187.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 1849/2629 [00:09<00:04, 193.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████            | 1799/2629 [00:09<00:05, 160.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▏          | 1878/2629 [00:09<00:02, 258.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 1975/2629 [00:09<00:03, 216.06it/s]\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▊       | 2132/2629 [00:09<00:02, 175.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▏          | 1877/2629 [00:09<00:03, 210.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████▎         | 1961/2629 [00:09<00:03, 171.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▎           | 1817/2629 [00:09<00:05, 159.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████       | 2153/2629 [00:09<00:02, 184.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▌          | 1906/2629 [00:09<00:02, 252.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▉         | 1998/2629 [00:09<00:02, 210.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▋         | 1987/2629 [00:10<00:03, 191.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▍          | 1901/2629 [00:09<00:03, 208.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▌           | 1835/2629 [00:10<00:04, 164.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▍      | 2176/2629 [00:10<00:02, 195.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████▉          | 1933/2629 [00:10<00:02, 256.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 2023/2629 [00:10<00:02, 215.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▏        | 2018/2629 [00:10<00:02, 222.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▊           | 1858/2629 [00:10<00:04, 182.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 1967/2629 [00:10<00:03, 190.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████▉      | 2211/2629 [00:10<00:01, 235.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 1960/2629 [00:10<00:02, 256.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 2049/2629 [00:10<00:02, 227.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████▌        | 2042/2629 [00:10<00:02, 223.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▏          | 1878/2629 [00:10<00:04, 186.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▋         | 1988/2629 [00:10<00:03, 193.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████▍     | 2248/2629 [00:10<00:01, 273.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 1991/2629 [00:10<00:02, 269.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 2081/2629 [00:10<00:02, 268.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 2080/2629 [00:10<00:02, 240.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▍          | 1898/2629 [00:10<00:05, 143.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 87%|████████████████████████████████▉     | 2276/2629 [00:10<00:01, 219.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 2019/2629 [00:10<00:02, 215.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 2109/2629 [00:10<00:02, 212.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▋          | 1916/2629 [00:10<00:04, 151.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 2017/2629 [00:10<00:03, 181.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 2043/2629 [00:10<00:02, 211.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 2028/2629 [00:10<00:03, 151.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▊       | 2133/2629 [00:10<00:02, 210.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 2126/2629 [00:10<00:02, 185.24it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████          | 1940/2629 [00:10<00:04, 169.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 2067/2629 [00:10<00:02, 214.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 2056/2629 [00:10<00:03, 178.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▍        | 2038/2629 [00:10<00:03, 171.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2157/2629 [00:10<00:02, 217.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 2147/2629 [00:10<00:02, 187.57it/s]\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▌    | 2322/2629 [00:10<00:01, 187.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 2101/2629 [00:10<00:02, 246.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 2057/2629 [00:10<00:03, 173.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 2082/2629 [00:10<00:02, 193.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████▎         | 1959/2629 [00:10<00:04, 150.70it/s]\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▉    | 2346/2629 [00:10<00:01, 199.09it/s]\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 2168/2629 [00:10<00:02, 180.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 2127/2629 [00:10<00:02, 241.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 2210/2629 [00:10<00:01, 236.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 1976/2629 [00:11<00:04, 147.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 2197/2629 [00:10<00:02, 207.03it/s]\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████▏   | 2368/2629 [00:11<00:01, 198.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2157/2629 [00:10<00:01, 256.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 2103/2629 [00:10<00:02, 196.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▎     | 2239/2629 [00:11<00:01, 248.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 1995/2629 [00:11<00:04, 157.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▌   | 2392/2629 [00:11<00:01, 208.54it/s]\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 2219/2629 [00:11<00:02, 201.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 2188/2629 [00:11<00:01, 269.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2267/2629 [00:11<00:01, 255.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▍        | 2035/2629 [00:11<00:02, 217.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 2151/2629 [00:11<00:02, 196.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|███████████████████████████████████   | 2423/2629 [00:11<00:00, 233.19it/s]\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 2242/2629 [00:11<00:01, 208.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 2295/2629 [00:11<00:01, 262.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2155/2629 [00:11<00:02, 220.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 2216/2629 [00:11<00:01, 242.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 2176/2629 [00:11<00:02, 208.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 2264/2629 [00:11<00:01, 199.86it/s]\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▍  | 2448/2629 [00:11<00:00, 211.09it/s]\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2322/2629 [00:11<00:01, 261.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 2180/2629 [00:11<00:02, 221.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 2242/2629 [00:11<00:01, 234.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 2198/2629 [00:11<00:02, 181.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▊        | 2059/2629 [00:11<00:03, 153.52it/s]\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████    | 2353/2629 [00:11<00:01, 270.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 2203/2629 [00:11<00:01, 216.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 2285/2629 [00:11<00:01, 174.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2271/2629 [00:11<00:01, 249.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|████████████████████████████████████  | 2494/2629 [00:11<00:00, 208.86it/s]\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 2389/2629 [00:11<00:00, 291.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 2226/2629 [00:11<00:02, 194.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 2078/2629 [00:11<00:04, 123.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 2218/2629 [00:11<00:03, 133.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 2419/2629 [00:11<00:00, 257.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 2304/2629 [00:11<00:02, 131.61it/s]\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▎ | 2516/2629 [00:11<00:00, 171.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2321/2629 [00:11<00:01, 223.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 2247/2629 [00:11<00:02, 182.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 2242/2629 [00:11<00:02, 155.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2320/2629 [00:11<00:02, 131.68it/s]\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▋ | 2536/2629 [00:11<00:00, 176.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2346/2629 [00:11<00:01, 227.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 2446/2629 [00:11<00:00, 226.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2273/2629 [00:11<00:01, 197.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 2110/2629 [00:11<00:03, 132.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 2370/2629 [00:11<00:01, 227.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 2473/2629 [00:11<00:00, 236.70it/s]\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 2557/2629 [00:11<00:00, 177.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 2300/2629 [00:11<00:01, 210.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▉       | 2140/2629 [00:12<00:02, 169.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 2335/2629 [00:11<00:02, 117.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 2404/2629 [00:12<00:00, 255.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 2506/2629 [00:12<00:00, 257.60it/s]\u001b[A\u001b[A\n",
      " 98%|█████████████████████████████████████▏| 2576/2629 [00:12<00:00, 166.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 2163/2629 [00:12<00:02, 184.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2318/2629 [00:11<00:01, 196.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2350/2629 [00:12<00:02, 124.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 2433/2629 [00:12<00:00, 262.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████▋ | 2534/2629 [00:12<00:00, 251.56it/s]\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▌| 2599/2629 [00:12<00:00, 180.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 2184/2629 [00:12<00:02, 186.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 2381/2629 [00:12<00:01, 160.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 2362/2629 [00:12<00:01, 212.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 2460/2629 [00:12<00:00, 254.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████ | 2566/2629 [00:12<00:00, 266.74it/s]\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████▊| 2618/2629 [00:12<00:00, 182.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 2211/2629 [00:12<00:02, 207.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:12<00:00, 212.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 2391/2629 [00:12<00:01, 230.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 2594/2629 [00:12<00:00, 264.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 2409/2629 [00:12<00:00, 254.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 2486/2629 [00:12<00:00, 230.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 2234/2629 [00:12<00:02, 186.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 2417/2629 [00:12<00:00, 213.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 2512/2629 [00:12<00:00, 235.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 2464/2629 [00:12<00:00, 223.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 2435/2629 [00:12<00:00, 234.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:12<00:00, 208.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 2539/2629 [00:12<00:00, 239.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 2440/2629 [00:12<00:00, 198.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 2460/2629 [00:12<00:00, 223.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2274/2629 [00:12<00:02, 167.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 2564/2629 [00:12<00:00, 220.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▉  | 2483/2629 [00:12<00:00, 203.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 2293/2629 [00:12<00:01, 172.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▎ | 2510/2629 [00:12<00:00, 188.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 2591/2629 [00:12<00:00, 219.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 2504/2629 [00:12<00:00, 180.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 2481/2629 [00:12<00:00, 163.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 2312/2629 [00:13<00:02, 150.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:13<00:00, 202.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 2516/2629 [00:12<00:00, 206.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 2523/2629 [00:12<00:00, 176.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2350/2629 [00:13<00:01, 204.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████████████████████████████████ | 2562/2629 [00:13<00:00, 266.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 2543/2629 [00:13<00:00, 181.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 2374/2629 [00:13<00:01, 212.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 2406/2629 [00:13<00:00, 240.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▊| 2619/2629 [00:13<00:00, 229.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:13<00:00, 197.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 2452/2629 [00:13<00:00, 299.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:13<00:00, 197.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:13<00:00, 197.60it/s]\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:13<00:00, 191.05it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_abolish.tsv\n",
      "Processing Started...\n",
      "Data Size:  281\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 60%|█████████████████████████▏                | 24/40 [00:00<00:00, 232.40it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 55%|███████████████████████                   | 22/40 [00:00<00:00, 200.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|██████████████████▉                       | 18/40 [00:00<00:00, 178.38it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 261.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████▏                 | 23/40 [00:00<00:00, 228.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 185.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▌                              | 11/40 [00:00<00:00, 101.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████████████                    | 21/40 [00:00<00:00, 126.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 167.59it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 156.01it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 130.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████▏                 | 23/40 [00:00<00:00, 110.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 142.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 113.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_abolish.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_abolish.tsv\n",
      "Processing Started...\n",
      "Data Size:  283\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 25%|██████████▊                                | 10/40 [00:00<00:00, 95.51it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 30%|████████████▌                             | 12/40 [00:00<00:00, 110.90it/s]\u001b[A\n",
      "\n",
      " 32%|█████████████▋                            | 13/40 [00:00<00:00, 111.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|████████████▌                             | 12/40 [00:00<00:00, 118.64it/s]\u001b[A\u001b[A\u001b[A\n",
      " 50%|█████████████████████▌                     | 20/40 [00:00<00:00, 74.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████████████████████████▉                | 25/40 [00:00<00:00, 91.25it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 24/40 [00:00<00:00, 96.84it/s]\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████████▏           | 29/40 [00:00<00:00, 76.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 106.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████▉                       | 18/40 [00:00<00:00, 172.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 100.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 113.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 40/40 [00:00<00:00, 80.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████▎               | 25/40 [00:00<00:00, 129.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 170.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 208.26it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 143.16it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_abolish.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_alter.tsv\n",
      "Processing Started...\n",
      "Data Size:  254\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 28%|███████████▉                               | 10/36 [00:00<00:00, 99.36it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 42%|█████████████████▌                        | 15/36 [00:00<00:00, 137.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|███████████████████████▎                  | 20/36 [00:00<00:00, 193.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|█████████████████▌                        | 15/36 [00:00<00:00, 137.75it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|██████                                      | 5/36 [00:00<00:00, 48.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▋                                        | 3/36 [00:00<00:01, 24.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|██████████████▎                            | 12/36 [00:00<00:00, 58.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 36/36 [00:00<00:00, 87.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████▋        | 29/36 [00:00<00:00, 89.98it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 104.27it/s]\n",
      "100%|███████████████████████████████████████████| 36/36 [00:00<00:00, 94.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 106.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████████       | 30/36 [00:00<00:00, 111.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 101.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 111.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 157.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_alter.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_alter.tsv\n",
      "Processing Started...\n",
      "Data Size:  255\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 56%|███████████████████████▎                  | 20/36 [00:00<00:00, 197.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████▎                         | 14/36 [00:00<00:00, 120.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|███████████                                 | 9/36 [00:00<00:00, 86.00it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 135.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████              | 24/36 [00:00<00:00, 122.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 81%|█████████████████████████████████▊        | 29/36 [00:00<00:00, 103.20it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 75%|████████████████████████████████▎          | 27/36 [00:00<00:00, 94.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 116.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|██████                                      | 5/36 [00:00<00:00, 49.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 36/36 [00:00<00:00, 97.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 115.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▋                | 22/36 [00:00<00:00, 118.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 148.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 106.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_alter.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_begin.tsv\n",
      "Processing Started...\n",
      "Data Size:  683\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/97 [00:00<00:00, 100.25it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 11%|████▉                                      | 11/97 [00:00<00:00, 95.84it/s]\u001b[A\n",
      "\n",
      "  7%|███▏                                        | 7/97 [00:00<00:01, 67.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 28%|███████████▋                              | 27/97 [00:00<00:00, 132.68it/s]\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/97 [00:00<00:00, 104.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 27%|███████████▎                              | 26/97 [00:00<00:00, 123.80it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 26/97 [00:00<00:00, 136.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████                    | 51/97 [00:00<00:00, 178.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|██████████████▎                           | 33/97 [00:00<00:00, 167.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████                                 | 21/97 [00:00<00:00, 189.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▎                                  | 17/97 [00:00<00:00, 152.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|████████████████▉                         | 39/97 [00:00<00:00, 120.23it/s]\u001b[A\n",
      "\n",
      " 41%|█████████████████▎                        | 40/97 [00:00<00:00, 122.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▉            | 69/97 [00:00<00:00, 174.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 42/97 [00:00<00:00, 161.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▎                        | 40/97 [00:00<00:00, 179.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|████████████████                          | 37/97 [00:00<00:00, 177.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|███████████████████████▍                  | 54/97 [00:00<00:00, 130.54it/s]\u001b[A\n",
      "\n",
      " 63%|██████████████████████████▍               | 61/97 [00:00<00:00, 151.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████████▏| 95/97 [00:00<00:00, 200.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 182.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▉            | 69/97 [00:00<00:00, 226.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▎              | 63/97 [00:00<00:00, 211.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|█████████████████████████████████▎        | 77/97 [00:00<00:00, 157.75it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 192.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████████▊  | 92/97 [00:00<00:00, 208.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 208.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 153.80it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 147.17it/s]\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 178.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 156.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_begin.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_begin.tsv\n",
      "Processing Started...\n",
      "Data Size:  684\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 22%|█████████                                 | 21/97 [00:00<00:00, 204.65it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 11%|████▉                                      | 11/97 [00:00<00:00, 98.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▉                                      | 11/97 [00:00<00:00, 95.55it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▋                                        | 8/97 [00:00<00:01, 78.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|█████████████▍                            | 31/97 [00:00<00:00, 150.25it/s]\u001b[A\n",
      "\n",
      " 29%|████████████                              | 28/97 [00:00<00:00, 137.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 42/97 [00:00<00:00, 160.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▋                                 | 20/97 [00:00<00:00, 190.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|████████████▌                             | 29/97 [00:00<00:00, 138.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▋                              | 27/97 [00:00<00:00, 141.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|██████████████████████▉                   | 53/97 [00:00<00:00, 178.02it/s]\u001b[A\n",
      "\n",
      " 43%|██████████████████▏                       | 42/97 [00:00<00:00, 133.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▌                | 59/97 [00:00<00:00, 156.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▌                       | 43/97 [00:00<00:00, 135.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▎                        | 40/97 [00:00<00:00, 179.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 42/97 [00:00<00:00, 145.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|████████████████████████████████▍         | 75/97 [00:00<00:00, 193.91it/s]\u001b[A\n",
      "\n",
      " 59%|████████████████████████▋                 | 57/97 [00:00<00:00, 139.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████████▋       | 80/97 [00:00<00:00, 174.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████████████████████████▍               | 61/97 [00:00<00:00, 151.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▌                | 59/97 [00:00<00:00, 153.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████                 | 58/97 [00:00<00:00, 175.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 183.55it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 178.73it/s]\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████          | 74/97 [00:00<00:00, 148.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████▊     | 85/97 [00:00<00:00, 175.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████       | 81/97 [00:00<00:00, 186.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▉         | 76/97 [00:00<00:00, 166.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 164.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 190.65it/s]\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████████▍  | 91/97 [00:00<00:00, 153.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████▋ | 94/97 [00:00<00:00, 170.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 141.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 170.44it/s]\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 145.59it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_begin.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_block.tsv\n",
      "Processing Started...\n",
      "Data Size:  362\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 15/51 [00:00<00:00, 139.86it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 25%|██████████▋                               | 13/51 [00:00<00:00, 129.91it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 24%|█████████▉                                | 12/51 [00:00<00:00, 119.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|█████████                                 | 11/51 [00:00<00:00, 103.44it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▏              | 33/51 [00:00<00:00, 162.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 9/51 [00:00<00:00, 83.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|██████████████████████▏                   | 27/51 [00:00<00:00, 126.79it/s]\u001b[A\n",
      "\n",
      " 51%|█████████████████████▍                    | 26/51 [00:00<00:00, 131.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|██████████████                            | 17/51 [00:00<00:00, 167.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|███████████████████████▉                  | 29/51 [00:00<00:00, 141.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████                   | 28/51 [00:00<00:00, 137.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▏                   | 27/51 [00:00<00:00, 130.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 150.65it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▍      | 43/51 [00:00<00:00, 146.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████████▏     | 44/51 [00:00<00:00, 138.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████              | 34/51 [00:00<00:00, 140.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 133.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 152.21it/s]\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 133.21it/s]\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 129.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▊        | 41/51 [00:00<00:00, 110.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 139.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 117.96it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_block.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_block.tsv\n",
      "Processing Started...\n",
      "Data Size:  363\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 37%|███████████████▋                          | 19/51 [00:00<00:00, 185.18it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 27%|███████████▌                              | 14/51 [00:00<00:00, 136.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 22%|█████████▎                                 | 11/51 [00:00<00:00, 52.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▎                                 | 11/51 [00:00<00:00, 61.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 55%|███████████████████████▌                   | 28/51 [00:00<00:00, 94.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▎          | 38/51 [00:00<00:00, 104.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████▉                     | 26/51 [00:00<00:00, 91.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████                      | 25/51 [00:00<00:00, 94.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|███████████████████████▉                  | 29/51 [00:00<00:00, 104.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▋                               | 13/51 [00:00<00:00, 121.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 125.93it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 119.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▏              | 33/51 [00:00<00:00, 167.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████▊        | 41/51 [00:00<00:00, 112.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▉         | 40/51 [00:00<00:00, 111.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████████████████████████████████▉    | 46/51 [00:00<00:00, 123.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 109.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 112.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 112.74it/s]\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 156.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 124.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_block.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_catalyse.tsv\n",
      "Processing Started...\n",
      "Data Size:  195\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 52%|█████████████████████▊                    | 14/27 [00:00<00:00, 122.37it/s]\n",
      " 30%|█████████████                               | 8/27 [00:00<00:00, 78.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████                         | 11/27 [00:00<00:00, 103.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|█████████████                               | 8/27 [00:00<00:00, 79.15it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████▉                           | 10/27 [00:00<00:00, 90.70it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 122.16it/s]\n",
      "\n",
      " 67%|████████████████████████████▋              | 18/27 [00:00<00:00, 85.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|██████████████▋                             | 9/27 [00:00<00:00, 89.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 123.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████████▎            | 19/27 [00:00<00:00, 92.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████▍                                | 7/27 [00:00<00:00, 69.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 114.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 93.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 86.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 84.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 68.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_catalyse.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_catalyse.tsv\n",
      "Processing Started...\n",
      "Data Size:  196\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|████████████████▌                         | 11/28 [00:00<00:00, 102.79it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:00, 76.49it/s]\u001b[A\n",
      "\n",
      " 32%|██████████████▏                             | 9/28 [00:00<00:00, 78.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|███████████                                 | 7/28 [00:00<00:00, 69.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████                                 | 7/28 [00:00<00:00, 66.85it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 79%|█████████████████████████████████         | 22/28 [00:00<00:00, 100.05it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 111.50it/s]\u001b[A\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:00<00:00, 92.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 21/28 [00:00<00:00, 106.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:00, 86.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 106.37it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 82.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 82.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:00<00:00, 89.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:00<00:00, 69.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 95.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 74.61it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 80.12it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_catalyse.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_confer.tsv\n",
      "Processing Started...\n",
      "Data Size:  317\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 33%|██████████████                            | 15/45 [00:00<00:00, 147.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|██████████████                            | 15/45 [00:00<00:00, 138.28it/s]\u001b[A\n",
      "\n",
      " 33%|██████████████                            | 15/45 [00:00<00:00, 145.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▌        | 36/45 [00:00<00:00, 177.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|██████████████                            | 15/45 [00:00<00:00, 136.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████                             | 14/45 [00:00<00:00, 133.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|███████████████████████████               | 29/45 [00:00<00:00, 124.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▌                                 | 10/45 [00:00<00:00, 87.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 122.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████▊                | 28/45 [00:00<00:00, 86.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|██████████████████▏                        | 19/45 [00:00<00:00, 64.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 42/45 [00:00<00:00, 91.93it/s]\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████▏   | 41/45 [00:00<00:00, 98.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 102.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 111.46it/s]\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 102.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████▏    | 40/45 [00:00<00:00, 95.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████████▏  | 42/45 [00:00<00:00, 96.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 100.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 99.04it/s]\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 97.14it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_confer.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_confer.tsv\n",
      "Processing Started...\n",
      "Data Size:  319\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 36%|██████████████▉                           | 16/45 [00:00<00:00, 142.70it/s]\n",
      " 36%|██████████████▉                           | 16/45 [00:00<00:00, 147.09it/s]\u001b[A\n",
      "\n",
      " 33%|██████████████                            | 15/45 [00:00<00:00, 142.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▏                             | 13/45 [00:00<00:00, 126.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▏                             | 13/45 [00:00<00:00, 118.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|████████████████████████████▉             | 31/45 [00:00<00:00, 131.55it/s]\u001b[A\n",
      "\n",
      " 73%|██████████████████████████████▊           | 33/45 [00:00<00:00, 161.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████                             | 14/45 [00:00<00:00, 129.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|████████████████████████▎                 | 26/45 [00:00<00:00, 125.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 166.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 168.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 132.85it/s]\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████████▍     | 39/45 [00:00<00:00, 121.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▏                | 27/45 [00:00<00:00, 107.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████████████████████▉                     | 23/45 [00:00<00:00, 80.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 125.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 115.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▎    | 40/45 [00:00<00:00, 114.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 114.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 104.35it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_confer.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_decrease.tsv\n",
      "Processing Started...\n",
      "Data Size:  195\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 33%|██████████████▋                             | 9/27 [00:00<00:00, 89.40it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 44%|██████████████████▋                       | 12/27 [00:00<00:00, 114.35it/s]\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████▏       | 22/27 [00:00<00:00, 111.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 113.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 24/27 [00:00<00:00, 94.10it/s]\u001b[A\n",
      "\n",
      " 81%|███████████████████████████████████        | 22/27 [00:00<00:00, 96.64it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 156.59it/s]\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 99.70it/s]\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████▏       | 22/27 [00:00<00:00, 101.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 93.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 102.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████████████████████▊                    | 14/27 [00:00<00:00, 128.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 131.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 133.68it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_decrease.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_decrease.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 39%|████████████████▌                         | 11/28 [00:00<00:00, 107.80it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 46%|███████████████████▌                      | 13/28 [00:00<00:00, 128.18it/s]\u001b[A\n",
      "\n",
      " 46%|███████████████████▌                      | 13/28 [00:00<00:00, 121.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▌                      | 13/28 [00:00<00:00, 129.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████         | 22/28 [00:00<00:00, 208.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 141.21it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 143.20it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 191.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████            | 20/28 [00:00<00:00, 195.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████            | 20/28 [00:00<00:00, 175.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 120.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 169.16it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 171.84it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_decrease.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_delete.tsv\n",
      "Processing Started...\n",
      "Data Size:  411\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 29%|████████████▎                             | 17/58 [00:00<00:00, 163.54it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 36%|███████████████▏                          | 21/58 [00:00<00:00, 202.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 62%|██████████████████████████                | 36/58 [00:00<00:00, 177.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▏                          | 21/58 [00:00<00:00, 183.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 17/58 [00:00<00:00, 166.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|███████████████████████████████▏          | 43/58 [00:00<00:00, 211.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████                             | 18/58 [00:00<00:00, 176.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 17/58 [00:00<00:00, 164.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 188.04it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 211.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████▌                 | 34/58 [00:00<00:00, 166.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 48/58 [00:00<00:00, 215.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████                | 36/58 [00:00<00:00, 173.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 193.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 164.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 156.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████████   | 54/58 [00:00<00:00, 152.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 160.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 147.77it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_delete.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_delete.tsv\n",
      "Processing Started...\n",
      "Data Size:  413\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 34%|██████████████▏                           | 20/59 [00:00<00:00, 196.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 34%|██████████████▏                           | 20/59 [00:00<00:00, 195.44it/s]\u001b[A\n",
      "\n",
      " 25%|██████████▋                               | 15/59 [00:00<00:00, 147.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▊                             | 18/59 [00:00<00:00, 175.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████▏            | 41/59 [00:00<00:00, 196.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▍                              | 16/59 [00:00<00:00, 144.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|██████████████████████████████▌           | 43/59 [00:00<00:00, 213.27it/s]\u001b[A\n",
      "\n",
      " 63%|██████████████████████████▎               | 37/59 [00:00<00:00, 189.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▏                           | 20/59 [00:00<00:00, 198.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 227.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 198.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 197.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 194.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 201.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 179.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 172.52it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_delete.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_develop.tsv\n",
      "Processing Started...\n",
      "Data Size:  62\n",
      "number of threads:  7\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 131.87it/s]\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 156.58it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 161.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 184.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 117.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 126.13it/s]\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 178.38it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_develop.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_develop.tsv\n",
      "Processing Started...\n",
      "Data Size:  62\n",
      "number of threads:  7\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 268.22it/s]\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 155.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 154.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 208.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 106.19it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 96.05it/s]\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 105.05it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_develop.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_disrupt.tsv\n",
      "Processing Started...\n",
      "Data Size:  119\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 213.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 88%|█████████████████████████████████████     | 15/17 [00:00<00:00, 139.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 143.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████████▌       | 14/17 [00:00<00:00, 132.31it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 131.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 137.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 102.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|███████████████████████▎                    | 9/17 [00:00<00:00, 86.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 126.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 108.04it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_disrupt.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_disrupt.tsv\n",
      "Processing Started...\n",
      "Data Size:  120\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 212.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 209.24it/s]\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████          | 13/17 [00:00<00:00, 129.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 142.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 53%|███████████████████████▎                    | 9/17 [00:00<00:00, 84.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 147.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 173.66it/s]\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 114.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 148.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_disrupt.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_eliminate.tsv\n",
      "Processing Started...\n",
      "Data Size:  191\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 70%|█████████████████████████████▌            | 19/27 [00:00<00:00, 188.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 70%|█████████████████████████████▌            | 19/27 [00:00<00:00, 180.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 191.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 198.85it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 223.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 199.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▋         | 21/27 [00:00<00:00, 197.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 189.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 198.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 211.39it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_eliminate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_eliminate.tsv\n",
      "Processing Started...\n",
      "Data Size:  192\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 93%|██████████████████████████████████████▉   | 25/27 [00:00<00:00, 225.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 67%|████████████████████████████              | 18/27 [00:00<00:00, 177.78it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 224.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████████████████████▊                    | 14/27 [00:00<00:00, 132.76it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 184.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 205.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 174.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▎    | 24/27 [00:00<00:00, 232.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 220.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 228.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 224.58it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_eliminate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_encode.tsv\n",
      "Processing Started...\n",
      "Data Size:  196\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 39%|████████████████▌                         | 11/28 [00:00<00:00, 105.45it/s]\n",
      " 39%|████████████████▉                          | 11/28 [00:00<00:00, 82.17it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 121.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 93%|███████████████████████████████████████   | 26/28 [00:00<00:00, 102.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 97.17it/s]\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 12/28 [00:00<00:00, 109.61it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 12/28 [00:00<00:00, 109.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:00, 99.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 140.08it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 112.12it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 144.70it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 156.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 107.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_encode.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_encode.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 79%|█████████████████████████████████         | 22/28 [00:00<00:00, 218.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 231.51it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 86%|████████████████████████████████████      | 24/28 [00:00<00:00, 221.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████         | 22/28 [00:00<00:00, 218.23it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 214.46it/s]\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 210.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|████████████████▌                         | 11/28 [00:00<00:00, 109.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 169.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 157.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 155.21it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_encode.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_express.tsv\n",
      "Processing Started...\n",
      "Data Size:  395\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 24/56 [00:00<00:00, 236.34it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 38%|███████████████▊                          | 21/56 [00:00<00:00, 209.08it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 21/56 [00:00<00:00, 197.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|█████████████████▎                        | 23/56 [00:00<00:00, 225.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 15/56 [00:00<00:00, 141.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 15/56 [00:00<00:00, 147.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████████      | 48/56 [00:00<00:00, 201.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|███████████████████████████████▌          | 42/56 [00:00<00:00, 186.30it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 218.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 206.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 236.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 208.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 42/56 [00:00<00:00, 201.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 178.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 191.76it/s]\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 178.28it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_express.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_express.tsv\n",
      "Processing Started...\n",
      "Data Size:  397\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 39%|████████████████▌                         | 22/56 [00:00<00:00, 219.59it/s]\u001b[A\n",
      "\n",
      " 43%|██████████████████                        | 24/56 [00:00<00:00, 229.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▌    | 50/56 [00:00<00:00, 253.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▎                           | 19/56 [00:00<00:00, 186.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 247.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████                           | 20/56 [00:00<00:00, 197.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████████▎      | 47/56 [00:00<00:00, 222.56it/s]\u001b[A\u001b[A\n",
      " 79%|█████████████████████████████████         | 44/56 [00:00<00:00, 197.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 232.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 220.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████         | 44/56 [00:00<00:00, 211.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 195.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 208.51it/s]\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 178.31it/s]\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 175.42it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_express.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_generate.tsv\n",
      "Processing Started...\n",
      "Data Size:  370\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 31%|████████████▉                             | 16/52 [00:00<00:00, 153.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|█████████████▋                            | 17/52 [00:00<00:00, 165.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|██████████████▌                           | 18/52 [00:00<00:00, 168.55it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████                              | 15/52 [00:00<00:00, 147.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████             | 36/52 [00:00<00:00, 175.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|████████████████▏                         | 20/52 [00:00<00:00, 193.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|███████████████████████████▍              | 34/52 [00:00<00:00, 163.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|████████████████▏                         | 20/52 [00:00<00:00, 189.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████▎             | 35/52 [00:00<00:00, 165.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████                 | 31/52 [00:00<00:00, 148.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 168.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▋       | 43/52 [00:00<00:00, 211.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 208.55it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 224.29it/s]\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 160.76it/s]\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 172.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|█████████████████████████████████▉        | 42/52 [00:00<00:00, 198.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 159.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 200.30it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_generate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_generate.tsv\n",
      "Processing Started...\n",
      "Data Size:  371\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 38%|███████████████▊                          | 20/53 [00:00<00:00, 192.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 34%|██████████████▎                           | 18/53 [00:00<00:00, 173.97it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 28%|███████████▉                              | 15/53 [00:00<00:00, 143.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|████████████▋                             | 16/53 [00:00<00:00, 157.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████▍                        | 22/53 [00:00<00:00, 216.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████                               | 14/53 [00:00<00:00, 126.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|███████████████████████████████▋          | 40/53 [00:00<00:00, 176.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████▍                        | 22/53 [00:00<00:00, 207.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▎                | 32/53 [00:00<00:00, 158.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████████████████████████▉               | 34/53 [00:00<00:00, 165.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 241.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 170.99it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 179.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 177.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████████▍     | 46/53 [00:00<00:00, 197.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 143.49it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 171.16it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 149.25it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_generate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_inhibit.tsv\n",
      "Processing Started...\n",
      "Data Size:  339\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 50%|█████████████████████                     | 24/48 [00:00<00:00, 225.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|██████████████                            | 16/48 [00:00<00:00, 153.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 31%|█████████████▏                            | 15/48 [00:00<00:00, 138.32it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▋                         | 19/48 [00:00<00:00, 182.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▍                              | 13/48 [00:00<00:00, 125.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▉                                  | 10/48 [00:00<00:00, 99.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 39/48 [00:00<00:00, 195.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 206.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 36/48 [00:00<00:00, 176.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 201.03it/s]\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 215.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████▎               | 30/48 [00:00<00:00, 144.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 164.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 160.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 160.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 150.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_inhibit.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_inhibit.tsv\n",
      "Processing Started...\n",
      "Data Size:  341\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 23%|█████████▊                                 | 11/48 [00:00<00:00, 78.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 69%|████████████████████████████▉             | 33/48 [00:00<00:00, 148.59it/s]\u001b[A\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 18/48 [00:00<00:00, 177.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▎                             | 14/48 [00:00<00:00, 126.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▉                                  | 10/48 [00:00<00:00, 89.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 156.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|███████████████████████████████████▉      | 41/48 [00:00<00:00, 200.60it/s]\u001b[A\n",
      "\n",
      " 56%|███████████████████████▋                  | 27/48 [00:00<00:00, 125.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▏                     | 23/48 [00:00<00:00, 114.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 190.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 36/48 [00:00<00:00, 157.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 158.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████████████████▋    | 43/48 [00:00<00:00, 146.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████████▍  | 45/48 [00:00<00:00, 142.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 131.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 137.66it/s]\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 126.07it/s]\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 123.79it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_inhibit.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_initiate.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 75%|███████████████████████████████▌          | 21/28 [00:00<00:00, 203.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 57%|████████████████████████                  | 16/28 [00:00<00:00, 156.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 157.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 157.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 155.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████████████                     | 14/28 [00:00<00:00, 123.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 147.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 128.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 147.83it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 130.51it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_initiate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_initiate.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|████████████████████████████▌             | 19/28 [00:00<00:00, 169.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 61%|█████████████████████████▍                | 17/28 [00:00<00:00, 161.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|████████████████████████                  | 16/28 [00:00<00:00, 159.78it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 177.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 193.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▌                      | 13/28 [00:00<00:00, 121.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 149.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 130.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 148.20it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 166.34it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 149.40it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_initiate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_lead.tsv\n",
      "Processing Started...\n",
      "Data Size:  492\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▍                                 | 14/70 [00:00<00:00, 130.71it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 20%|████████▍                                 | 14/70 [00:00<00:00, 134.44it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|██████████▏                               | 17/70 [00:00<00:00, 165.89it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████████                                 | 15/70 [00:00<00:00, 144.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▏                      | 32/70 [00:00<00:00, 151.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▍                              | 19/70 [00:00<00:00, 186.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|██████████████████▌                       | 31/70 [00:00<00:00, 152.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▏                            | 22/70 [00:00<00:00, 217.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████▌                    | 36/70 [00:00<00:00, 173.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████████████████████▌                    | 36/70 [00:00<00:00, 181.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████████████████████████████▊          | 53/70 [00:00<00:00, 169.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▏                | 42/70 [00:00<00:00, 208.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|███████████████████████████████▊          | 53/70 [00:00<00:00, 178.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████               | 45/70 [00:00<00:00, 216.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 58/70 [00:00<00:00, 197.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 169.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 186.78it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 173.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 202.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 209.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 177.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 188.80it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_lead.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_lead.tsv\n",
      "Processing Started...\n",
      "Data Size:  493\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 21%|█████████                                 | 15/70 [00:00<00:00, 142.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|███████▏                                  | 12/70 [00:00<00:00, 115.95it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 24%|██████████▏                               | 17/70 [00:00<00:00, 168.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████▊                               | 18/70 [00:00<00:00, 179.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████████████████████▌                    | 36/70 [00:00<00:00, 179.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▊                               | 18/70 [00:00<00:00, 175.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|█████████████████████▌                    | 36/70 [00:00<00:00, 185.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████                              | 20/70 [00:00<00:00, 185.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████████████████████                     | 35/70 [00:00<00:00, 173.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████         | 55/70 [00:00<00:00, 182.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▏                | 42/70 [00:00<00:00, 181.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████████████████████▌                    | 36/70 [00:00<00:00, 178.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 56/70 [00:00<00:00, 189.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████▊             | 48/70 [00:00<00:00, 233.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 193.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 181.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▏    | 62/70 [00:00<00:00, 188.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 183.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 190.71it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 215.76it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 175.40it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 177.73it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_lead.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_lose.tsv\n",
      "Processing Started...\n",
      "Data Size:  286\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 52%|██████████████████████                    | 21/40 [00:00<00:00, 204.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 48%|███████████████████▉                      | 19/40 [00:00<00:00, 189.82it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|█████████████████▊                        | 17/40 [00:00<00:00, 169.19it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 219.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████████████                     | 20/40 [00:00<00:00, 199.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████▏                 | 23/40 [00:00<00:00, 227.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 191.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████                   | 22/40 [00:00<00:00, 216.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████████▊   | 37/40 [00:00<00:00, 179.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 180.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 183.58it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 202.30it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 230.27it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 203.88it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_lose.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_lose.tsv\n",
      "Processing Started...\n",
      "Data Size:  287\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 49%|████████████████████▍                     | 20/41 [00:00<00:00, 198.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|███████████████████▍                      | 19/41 [00:00<00:00, 184.15it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 59%|████████████████████████▌                 | 24/41 [00:00<00:00, 233.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 218.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████▌                 | 24/41 [00:00<00:00, 232.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 202.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 202.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 201.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 195.64it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 200.75it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 210.53it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_lose.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_modify.tsv\n",
      "Processing Started...\n",
      "Data Size:  166\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 216.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 83%|██████████████████████████████████▋       | 19/23 [00:00<00:00, 181.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 172.33it/s]\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 61%|█████████████████████████▌                | 14/23 [00:00<00:00, 126.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 145.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 148.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 121.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 133.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 114.66it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_modify.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_modify.tsv\n",
      "Processing Started...\n",
      "Data Size:  167\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 78%|████████████████████████████████▊         | 18/23 [00:00<00:00, 175.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|█████████████████████████████▏            | 16/23 [00:00<00:00, 157.15it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 174.57it/s]\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▊         | 18/23 [00:00<00:00, 173.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 154.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 157.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 161.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 155.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 148.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 182.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_modify.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_mutate.tsv\n",
      "Processing Started...\n",
      "Data Size:  244\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 53%|██████████████████████▏                   | 18/34 [00:00<00:00, 178.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|███████████████████▊                      | 16/34 [00:00<00:00, 158.22it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|██████████████████▌                       | 15/34 [00:00<00:00, 145.57it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 208.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 196.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 210.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 180.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▎                        | 14/34 [00:00<00:00, 133.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████▍                  | 19/34 [00:00<00:00, 166.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 179.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 164.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 34/34 [00:00<00:00, 142.36it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_mutate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_mutate.tsv\n",
      "Processing Started...\n",
      "Data Size:  245\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▏                      | 16/35 [00:00<00:00, 154.76it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 49%|████████████████████▍                     | 17/35 [00:00<00:00, 168.32it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 46%|███████████████████▏                      | 16/35 [00:00<00:00, 159.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▍                           | 12/35 [00:00<00:00, 116.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 176.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▍                           | 12/35 [00:00<00:00, 114.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 173.51it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 178.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▍                           | 12/35 [00:00<00:00, 110.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 29/35 [00:00<00:00, 147.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████▊             | 24/35 [00:00<00:00, 111.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 136.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 121.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 130.64it/s]\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 134.12it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_mutate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_proliferate.tsv\n",
      "Processing Started...\n",
      "Data Size:  162\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 43%|██████████████████▋                        | 10/23 [00:00<00:00, 96.99it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 61%|█████████████████████████▌                | 14/23 [00:00<00:00, 134.00it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 132.17it/s]\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 158.72it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 177.56it/s]\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▏            | 16/23 [00:00<00:00, 150.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▊         | 18/23 [00:00<00:00, 168.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▏            | 16/23 [00:00<00:00, 153.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 161.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 128.71it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 141.47it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 167.85it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_proliferate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_proliferate.tsv\n",
      "Processing Started...\n",
      "Data Size:  163\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▏            | 16/23 [00:00<00:00, 153.51it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 48%|████████████████████                      | 11/23 [00:00<00:00, 109.55it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 164.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 131.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 126.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 158.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 173.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 155.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 160.38it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_proliferate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_recognize.tsv\n",
      "Processing Started...\n",
      "Data Size:  291\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 37%|███████████████▎                          | 15/41 [00:00<00:00, 144.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 34%|██████████████▎                           | 14/41 [00:00<00:00, 132.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████▍                         | 16/41 [00:00<00:00, 153.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▊        | 33/41 [00:00<00:00, 162.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▍                        | 17/41 [00:00<00:00, 156.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 170.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████████████████████▍                     | 20/41 [00:00<00:00, 195.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 155.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 165.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 166.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 198.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 141.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 174.29it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_recognize.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_recognize.tsv\n",
      "Processing Started...\n",
      "Data Size:  293\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▍                        | 17/41 [00:00<00:00, 168.71it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|████████████████████████▌                 | 24/41 [00:00<00:00, 236.88it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 222.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████▌                 | 24/41 [00:00<00:00, 218.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|███████████████████████████████▊          | 31/41 [00:00<00:00, 153.70it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|██████████████████████▌                   | 22/41 [00:00<00:00, 192.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 201.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 216.14it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 162.76it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 227.04it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 210.13it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 231.92it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_recognize.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_result.tsv\n",
      "Processing Started...\n",
      "Data Size:  496\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 23%|█████████▌                                | 16/70 [00:00<00:00, 153.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|█████████▌                                | 16/70 [00:00<00:00, 154.64it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▊                                    | 11/70 [00:00<00:00, 97.46it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▊                                    | 11/70 [00:00<00:00, 97.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▏                   | 37/70 [00:00<00:00, 185.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████████▏                               | 17/70 [00:00<00:00, 169.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|█████████████████████                     | 35/70 [00:00<00:00, 173.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████▌                             | 21/70 [00:00<00:00, 201.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|████████████████████▍                     | 34/70 [00:00<00:00, 165.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 30/70 [00:00<00:00, 148.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████▍      | 59/70 [00:00<00:00, 200.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 231.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 187.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▏                | 42/70 [00:00<00:00, 172.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 172.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████████▌     | 61/70 [00:00<00:00, 167.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 183.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 150.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 185.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 143.77it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_result.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_result.tsv\n",
      "Processing Started...\n",
      "Data Size:  497\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 21%|████████▊                                 | 15/71 [00:00<00:00, 141.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 18%|███████▋                                  | 13/71 [00:00<00:00, 123.02it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 24%|██████████                                | 17/71 [00:00<00:00, 165.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▍                                | 16/71 [00:00<00:00, 151.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████                             | 22/71 [00:00<00:00, 218.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████████████████████▎                    | 36/71 [00:00<00:00, 179.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|██████████████████▎                       | 31/71 [00:00<00:00, 137.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▊                              | 20/71 [00:00<00:00, 191.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|████████████████████                      | 34/71 [00:00<00:00, 148.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|██████████████████▉                       | 32/71 [00:00<00:00, 108.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████                | 44/71 [00:00<00:00, 146.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|███████████████████████████████▉          | 54/71 [00:00<00:00, 136.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|███████████████████████████▊              | 47/71 [00:00<00:00, 162.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████▋                  | 40/71 [00:00<00:00, 143.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|████████████████████████████▉             | 49/71 [00:00<00:00, 120.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 184.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 149.28it/s]\n",
      "\n",
      " 93%|███████████████████████████████████████   | 66/71 [00:00<00:00, 153.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 179.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 143.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████████▍   | 65/71 [00:00<00:00, 181.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████████████████████████████████▊    | 64/71 [00:00<00:00, 129.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 138.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 133.45it/s]\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 160.81it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_result.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_skip.tsv\n",
      "Processing Started...\n",
      "Data Size:  79\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 154.69it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 139.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 127.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 142.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 133.79it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 96.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 94.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_skip.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_skip.tsv\n",
      "Processing Started...\n",
      "Data Size:  80\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 161.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 144.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 134.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████████    | 10/11 [00:00<00:00, 92.05it/s]\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 92.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████████            | 8/11 [00:00<00:00, 79.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 92.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 104.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 148.23it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_skip.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_splice.tsv\n",
      "Processing Started...\n",
      "Data Size:  390\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 38%|████████████████                          | 21/55 [00:00<00:00, 208.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 44%|██████████████████▎                       | 24/55 [00:00<00:00, 237.32it/s]\u001b[A\n",
      "\n",
      " 33%|█████████████▋                            | 18/55 [00:00<00:00, 175.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|██████████████▌                           | 19/55 [00:00<00:00, 179.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|████████████████                          | 21/55 [00:00<00:00, 180.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▋                               | 14/55 [00:00<00:00, 137.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████          | 42/55 [00:00<00:00, 189.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 87%|████████████████████████████████████▋     | 48/55 [00:00<00:00, 206.87it/s]\u001b[A\n",
      "\n",
      " 65%|███████████████████████████▍              | 36/55 [00:00<00:00, 167.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 212.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 194.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▎          | 41/55 [00:00<00:00, 189.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████             | 38/55 [00:00<00:00, 196.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 185.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 193.99it/s]\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 204.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 157.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 152.87it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_splice.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_splice.tsv\n",
      "Processing Started...\n",
      "Data Size:  390\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 31%|████████████▉                             | 17/55 [00:00<00:00, 166.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▏                             | 16/55 [00:00<00:00, 156.41it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 25%|██████████▋                               | 14/55 [00:00<00:00, 137.47it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████▏      | 46/55 [00:00<00:00, 236.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████████████████████▍                    | 28/55 [00:00<00:00, 265.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 44/55 [00:00<00:00, 221.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▏                   | 29/55 [00:00<00:00, 273.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|█████████████████████████████▊            | 39/55 [00:00<00:00, 199.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 210.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 180.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 194.71it/s]\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 172.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████▏      | 46/55 [00:00<00:00, 200.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 211.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 207.27it/s]\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 206.40it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_splice.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_transcribe.tsv\n",
      "Processing Started...\n",
      "Data Size:  799\n",
      "number of threads:  7\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▋                                | 24/114 [00:00<00:00, 234.64it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▊                                  | 19/114 [00:00<00:00, 185.69it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 18%|███████▌                                 | 21/114 [00:00<00:00, 204.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▎                                | 23/114 [00:00<00:00, 226.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▏                                 | 20/114 [00:00<00:00, 199.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▌                                 | 21/114 [00:00<00:00, 204.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▊                                  | 19/114 [00:00<00:00, 179.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|█████████████▋                           | 38/114 [00:00<00:00, 165.12it/s]\u001b[A\n",
      "\n",
      " 42%|█████████████████▎                       | 48/114 [00:00<00:00, 177.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████████████████▌                        | 46/114 [00:00<00:00, 217.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▍                          | 40/114 [00:00<00:00, 198.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▌                        | 46/114 [00:00<00:00, 220.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████                           | 39/114 [00:00<00:00, 189.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|████████████████████▏                    | 56/114 [00:00<00:00, 171.14it/s]\u001b[A\n",
      "\n",
      "\n",
      " 60%|████████████████████████▍                | 68/114 [00:00<00:00, 209.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▉                   | 61/114 [00:00<00:00, 169.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████████████████████▌                   | 60/114 [00:00<00:00, 195.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████                 | 67/114 [00:00<00:00, 139.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████████████████████▌                   | 60/114 [00:00<00:00, 183.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████         | 89/114 [00:00<00:00, 204.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|████████████████████████████▊            | 80/114 [00:00<00:00, 189.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|████████████████████████████▍            | 79/114 [00:00<00:00, 162.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|█████████████████████████████▊           | 83/114 [00:00<00:00, 137.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 93/114 [00:00<00:00, 159.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|█████████████████████████████████▍       | 93/114 [00:00<00:00, 166.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|██████████████████████████████████████▌ | 110/114 [00:00<00:00, 170.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|███████████████████████████████████     | 100/114 [00:00<00:00, 154.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|██████████████████████████████████▌      | 96/114 [00:00<00:00, 131.14it/s]\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 179.97it/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 167.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|███████████████████████████████████     | 100/114 [00:00<00:00, 162.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 176.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 173.43it/s]\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 143.98it/s]\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 157.06it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 144.80it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_transcribe.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_transcribe.tsv\n",
      "Processing Started...\n",
      "Data Size:  801\n",
      "number of threads:  7\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 18%|███████▌                                 | 21/114 [00:00<00:00, 202.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▉                                 | 22/114 [00:00<00:00, 212.59it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 16%|██████▍                                  | 18/114 [00:00<00:00, 176.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|███████▌                                 | 21/114 [00:00<00:00, 155.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▍                                   | 15/114 [00:00<00:00, 135.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████                                      | 11/114 [00:00<00:01, 86.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████                          | 42/114 [00:00<00:00, 143.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▍                              | 29/114 [00:00<00:00, 126.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▉                                | 25/114 [00:00<00:00, 112.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▉                            | 36/114 [00:00<00:00, 116.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▎                               | 26/114 [00:00<00:00, 118.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|█████████████▎                           | 37/114 [00:00<00:00, 122.79it/s]\u001b[A\u001b[A\u001b[A\n",
      " 51%|████████████████████▊                    | 58/114 [00:00<00:00, 138.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████████████████▉                        | 47/114 [00:00<00:00, 146.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████                          | 42/114 [00:00<00:00, 133.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|█████████████████▌                       | 49/114 [00:00<00:00, 119.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████                          | 42/114 [00:00<00:00, 135.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|█████████████████▉                       | 50/114 [00:00<00:00, 120.88it/s]\u001b[A\u001b[A\u001b[A\n",
      " 53%|█████████████████████▌                   | 60/114 [00:00<00:00, 135.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|██████████████████████▎                  | 62/114 [00:00<00:00, 142.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████████████████████▌                    | 57/114 [00:00<00:00, 134.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████████████████████▌                    | 57/114 [00:00<00:00, 136.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████████████████████████▎              | 73/114 [00:00<00:00, 106.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|██████████████████████▊                   | 62/114 [00:00<00:00, 90.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████████████▋             | 77/114 [00:00<00:00, 132.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|██████████████████████████▉              | 75/114 [00:00<00:00, 108.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████████████▌               | 71/114 [00:00<00:00, 124.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████████████▌               | 71/114 [00:00<00:00, 129.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|██████████████████████████████▌          | 85/114 [00:00<00:00, 103.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|████████████████████████████             | 78/114 [00:00<00:00, 108.10it/s]\u001b[A\u001b[A\n",
      " 80%|████████████████████████████████▋        | 91/114 [00:00<00:00, 119.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████████████████████████████▋        | 91/114 [00:00<00:00, 130.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|██████████████████████████████▏          | 84/114 [00:00<00:00, 125.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|███████████████████████████████████▍    | 101/114 [00:00<00:00, 113.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|█████████████████████████████████        | 92/114 [00:00<00:00, 116.53it/s]\u001b[A\u001b[A\n",
      " 93%|█████████████████████████████████████▏  | 106/114 [00:00<00:00, 127.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|████████████████████████████████████▊   | 105/114 [00:00<00:00, 131.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|██████████████████████████████████████▉ | 111/114 [00:00<00:00, 146.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████████████████████████████████▉  | 108/114 [00:00<00:00, 156.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 131.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 134.91it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 123.79it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 137.01it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 143.23it/s]\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 127.23it/s]\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:01<00:00, 113.97it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_transcribe.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_transform.tsv\n",
      "Processing Started...\n",
      "Data Size:  375\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 25%|██████████▎                               | 13/53 [00:00<00:00, 129.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|████████                                   | 10/53 [00:00<00:00, 92.25it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 32%|█████████████▍                            | 17/53 [00:00<00:00, 168.90it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 49%|████████████████████▌                     | 26/53 [00:00<00:00, 128.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▋                                 | 11/53 [00:00<00:00, 105.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|█████████████████████▍                    | 27/53 [00:00<00:00, 135.15it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 20/53 [00:00<00:00, 193.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████        | 43/53 [00:00<00:00, 218.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 215.64it/s]\u001b[A\u001b[A\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 45/53 [00:00<00:00, 155.42it/s]\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▏                   | 28/53 [00:00<00:00, 142.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 45/53 [00:00<00:00, 149.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████▋                             | 16/53 [00:00<00:00, 141.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 188.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 147.84it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 136.84it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 200.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████        | 43/53 [00:00<00:00, 138.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 145.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 144.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_transform.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_transform.tsv\n",
      "Processing Started...\n",
      "Data Size:  377\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▋                         | 21/53 [00:00<00:00, 205.97it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|███████████                               | 14/53 [00:00<00:00, 135.56it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 36%|███████████████                           | 19/53 [00:00<00:00, 185.32it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 23/53 [00:00<00:00, 205.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████                           | 19/53 [00:00<00:00, 188.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████    | 48/53 [00:00<00:00, 236.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|███████████████████                       | 24/53 [00:00<00:00, 239.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 233.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 209.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████▎        | 42/53 [00:00<00:00, 205.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 44/53 [00:00<00:00, 185.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 187.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 202.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 182.68it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 219.97it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 197.86it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_transform.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_translate.tsv\n",
      "Processing Started...\n",
      "Data Size:  678\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 15%|██████▏                                   | 14/96 [00:00<00:00, 134.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|██████▏                                   | 14/96 [00:00<00:00, 128.41it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 14%|█████▋                                    | 13/96 [00:00<00:00, 127.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/96 [00:00<00:00, 108.28it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 31%|█████████████▏                            | 30/96 [00:00<00:00, 146.51it/s]\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/96 [00:00<00:00, 109.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|████████████▎                             | 28/96 [00:00<00:00, 129.92it/s]\u001b[A\n",
      "\n",
      " 29%|████████████▎                             | 28/96 [00:00<00:00, 140.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|███████                                   | 16/96 [00:00<00:00, 156.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████▏                                   | 14/96 [00:00<00:00, 134.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|██████████████████████▊                   | 52/96 [00:00<00:00, 177.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▉                           | 34/96 [00:00<00:00, 178.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|█████████████████████                     | 48/96 [00:00<00:00, 154.56it/s]\u001b[A\n",
      "\n",
      " 45%|██████████████████▊                       | 43/96 [00:00<00:00, 144.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▋                         | 38/96 [00:00<00:00, 193.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|███████████████████▋                      | 45/96 [00:00<00:00, 152.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|████████████████████████████████▍         | 74/96 [00:00<00:00, 192.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|██████████████████████▊                   | 52/96 [00:00<00:00, 177.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▍                | 58/96 [00:00<00:00, 132.72it/s]\n",
      " 67%|████████████████████████████              | 64/96 [00:00<00:00, 134.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████████▏| 94/96 [00:00<00:00, 182.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|█████████████████▉                        | 41/96 [00:00<00:00, 113.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|██████████████████████████████▋           | 70/96 [00:00<00:00, 173.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████████████████████████▋               | 61/96 [00:00<00:00, 132.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████████▌       | 79/96 [00:00<00:00, 153.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████▎        | 76/96 [00:00<00:00, 174.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 78/96 [00:00<00:00, 124.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████                  | 55/96 [00:00<00:00, 119.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████████▌   | 88/96 [00:00<00:00, 164.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 162.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████████▌| 95/96 [00:00<00:00, 145.20it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 144.11it/s]\n",
      " 95%|███████████████████████████████████████▊  | 91/96 [00:00<00:00, 124.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▊            | 68/96 [00:00<00:00, 122.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 167.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 128.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 133.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_translate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_translate.tsv\n",
      "Processing Started...\n",
      "Data Size:  678\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 15%|██████▏                                   | 14/96 [00:00<00:00, 133.78it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████▌                                   | 15/96 [00:00<00:00, 140.72it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 10%|████▍                                      | 10/96 [00:00<00:00, 96.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/96 [00:00<00:00, 109.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 28/96 [00:00<00:00, 124.84it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 36%|███████████████▎                          | 35/96 [00:00<00:00, 173.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████▍                                      | 10/96 [00:00<00:00, 96.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 26%|██████████▉                               | 25/96 [00:00<00:00, 123.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|████▍                                      | 10/96 [00:00<00:01, 79.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████                                | 23/96 [00:00<00:00, 108.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|███▏                                        | 7/96 [00:00<00:01, 59.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▏                                | 21/96 [00:00<00:00, 101.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|█████████████████▉                        | 41/96 [00:00<00:00, 116.42it/s]\u001b[A\n",
      "\n",
      " 40%|████████████████▋                         | 38/96 [00:00<00:00, 111.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|████████                                   | 18/96 [00:00<00:01, 73.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|██████████████▉                           | 34/96 [00:00<00:00, 101.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▉                                  | 20/96 [00:00<00:00, 94.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|██████████████████████████████▋           | 70/96 [00:00<00:00, 158.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████▏                  | 53/96 [00:00<00:00, 103.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▎                   | 51/96 [00:00<00:00, 114.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▉                           | 34/96 [00:00<00:00, 108.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████████████                     | 48/96 [00:00<00:00, 111.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|█████████████████████                      | 47/96 [00:00<00:00, 96.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 64/96 [00:00<00:00, 97.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|███████████████████████████▌              | 63/96 [00:00<00:00, 102.36it/s]\u001b[A\u001b[A\n",
      " 90%|█████████████████████████████████████▋    | 86/96 [00:00<00:00, 128.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|███████████████████▋                      | 45/96 [00:00<00:00, 107.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████████████████████████▌                 | 57/96 [00:00<00:00, 95.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 142.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████████▏         | 74/96 [00:00<00:00, 94.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████▋        | 77/96 [00:00<00:00, 112.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|███████████████████████████▌              | 63/96 [00:00<00:00, 126.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████▏                      | 45/96 [00:00<00:00, 71.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|██████████████████████████████████████▉   | 89/96 [00:00<00:00, 109.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▊           | 71/96 [00:00<00:00, 86.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 107.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████▎      | 81/96 [00:00<00:00, 96.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 100.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████▎      | 81/96 [00:00<00:00, 75.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▊               | 62/96 [00:00<00:00, 69.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 96/96 [00:00<00:00, 97.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████▉    | 87/96 [00:00<00:00, 87.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████████▊   | 89/96 [00:01<00:00, 71.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 96/96 [00:00<00:00, 96.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 96/96 [00:01<00:00, 85.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████▉    | 87/96 [00:01<00:00, 89.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 96/96 [00:01<00:00, 77.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_translate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testa_truncate.tsv\n",
      "Processing Started...\n",
      "Data Size:  161\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 26%|███████████▍                                | 6/23 [00:00<00:00, 59.92it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 17%|███████▋                                    | 4/23 [00:00<00:00, 39.17it/s]\u001b[A\n",
      "\n",
      " 52%|██████████████████████▍                    | 12/23 [00:00<00:00, 43.53it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 43%|██████████████████▋                        | 10/23 [00:00<00:00, 47.39it/s]\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████▎   | 21/23 [00:00<00:00, 58.11it/s]\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 55.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 52%|██████████████████████▍                    | 12/23 [00:00<00:00, 51.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 43.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▌                                  | 5/23 [00:00<00:00, 38.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████████▋         | 18/23 [00:00<00:00, 45.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▋                                      | 3/23 [00:00<00:00, 29.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 48.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████████████████████████▏                | 14/23 [00:00<00:00, 59.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▋                        | 10/23 [00:00<00:00, 51.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|█████████▌                                  | 5/23 [00:00<00:00, 36.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████▋         | 18/23 [00:00<00:00, 50.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▉             | 16/23 [00:00<00:00, 54.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████████▍     | 20/23 [00:00<00:00, 54.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 57.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 43.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 53.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 49.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testa_truncate.json\n",
      "Loading raw data for task conllsrl from content/data/ner_coNLL_testb_truncate.tsv\n",
      "Processing Started...\n",
      "Data Size:  161\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 35%|███████████████▎                            | 8/23 [00:00<00:00, 76.61it/s]/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 70%|█████████████████████████████▉             | 16/23 [00:00<00:00, 73.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 48.53it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 17%|███████▋                                    | 4/23 [00:00<00:00, 33.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 62.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 22%|█████████▌                                  | 5/23 [00:00<00:00, 43.50it/s]\u001b[A\u001b[A/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▌                                  | 5/23 [00:00<00:00, 43.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 54.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 51.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▋                                      | 3/23 [00:00<00:00, 27.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|██████████████████████▍                    | 12/23 [00:00<00:00, 55.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 52.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|█████████████████████████████████████████▏ | 22/23 [00:00<00:00, 49.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████████▌       | 19/23 [00:00<00:00, 64.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▋                        | 10/23 [00:00<00:00, 50.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 47.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 57.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 60.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 51.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 64.27it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 56.36it/s]\n",
      "Data Processing done for conllsrl. File saved at content/data/bert-base-uncased_prepared_data/ner_conll_testb_truncate.json\n"
     ]
    }
   ],
   "source": [
    "!python ../data_preparation.py \\\n",
    "    --task_file 'tasks_file_SRL.yml' \\\n",
    "    --data_dir 'content/data/' \\\n",
    "    --max_seq_len 50\n",
    "    # --data_dir '../../data' \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step -3 Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Dat768\n",
      "DatLinear(in_features=768, out_features=768, bias=True)\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch: 0:   1%|▏                          | 93/10434 [04:32<14:25:46,  5.02s/it]"
     ]
    }
   ],
   "source": [
    "!python ../train.py \\\n",
    "    --data_dir 'content/data/bert-base-uncased_prepared_data' \\\n",
    "    --task_file 'tasks_file_SRL.yml' \\\n",
    "    --out_dir 'conll_ner_pos_bert_base' \\\n",
    "    --epochs 5 \\\n",
    "    --train_batch_size 4 \\\n",
    "    --eval_batch_size 8 \\\n",
    "    --grad_accumulation_steps 2 \\\n",
    "    --log_per_updates 25 \\\n",
    "    --save_per_updates 100 \\\n",
    "    --eval_while_train \\\n",
    "    --test_while_train \\\n",
    "    --max_seq_len 50 \\\n",
    "    --silent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 4 Infering\n",
    "\n",
    "You can import and use the ``inferPipeline`` to get predictions for the required tasks.\n",
    "The trained model and maximum sequence length to be used needs to be specified.\n",
    "\n",
    "For knowing more details about infering, refer to <a href=\"https://multi-task-nlp.readthedocs.io/en/latest/infering.html\">infer pipeline</a> in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "     --------------------------------------- 10.0/10.0 MB 18.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\anaconda3\\envs\\biosyntax\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\admin\\anaconda3\\envs\\biosyntax\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\biosyntax\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.3.5 pytz-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from infer_pipeline import inferPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is DATA\n",
      " Volume Serial Number is A28A-1BEE\n",
      "\n",
      " Directory of d:\\FORME\\Khoa-Luan\\src\\model\\Model don t�c v? SRL\\SRL model\n",
      "\n",
      "08/22/2023  08:23 PM    <DIR>          .\n",
      "08/22/2023  07:31 PM    <DIR>          ..\n",
      "07/31/2023  12:55 PM    <DIR>          coNLL_data\n",
      "08/22/2023  08:23 PM     1,311,759,473 multi_task_model_9_204.pt\n",
      "08/22/2023  08:23 PM         1,275,450 SRL_model.ipynb\n",
      "07/31/2023  12:55 PM             2,269 tasks_file_SRL.yml\n",
      "07/31/2023  12:55 PM             2,438 transform_file_conll.yml\n",
      "               4 File(s)  1,313,039,630 bytes\n",
      "               3 Dir(s)  924,531,265,536 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size layers of shared model:  12\n",
      "first layer:  BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "size of first layer embeddings:  torch.Size([768, 768])\n",
      "first 10 words from first layer embeddings:  tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0154,  0.0768,  0.0548],\n",
      "        [-0.0326,  0.0346, -0.0423,  ..., -0.0527,  0.1393,  0.0078],\n",
      "        [ 0.0105,  0.0334,  0.0109,  ..., -0.0279,  0.0258, -0.0468],\n",
      "        ...,\n",
      "        [-0.0304, -0.0418,  0.0205,  ...,  0.0080, -0.0145,  0.0207],\n",
      "        [ 0.0789,  0.0158,  0.0276,  ...,  0.0005,  0.0096,  0.0101],\n",
      "        [ 0.0109,  0.0529,  0.0141,  ..., -0.0416,  0.0049,  0.0078]])\n"
     ]
    }
   ],
   "source": [
    "pipe = inferPipeline(modelPath='./multi_task_model_9_204.pt', maxSeqLen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
